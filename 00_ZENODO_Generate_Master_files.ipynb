{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9126cbe4-72ca-48bb-a657-4a146e460fcc",
   "metadata": {},
   "source": [
    "Generate a dcb master dataframe (with Daily dcb and/or Monthly code P1P2 and P1C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fef35-fc06-42fa-b732-d6882ce8116f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Build a master satellite DCB table (C2W–C1C) by combining daily SINEX-BIAS files and monthly CODE DCB products.\n",
    "\"\"\"\n",
    "Output: CSV with one row per (date, PRN) containing:\n",
    "  date, year, doy, prn, C2W-C1C_sat_m, source,\n",
    "  daily_file, monthly_p1c1, monthly_p1p2\n",
    "\n",
    "Paths are configured via the CONFIG section below. Adapt them to your layout.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONFIG (EDIT FOR YOUR SETUP)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Base project directory (by default: folder containing this script)\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "\n",
    "# Directory with daily SINEX-BIAS / DCB files (e.g., CAS0MGXRAP_YYYYDOY....BIA/BSX)\n",
    "DAILY_DIR = PROJECT_ROOT / \"data\" / \"dcb_daily\"\n",
    "\n",
    "# Directory with monthly CODE DCB files (e.g., P1C1YYMM.DCB, P1P2YYMM_ALL.DCB)\n",
    "MONTHLY_DIR = PROJECT_ROOT / \"data\" / \"dcb_monthly\"\n",
    "\n",
    "# Output CSV\n",
    "OUT_CSV = PROJECT_ROOT / \"outputs\" / \"all_dcb_master.csv\"\n",
    "\n",
    "# Date range (inclusive)\n",
    "START = date(2015, 10, 1)\n",
    "END   = date(2025, 9, 26)\n",
    "\n",
    "# Preference order for SINEX rows (DSB vs DCB)\n",
    "PREFER_ORDER = (\"DSB\", \"DCB\")\n",
    "\n",
    "# Import the DCB parser module (make sure this file is in the same repo / PYTHONPATH)\n",
    "# NOTE: rename your parser file to a valid module name, e.g.:\n",
    "#   latest_bsx_dsb_dcb_sat_c2w_c1c_parser_corr.py\n",
    "import latest_bsx_dsb_dcb_sat_c2w_c1c_parser_corr as bsx_mod  # type: ignore\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def yymm_from_ym(y: int, m: int) -> str:\n",
    "    \"\"\"Return YYMM string from year, month.\"\"\"\n",
    "    return f\"{y % 100:02d}{m:02d}\"\n",
    "\n",
    "\n",
    "def monthly_paths(y: int, m: int) -> tuple[Path | None, Path | None]:\n",
    "    \"\"\"\n",
    "    Return candidate monthly CODE paths for P1C1 and P1P2.\n",
    "\n",
    "    Examples:\n",
    "      P1C1YYMM.DCB\n",
    "      P1P2YYMM_ALL.DCB\n",
    "    \"\"\"\n",
    "    yymm = yymm_from_ym(y, m)\n",
    "    p1c1 = MONTHLY_DIR / f\"P1C1{yymm}.DCB\"\n",
    "    p1p2 = MONTHLY_DIR / f\"P1P2{yymm}_ALL.DCB\"\n",
    "    return (p1c1 if p1c1.exists() else None,\n",
    "            p1p2 if p1p2.exists() else None)\n",
    "\n",
    "\n",
    "def cache_monthly(cache: dict, y: int, m: int):\n",
    "    \"\"\"\n",
    "    Load and cache monthly P1C1 / P1P2 DCB tables for a given (year, month).\n",
    "\n",
    "    Returns (month_p1c1, month_p1p2, p1c1_path, p1p2_path) where\n",
    "    month_p1c1 and month_p1p2 are dict-like {prn: value_in_meters}.\n",
    "    \"\"\"\n",
    "    key = (y, m)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "\n",
    "    p1c1_path, p1p2_path = monthly_paths(y, m)\n",
    "    m1 = bsx_mod.parse_code_monthly_table(str(p1c1_path)) if p1c1_path else None\n",
    "    m2 = bsx_mod.parse_code_monthly_table(str(p1p2_path)) if p1p2_path else None\n",
    "\n",
    "    cache[key] = (m1, m2, p1c1_path, p1p2_path)\n",
    "    return cache[key]\n",
    "\n",
    "\n",
    "def fn_candidates(y: int, doy: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return candidate daily filename stems for given (year, day-of-year).\n",
    "    Adjust this if you use other IAACs or naming conventions.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f\"CAS0OPSRAP_{y}{doy:03d}0000_01D_01D_DCB.BIA\",\n",
    "        f\"CAS0MGXRAP_{y}{doy:03d}0000_01D_01D_DCB.BSX\",\n",
    "        f\"GFZ0OPSRAP_{y}{doy:03d}0000_01D_01D_DCB.BIA\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def pick_daily_file(y: int, doy: int) -> Path | None:\n",
    "    \"\"\"\n",
    "    Look for a daily file (.BIA/.BSX or .gz) in DAILY_DIR matching the IAAC patterns.\n",
    "    Returns a Path or None if nothing is found.\n",
    "    \"\"\"\n",
    "    for stem in fn_candidates(y, doy):\n",
    "        p = DAILY_DIR / stem\n",
    "        if p.exists():\n",
    "            return p\n",
    "        gz = p.with_suffix(p.suffix + \".gz\")\n",
    "        if gz.exists():\n",
    "            return gz\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_from_monthlies(month_p1c1: dict | None,\n",
    "                         month_p1p2: dict | None) -> list[tuple[str, float, str]]:\n",
    "    \"\"\"\n",
    "    Build a list of (prn, value_m, source) from monthly CODE products.\n",
    "\n",
    "    Uses the relation:\n",
    "      C2W–C1C = (P1–C1) – (P1–P2)\n",
    "    when both monthly tables are available.\n",
    "    \"\"\"\n",
    "    rows: list[tuple[str, float, str]] = []\n",
    "\n",
    "    if not (month_p1c1 and month_p1p2):\n",
    "        return rows\n",
    "\n",
    "    common = set(month_p1c1.keys()) & set(month_p1p2.keys())\n",
    "    for prn in sorted(common):\n",
    "        val = month_p1c1[prn] - month_p1p2[prn]  # meters\n",
    "        rows.append((prn, float(val), \"derived:monthly(P1-C1)-(P1-P2)\"))\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN LOGIC\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    records: list[dict] = []\n",
    "    monthly_cache: dict = {}\n",
    "\n",
    "    cur = START\n",
    "    while cur <= END:\n",
    "        y = cur.year\n",
    "        doy = int(cur.strftime(\"%j\"))\n",
    "        m = cur.month\n",
    "\n",
    "        daily_path = pick_daily_file(y, doy)\n",
    "\n",
    "        # Load monthly DCBs for this year/month (used if daily is missing or incomplete)\n",
    "        month_p1c1, month_p1p2, p1c1_path, p1p2_path = cache_monthly(monthly_cache, y, m)\n",
    "\n",
    "        if daily_path:\n",
    "            # 1) Try daily SINEX-based table, possibly backed by monthlies\n",
    "            try:\n",
    "                rows = bsx_mod.parse_sinex_bias(str(daily_path))\n",
    "            except Exception as exc:  # noqa: F841\n",
    "                rows = []\n",
    "\n",
    "            tbl = bsx_mod.build_table(\n",
    "                rows,\n",
    "                prefer=PREFER_ORDER,\n",
    "                monthly_p1c1=month_p1c1,\n",
    "                monthly_p1p2=None,  # we only use P1C1 here; P1P2 goes through build_from_monthlies if needed\n",
    "            )\n",
    "\n",
    "            if not tbl:\n",
    "                # Fallback: no usable SINEX rows, derive from monthlies only (if available)\n",
    "                tbl = build_from_monthlies(month_p1c1, month_p1p2)\n",
    "\n",
    "            for prn, val_m, src in tbl:\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"date\": cur.isoformat(),\n",
    "                        \"year\": y,\n",
    "                        \"doy\": doy,\n",
    "                        \"prn\": prn,\n",
    "                        \"C2W-C1C_sat_m\": val_m,\n",
    "                        \"source\": src,\n",
    "                        \"daily_file\": daily_path.name if daily_path else \"\",\n",
    "                        \"monthly_p1c1\": p1c1_path.name if p1c1_path else \"\",\n",
    "                        \"monthly_p1p2\": p1p2_path.name if p1p2_path else \"\",\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            # No daily file: build from monthlies if possible\n",
    "            tbl = build_from_monthlies(month_p1c1, month_p1p2)\n",
    "            for prn, val_m, src in tbl:\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"date\": cur.isoformat(),\n",
    "                        \"year\": y,\n",
    "                        \"doy\": doy,\n",
    "                        \"prn\": prn,\n",
    "                        \"C2W-C1C_sat_m\": val_m,\n",
    "                        \"source\": src,\n",
    "                        \"daily_file\": \"\",\n",
    "                        \"monthly_p1c1\": p1c1_path.name if p1c1_path else \"\",\n",
    "                        \"monthly_p1p2\": p1p2_path.name if p1p2_path else \"\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "    # Build DataFrame and export\n",
    "    df = pd.DataFrame.from_records(\n",
    "        records,\n",
    "        columns=[\n",
    "            \"date\",\n",
    "            \"year\",\n",
    "            \"doy\",\n",
    "            \"prn\",\n",
    "            \"C2W-C1C_sat_m\",\n",
    "            \"source\",\n",
    "            \"daily_file\",\n",
    "            \"monthly_p1c1\",\n",
    "            \"monthly_p1p2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"Rows: {len(df)} | written: {OUT_CSV}\")\n",
    "\n",
    "    # Quick summary by source\n",
    "    print(\"\\nCounts by source:\")\n",
    "    print(df[\"source\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9cf84-2917-465e-8da0-bac6b11a3be9",
   "metadata": {},
   "source": [
    "Generate a navigation master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71db1e7-3d3f-4282-992b-6e558f103560",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mini-batch RINEX-2 Navigation --> single DataFrame (and optional CSV).\n",
    "\n",
    "- Scans a directory for RINEX-2 navigation files (*.??n / *.n, case-insensitive)\n",
    "- Uses the parser in nav_rinex2_to_df.py\n",
    "- Outputs a master DataFrame with one row per (PRN, toc)\n",
    "- Optionally writes the result to a CSV file\n",
    "\n",
    "Adapt the CONFIG section to match your local layout.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONFIG (EDIT FOR YOUR SETUP)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Base project directory (defaults to folder containing this script)\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "\n",
    "# Directory containing RINEX-2 navigation files (*.??n / *.n)\n",
    "IN_DIR = PROJECT_ROOT / \"data\" / \"rinex_nav\"\n",
    "\n",
    "# Output CSV (if WRITE_CSV=True)\n",
    "OUT_CSV = PROJECT_ROOT / \"outputs\" / \"all_nav_master.csv\"\n",
    "\n",
    "# Whether to search subdirectories\n",
    "RECURSIVE = False  # True = include subfolders\n",
    "\n",
    "# Whether to write CSV to disk\n",
    "WRITE_CSV = True\n",
    "\n",
    "# Import the navigation parser module from this repository\n",
    "# Make sure nav_rinex2_to_df.py is in the same folder (or on PYTHONPATH)\n",
    "import nav_rinex2_to_df as nav_mod  # type: ignore\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN LOGIC\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def collect_nav_files(in_dir: Path, recursive: bool = False) -> list[Path]:\n",
    "    \"\"\"Collect RINEX-2 NAV files (*.??n, *.n) from a directory.\"\"\"\n",
    "    if not in_dir.is_dir():\n",
    "        raise SystemExit(f\"Input directory not found: {in_dir}\")\n",
    "\n",
    "    patterns = [\"*.??n\", \"*.n\", \"*.??N\", \"*.N\"]\n",
    "    files: set[Path] = set()\n",
    "\n",
    "    for pat in patterns:\n",
    "        if recursive:\n",
    "            files.update(p for p in in_dir.rglob(pat) if p.is_file())\n",
    "        else:\n",
    "            files.update(p for p in in_dir.glob(pat) if p.is_file())\n",
    "\n",
    "    files_sorted = sorted(p.resolve() for p in files)\n",
    "    print(f\"Found NAV files: {len(files_sorted)}\")\n",
    "    return files_sorted\n",
    "\n",
    "\n",
    "def parse_nav_files(files: list[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a list of RINEX-2 NAV files with nav_mod.parse_rinex2_nav()\n",
    "    and aggregate into a single DataFrame.\n",
    "    \"\"\"\n",
    "    rows_all: list[dict] = []\n",
    "\n",
    "    for p in files:\n",
    "        try:\n",
    "            ephs = nav_mod.parse_rinex2_nav(str(p))  # list of dicts\n",
    "            if not ephs:\n",
    "                print(f\"Warning: no ephemerides read from: {p.name}\")\n",
    "                continue\n",
    "\n",
    "            for e in ephs:\n",
    "                d = dict(e)\n",
    "                # Convert datetime → ISO 8601 string for 'toc'\n",
    "                if \"toc\" in d and hasattr(d[\"toc\"], \"isoformat\"):\n",
    "                    d[\"toc\"] = d[\"toc\"].isoformat()\n",
    "                d[\"nav_file\"] = p.name  # traceability\n",
    "                rows_all.append(d)\n",
    "\n",
    "        except Exception as ex:  # noqa: BLE001\n",
    "            print(f\"Warning: error while parsing {p.name}: {ex}\")\n",
    "\n",
    "    # Expected columns (order) based on parser structure\n",
    "    cols_order = [\n",
    "        \"prn\", \"toc\", \"af0\", \"af1\", \"af2\",\n",
    "        \"IODE\", \"Crs\", \"d_n\", \"M0\", \"Cuc\", \"e\", \"Cus\", \"sqrtA\",\n",
    "        \"Toe\", \"Cic\", \"Omega0\", \"Cis\", \"i0\", \"Crc\", \"omega\", \"OMEGA_DOT\",\n",
    "        \"IDOT\", \"week\", \"sv_accuracy\", \"sv_health\", \"Tgd\", \"IODC\",\n",
    "        \"TxTime\", \"fit_int\",\n",
    "        \"nav_file\",\n",
    "    ]\n",
    "\n",
    "    df_nav = pd.DataFrame(rows_all)\n",
    "\n",
    "    if df_nav.empty:\n",
    "        print(\"No ephemerides parsed; DataFrame is empty.\")\n",
    "        return df_nav\n",
    "\n",
    "    # Keep only known columns (if some are missing/NaN it's not fatal)\n",
    "    df_nav = df_nav[[c for c in cols_order if c in df_nav.columns]].copy()\n",
    "\n",
    "    # De-duplicate: one row per (PRN, toc)\n",
    "    if {\"prn\", \"toc\"}.issubset(df_nav.columns):\n",
    "        before = len(df_nav)\n",
    "        df_nav.drop_duplicates(subset=[\"prn\", \"toc\"], inplace=True)\n",
    "        after = len(df_nav)\n",
    "        if after != before:\n",
    "            print(f\"Deduplication: {before} → {after} rows (key: prn+toc)\")\n",
    "\n",
    "        df_nav.sort_values([\"prn\", \"toc\"], inplace=True, ignore_index=True)\n",
    "\n",
    "    print(f\"Aggregated rows: {len(df_nav)}\")\n",
    "    return df_nav\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    files = collect_nav_files(IN_DIR, recursive=RECURSIVE)\n",
    "    df_nav = parse_nav_files(files)\n",
    "\n",
    "    # Show a small preview in the console\n",
    "    if not df_nav.empty:\n",
    "        print(\"\\nHead of NAV master DataFrame:\")\n",
    "        print(df_nav.head(20))\n",
    "\n",
    "    # Optional: write a single CSV to disk\n",
    "    if WRITE_CSV and not df_nav.empty:\n",
    "        OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_nav.to_csv(OUT_CSV, index=False)\n",
    "        print(f\"\\nCSV written: {OUT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a0afa-49cc-49cf-aef7-c0af086ab476",
   "metadata": {},
   "source": [
    "Generate Parquet files from RINEX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82431ebe-d2a7-4889-b841-0f0b39e9fbe3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build lean observation master from RINEX-2 OBS files\n",
    "#\n",
    "# This cell:\n",
    "#   - Scans a directory for RINEX-2 observation files (*.??o / *.o)\n",
    "#   - Uses `parse_rinex2_obs_lean()` to extract a minimal observable:\n",
    "#       time (UTC), prn, P2_minus_C1_m\n",
    "#   - Aggregates results into a single DataFrame\n",
    "#   - Optionally writes CSV and Parquet master files\n",
    "#\n",
    "# Requirements:\n",
    "#   - \"rinex2_obs_lean.py\" in the same folder (or on PYTHONPATH)\n",
    "#   - The input directory contains standard RINEX-2 observation files\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from rinex2_obs_lean import parse_rinex2_obs_lean  # local parser module\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONFIG (EDIT THESE PATHS / FLAGS FOR YOUR SETUP)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Base project directory: for a notebook, this is usually the current working dir\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Directory containing *.??o / *.o RINEX observation files\n",
    "IN_DIR = PROJECT_ROOT / \"data\" / \"rinex_obs\"  # adapt to your layout\n",
    "\n",
    "# Whether to search subdirectories\n",
    "RECURSIVE = True  # set to False to only scan IN_DIR\n",
    "\n",
    "# Output files\n",
    "WRITE_CSV = True\n",
    "WRITE_PARQUET = True\n",
    "OUT_MASTER_CSV = PROJECT_ROOT / \"outputs\" / \"obs_master_lean.csv\"\n",
    "OUT_PARQUET = PROJECT_ROOT / \"outputs\" / \"obs_master_lean.parquet\"\n",
    "\n",
    "# Optional: restrict processing to a slice of the file list\n",
    "# Set END_INDEX = None to process all files from START_INDEX\n",
    "START_INDEX = 0\n",
    "END_INDEX = None  # e.g., 278 to mimic files[1200:1478] with different offsets\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# COLLECT RINEX-2 OBS FILES\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "patterns = [\"*.??o\", \"*.o\", \"*.??O\", \"*.O\"]\n",
    "files = []\n",
    "\n",
    "for pat in patterns:\n",
    "    if RECURSIVE:\n",
    "        files.extend(IN_DIR.rglob(pat))\n",
    "    else:\n",
    "        files.extend(IN_DIR.glob(pat))\n",
    "\n",
    "files = sorted({p.resolve() for p in files if p.is_file()})\n",
    "print(f\"Found OBS files: {len(files)}\")\n",
    "\n",
    "# Apply optional slice\n",
    "if END_INDEX is not None:\n",
    "    files_to_process = files[START_INDEX:END_INDEX]\n",
    "else:\n",
    "    files_to_process = files[START_INDEX:]\n",
    "\n",
    "print(f\"Files to process: {len(files_to_process)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# PARSE OBS FILES AND BUILD MASTER DATAFRAME\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, p in enumerate(files_to_process, 1):\n",
    "    try:\n",
    "        df = parse_rinex2_obs_lean(str(p))\n",
    "        if not df.empty:\n",
    "            df[\"source_file\"] = p.name  # traceability\n",
    "            df[\"source_file\"] = df[\"source_file\"].astype(\"category\")\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"  - {p.name}: no readable C1/P2 observations\")\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Warning on {p.name}: {exc}\")\n",
    "\n",
    "if dfs:\n",
    "    obs_lean = pd.concat(dfs, ignore_index=True)\n",
    "    obs_lean.sort_values([\"time\", \"prn\"], inplace=True, ignore_index=True)\n",
    "    print(f\"Total rows: {len(obs_lean)}\")\n",
    "else:\n",
    "    obs_lean = pd.DataFrame(\n",
    "        columns=[\"time\", \"prn\", \"P2_minus_C1_m\", \"source_file\"]\n",
    "    )\n",
    "    print(\"No data parsed.\")\n",
    "\n",
    "# Preview in the notebook\n",
    "try:\n",
    "    from IPython.display import display  # type: ignore\n",
    "    display(obs_lean.head(20))\n",
    "except Exception:\n",
    "    print(obs_lean.head(20))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# WRITE MASTER FILES (CSV / PARQUET)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if (WRITE_CSV or WRITE_PARQUET) and not obs_lean.empty:\n",
    "    if WRITE_CSV:\n",
    "        OUT_MASTER_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "        obs_lean.to_csv(OUT_MASTER_CSV, index=False)\n",
    "        print(f\"CSV written: {OUT_MASTER_CSV}\")\n",
    "\n",
    "    if WRITE_PARQUET:\n",
    "        OUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "        obs_lean.to_parquet(OUT_PARQUET, index=False)\n",
    "        print(f\"Parquet written: {OUT_PARQUET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df674f81-c5d9-42dc-971b-7b5b7f2b0ced",
   "metadata": {},
   "source": [
    "Generate Receiver bias per day over a period: WLS vs Minimum-Scalloping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337409e-2bde-4192-ac87-9dd7050fa067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimate one daily receiver code bias using two methods:\n",
    "  - Weighted Least Squares (WLS, two-pass with residual clipping)\n",
    "  - Minimum-Scalloping (MS) using nighttime VTEC smoothness\n",
    "\n",
    "Inputs (expected layout, adapt as needed):\n",
    "\n",
    "- Observation master (Parquet/CSV):\n",
    "    columns: time (UTC, ISO8601), prn, P2_minus_C1_m\n",
    "    directory: data/obs_master/\n",
    "\n",
    "- Navigation master (CSV):\n",
    "    columns: prn, toc, week, Toe, sqrtA, d_n, e, M0, Cuc, Cus, Crc, Crs, Cic, Cis, i0, IDOT, Omega0, OMEGA_DOT, omega\n",
    "    path: data/nav/NEW_all_nav_master.csv\n",
    "\n",
    "- DCB master (CSV):\n",
    "    columns: date (or year + doy), prn, C2W-C1C_sat_m\n",
    "    path: data/dcb/NEW_all_dcb_master.csv\n",
    "\n",
    "Output:\n",
    "\n",
    "- CSV with one row per day:\n",
    "    date, D_rx_WLS_m, D_rx_MS_m, delta_MS_minus_WLS_m, N_obs_pass1, N_obs_pass2, N_epochs, Nsat_mean, MS_score_min\n",
    "\n",
    "This script is self-contained except for the input files.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "\n",
    "# Observation master directory (Parquet and/or CSV)\n",
    "OBS_DIR = PROJECT_ROOT / \"data\" / \"obs_master\"\n",
    "\n",
    "# Navigation master CSV (broadcast ephemerides)\n",
    "NAV_FILE = PROJECT_ROOT / \"data\" / \"nav\" / \"NEW_all_nav_master.csv\"\n",
    "\n",
    "# DCB master CSV (satellite code biases)\n",
    "DCB_FILE = PROJECT_ROOT / \"data\" / \"dcb\" / \"NEW_all_dcb_master.csv\"\n",
    "\n",
    "# Output CSV for daily receiver biases\n",
    "OUT_DRX_CSV = PROJECT_ROOT / \"outputs\" / \"Drx_WLS_vs_MS_daily.csv\"\n",
    "\n",
    "# Date range (UTC)\n",
    "DATE_START = \"2015-10-01\"\n",
    "DATE_END = \"2025-09-26\"\n",
    "\n",
    "# Receiver coordinates (ECEF, meters)\n",
    "RX_XYZ = (5411106.3084, -747628.5974, 3286931.3223)\n",
    "\n",
    "# Geometry / QC parameters\n",
    "ELEV_MIN_DEG = 10.0\n",
    "H_IONO = 350_000.0\n",
    "R_EARTH = 6_378_137.0\n",
    "MAX_RESID_M = 5.0\n",
    "MAX_VTEC_TECU = 200.0\n",
    "MERGE_TOL = \"2h\"  # obs-ephemeris nearest tolerance\n",
    "\n",
    "# GPS L1/L2\n",
    "f1 = 1575.42e6\n",
    "f2 = 1227.60e6\n",
    "K_m_per_TECU = 40.3 * (1.0 / f2**2 - 1.0 / f1**2)  # ~0.105 m / TECU\n",
    "\n",
    "# ============================================================================\n",
    "# NAV / DCB LOADING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_nav(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load broadcast ephemeris master CSV.\n",
    "\n",
    "    Expected columns:\n",
    "        prn, toc (ISO8601), week, Toe, sqrtA, d_n, e, M0,\n",
    "        Cuc, Cus, Crc, Crs, Cic, Cis, i0, IDOT, Omega0, OMEGA_DOT, omega\n",
    "    \"\"\"\n",
    "    nav = pd.read_csv(path)\n",
    "    nav[\"prn\"] = nav[\"prn\"].astype(str).str.upper().str.strip()\n",
    "    nav[\"toc\"] = pd.to_datetime(nav[\"toc\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    # Reference GPS time epoch\n",
    "    gps_epoch = pd.to_datetime(\"1980-01-06\", utc=True)\n",
    "\n",
    "    nav[\"ephem_time\"] = gps_epoch + pd.to_timedelta(nav[\"week\"] * 7, unit=\"D\") + pd.to_timedelta(\n",
    "        nav[\"Toe\"], unit=\"s\"\n",
    "    )\n",
    "\n",
    "    keep = [\n",
    "        \"prn\",\n",
    "        \"ephem_time\",\n",
    "        \"week\",\n",
    "        \"Toe\",\n",
    "        \"sqrtA\",\n",
    "        \"d_n\",\n",
    "        \"e\",\n",
    "        \"M0\",\n",
    "        \"Cuc\",\n",
    "        \"Cus\",\n",
    "        \"Crc\",\n",
    "        \"Crs\",\n",
    "        \"Cic\",\n",
    "        \"Cis\",\n",
    "        \"i0\",\n",
    "        \"IDOT\",\n",
    "        \"Omega0\",\n",
    "        \"OMEGA_DOT\",\n",
    "        \"omega\",\n",
    "    ]\n",
    "    nav = nav[keep].sort_values([\"prn\", \"ephem_time\"]).reset_index(drop=True)\n",
    "    return nav\n",
    "\n",
    "\n",
    "def load_dcb(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load satellite DCB master CSV.\n",
    "\n",
    "    Expected columns:\n",
    "        - Either: 'date' (YYYY-MM-DD) and 'prn'\n",
    "        - Or: 'year', 'doy' and 'prn'\n",
    "        - 'C2W-C1C_sat_m' (meters)\n",
    "    \"\"\"\n",
    "    dcb = pd.read_csv(path)\n",
    "    dcb[\"prn\"] = dcb[\"prn\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    if \"date\" in dcb.columns:\n",
    "        dcb[\"dcb_date\"] = pd.to_datetime(dcb[\"date\"], utc=True).dt.floor(\"D\")\n",
    "    else:\n",
    "        base = pd.to_datetime(dcb[\"year\"].astype(int).astype(str), format=\"%Y\", utc=True)\n",
    "        dcb[\"dcb_date\"] = (base + pd.to_timedelta(dcb[\"doy\"].astype(int) - 1, unit=\"D\")).dt.floor(\"D\")\n",
    "\n",
    "    dcb = dcb.rename(columns={\"C2W-C1C_sat_m\": \"sat_dcb_m\"})\n",
    "    return dcb[[\"prn\", \"dcb_date\", \"sat_dcb_m\"]]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# OBSERVATION LOADING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def _read_day_slice(fp: Path, t0: pd.Timestamp, t1: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a time slice [t0, t1) from a Parquet or CSV file,\n",
    "    keeping only 'time', 'prn', 'P2_minus_C1_m'.\n",
    "    \"\"\"\n",
    "    cols = [\"time\", \"prn\", \"P2_minus_C1_m\"]\n",
    "\n",
    "    if fp.suffix.lower() == \".parquet\":\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                fp,\n",
    "                columns=cols,\n",
    "                filters=[(\"time\", \">=\", t0), (\"time\", \"<\", t1)],\n",
    "            )\n",
    "        except Exception:  # noqa: BLE001\n",
    "            df = pd.read_parquet(fp, columns=cols)\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "            df = df[(df[\"time\"] >= t0) & (df[\"time\"] < t1)]\n",
    "    else:\n",
    "        df = pd.read_csv(fp, usecols=cols, parse_dates=[\"time\"])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "        df = df[(df[\"time\"] >= t0) & (df[\"time\"] < t1)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def stack_obs_for_day(t0: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Stack all observation files for a given UTC day into a single DataFrame.\n",
    "    \"\"\"\n",
    "    t1 = t0 + pd.Timedelta(days=1)\n",
    "    files = sorted(list(OBS_DIR.glob(\"*.parquet\")) + list(OBS_DIR.glob(\"*.csv\")))\n",
    "\n",
    "    if not files:\n",
    "        raise SystemExit(f\"No observation files found in: {OBS_DIR}\")\n",
    "\n",
    "    parts: list[pd.DataFrame] = []\n",
    "    for fp in files:\n",
    "        df = _read_day_slice(fp, t0, t1)\n",
    "        if df is not None and len(df):\n",
    "            parts.append(df)\n",
    "\n",
    "    if not parts:\n",
    "        return None\n",
    "\n",
    "    obs = pd.concat(parts, ignore_index=True)\n",
    "    obs[\"prn\"] = obs[\"prn\"].astype(str).str.upper().str.strip()\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"], utc=True)\n",
    "    obs = obs.dropna(subset=[\"time\", \"prn\", \"P2_minus_C1_m\"])\n",
    "\n",
    "    return obs.sort_values([\"prn\", \"time\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GEOMETRY\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def ecef_to_geodetic(x: float, y: float, z: float) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Convert ECEF (x, y, z) -> (lat, lon, h) in radians, radians, meters.\n",
    "    WGS84 ellipsoid.\n",
    "    \"\"\"\n",
    "    a = 6378137.0\n",
    "    f = 1.0 / 298.257223563\n",
    "    e2 = f * (2.0 - f)\n",
    "    b = a * (1.0 - f)\n",
    "    ep2 = (a * a - b * b) / (b * b)\n",
    "\n",
    "    r = math.hypot(x, y)\n",
    "    E2 = a * a - b * b\n",
    "    F = 54.0 * b * b * z * z\n",
    "    G = r * r + (1.0 - e2) * z * z - e2 * E2\n",
    "    c = (e2 * e2 * F * r * r) / (G * G * G)\n",
    "    s = (1.0 + c + math.sqrt(c * c + 2.0 * c)) ** (1.0 / 3.0)\n",
    "    P = F / (3.0 * (s + 1.0 / s + 1.0) ** 2 * G * G)\n",
    "    Q = math.sqrt(1.0 + 2.0 * e2 * e2 * P)\n",
    "    r0 = -(P * e2 * r) / (1.0 + Q) + math.sqrt(\n",
    "        0.5 * a * a * (1.0 + 1.0 / Q)\n",
    "        - (P * (1.0 - e2) * z * z) / (Q * (1.0 + Q))\n",
    "        - 0.5 * P * r * r\n",
    "    )\n",
    "    U = math.sqrt((r - e2 * r0) ** 2 + z * z)\n",
    "    V = math.sqrt((r - e2 * r0) ** 2 + (1.0 - e2) * z * z)\n",
    "    Z0 = b * b * z / (a * V)\n",
    "    h = U * (1.0 - b * b / (a * V))\n",
    "    lat = math.atan2(z + ep2 * Z0, r)\n",
    "    lon = math.atan2(y, x)\n",
    "    return lat, lon, h\n",
    "\n",
    "\n",
    "def elevation_deg(rx_xyz: tuple[float, float, float], sat_xyz: tuple[float, float, float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute elevation angle (degrees) of satellite relative to receiver.\n",
    "    \"\"\"\n",
    "    x, y, z = rx_xyz\n",
    "    lat, lon, _ = ecef_to_geodetic(x, y, z)\n",
    "\n",
    "    sl, cl = math.sin(lat), math.cos(lat)\n",
    "    slon, clon = math.sin(lon), math.cos(lon)\n",
    "\n",
    "    R = np.array(\n",
    "        [\n",
    "            [-slon, clon, 0.0],\n",
    "            [-clon * sl, -slon * sl, cl],\n",
    "            [clon * cl, slon * cl, sl],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    d = np.array([sat_xyz[0] - x, sat_xyz[1] - y, sat_xyz[2] - z])\n",
    "    e, n, u = R @ d\n",
    "    return math.degrees(math.atan2(u, math.hypot(e, n)))\n",
    "\n",
    "\n",
    "def sat_ecef_from_navrow(t_utc: pd.Timestamp, rnav: pd.Series) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute satellite ECEF position from a NAV ephemeris row at time t_utc.\n",
    "    \"\"\"\n",
    "    MU = 3.986005e14\n",
    "    OMEGA_E = 7.2921151467e-5\n",
    "\n",
    "    t = pd.Timestamp(t_utc).tz_convert(\"UTC\")\n",
    "    toe_time = pd.to_datetime(rnav[\"ephem_time\"], utc=True)\n",
    "    tk = (t - toe_time).total_seconds()\n",
    "\n",
    "    half = 302400.0\n",
    "    if tk > half:\n",
    "        tk -= 2.0 * half\n",
    "    if tk < -half:\n",
    "        tk += 2.0 * half\n",
    "\n",
    "    A = float(rnav[\"sqrtA\"]) ** 2\n",
    "    dn = float(rnav[\"d_n\"])\n",
    "    ecc = float(rnav[\"e\"])\n",
    "    M0 = float(rnav[\"M0\"])\n",
    "    Cuc = float(rnav[\"Cuc\"])\n",
    "    Cus = float(rnav[\"Cus\"])\n",
    "    Crc = float(rnav[\"Crc\"])\n",
    "    Crs = float(rnav[\"Crs\"])\n",
    "    Cic = float(rnav[\"Cic\"])\n",
    "    Cis = float(rnav[\"Cis\"])\n",
    "    i0 = float(rnav[\"i0\"])\n",
    "    IDOT = float(rnav[\"IDOT\"])\n",
    "    OMG0 = float(rnav[\"Omega0\"])\n",
    "    OMG_DOT = float(rnav[\"OMEGA_DOT\"])\n",
    "    omg = float(rnav[\"omega\"])\n",
    "    toe_s = float(rnav[\"Toe\"])\n",
    "\n",
    "    n0 = math.sqrt(MU / A**3)\n",
    "    n = n0 + dn\n",
    "    M = M0 + n * tk\n",
    "\n",
    "    # Solve Kepler's equation for E\n",
    "    E = M\n",
    "    for _ in range(12):\n",
    "        dE = (M + ecc * math.sin(E) - E) / (1.0 - ecc * math.cos(E))\n",
    "        E += dE\n",
    "        if abs(dE) < 1e-13:\n",
    "            break\n",
    "\n",
    "    v = math.atan2(math.sqrt(1.0 - ecc * ecc) * math.sin(E), math.cos(E) - ecc)\n",
    "    phi = v + omg\n",
    "\n",
    "    du = Cuc * math.cos(2.0 * phi) + Cus * math.sin(2.0 * phi)\n",
    "    dr = Crc * math.cos(2.0 * phi) + Crs * math.sin(2.0 * phi)\n",
    "    di = Cic * math.cos(2.0 * phi) + Cis * math.sin(2.0 * phi)\n",
    "\n",
    "    u = phi + du\n",
    "    r = A * (1.0 - ecc * math.cos(E)) + dr\n",
    "    i = i0 + IDOT * tk + di\n",
    "\n",
    "    x_orb = r * math.cos(u)\n",
    "    y_orb = r * math.sin(u)\n",
    "\n",
    "    OMG = OMG0 + (OMG_DOT - OMEGA_E) * tk - OMEGA_E * toe_s\n",
    "    cosO, sinO = math.cos(OMG), math.sin(OMG)\n",
    "    cosi, sini = math.cos(i), math.sin(i)\n",
    "\n",
    "    x = x_orb * cosO - y_orb * cosi * sinO\n",
    "    y = x_orb * sinO + y_orb * cosi * cosO\n",
    "    z = y_orb * sini\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def merge_nav(\n",
    "    obs_day: pd.DataFrame, nav: pd.DataFrame, tol: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge observation and navigation by PRN via nearest-time (merge_asof).\n",
    "    \"\"\"\n",
    "    out: list[pd.DataFrame] = []\n",
    "\n",
    "    for prn, left in obs_day.groupby(\"prn\", sort=False):\n",
    "        right = nav[nav[\"prn\"] == prn]\n",
    "        if right.empty:\n",
    "            continue\n",
    "\n",
    "        left = left.sort_values(\"time\")\n",
    "        right = right.sort_values(\"ephem_time\")\n",
    "\n",
    "        kw: Dict[str, Any] = {\"direction\": \"nearest\"}\n",
    "        if tol is not None:\n",
    "            kw[\"tolerance\"] = pd.Timedelta(tol)\n",
    "\n",
    "        m = pd.merge_asof(\n",
    "            left=left,\n",
    "            right=right,\n",
    "            left_on=\"time\",\n",
    "            right_on=\"ephem_time\",\n",
    "            **kw,\n",
    "        )\n",
    "        if m is None or m.empty:\n",
    "            continue\n",
    "\n",
    "        if \"prn_x\" in m.columns and \"prn\" not in m.columns:\n",
    "            m = m.rename(columns={\"prn_x\": \"prn\"})\n",
    "        if \"prn_y\" in m.columns:\n",
    "            m = m.drop(columns=[\"prn_y\"])\n",
    "\n",
    "        out.append(m)\n",
    "\n",
    "    if not out:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAPPING + DCB\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def mapping_M(el_deg: np.ndarray, Re: float = R_EARTH, H: float = H_IONO) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple thin-shell mapping function M(elevation) = 1 / sqrt(1 - (Re / (Re + H) * cos el)^2).\n",
    "    \"\"\"\n",
    "    el = np.radians(el_deg)\n",
    "    arg = (Re * np.cos(el)) / (Re + H)\n",
    "    M = 1.0 / np.sqrt(1.0 - np.clip(arg * arg, 0.0, 0.9999))\n",
    "    return np.clip(M, 1.0, 20.0)\n",
    "\n",
    "\n",
    "def attach_sat_dcb(m: pd.DataFrame, dcb_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Join satellite DCBs and build y_m = (P2-C1) - DCB_sat.\n",
    "    \"\"\"\n",
    "    mm = m.merge(dcb_day, on=\"prn\", how=\"inner\").dropna(subset=[\"sat_dcb_m\"])\n",
    "    if mm.empty:\n",
    "        return mm\n",
    "    mm[\"y_m\"] = mm[\"P2_minus_C1_m\"] - mm[\"sat_dcb_m\"]\n",
    "    return mm\n",
    "\n",
    "\n",
    "def mapping_block(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add mapping function M, coefficient P = K * M, and weight w = 1 / M^2.\n",
    "    \"\"\"\n",
    "    df[\"M\"] = mapping_M(df[\"elev_deg\"].values)\n",
    "    df[\"P\"] = K_m_per_TECU * df[\"M\"].values\n",
    "    df[\"w\"] = 1.0 / (df[\"M\"].values**2)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# WLS (CLOSED FORM, TWO PASSES)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def solve_closed_form(df: pd.DataFrame) -> tuple[Optional[float], Optional[pd.DataFrame], Optional[dict]]:\n",
    "    \"\"\"\n",
    "    Closed-form solution for daily bias D and per-epoch VTEC\n",
    "    using the block structure across satellites and epochs.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[\"y_m\", \"M\", \"P\", \"w\", \"time\"])\n",
    "    if df.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    d = df.assign(\n",
    "        wP=df[\"w\"] * df[\"P\"],\n",
    "        wP2=df[\"w\"] * (df[\"P\"] ** 2),\n",
    "        wy=df[\"w\"] * df[\"y_m\"],\n",
    "        wPy=df[\"w\"] * df[\"P\"] * df[\"y_m\"],\n",
    "    )\n",
    "\n",
    "    G = d.groupby(\"time\", sort=True)[[\"w\", \"wP\", \"wP2\", \"wPy\"]].sum()\n",
    "    S0, S1, S2, b_t = G[\"w\"], G[\"wP\"], G[\"wP2\"], G[\"wPy\"]\n",
    "    b0 = d[\"wy\"].sum()\n",
    "\n",
    "    valid = (S2 > 0) & (S0 >= 3)\n",
    "    if not valid.any():\n",
    "        return None, None, None\n",
    "\n",
    "    S0, S1, S2, b_t = S0[valid], S1[valid], S2[valid], b_t[valid]\n",
    "    num = b0 - np.sum(S1 * b_t / S2)\n",
    "    den = np.sum(S0) - np.sum((S1**2) / S2)\n",
    "\n",
    "    if not np.isfinite(den) or den == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    D = num / den\n",
    "    V_t = (b_t - S1 * D) / S2\n",
    "\n",
    "    times = pd.DatetimeIndex(G.index[valid])\n",
    "    if times.tz is None:\n",
    "        times = times.tz_localize(\"UTC\")\n",
    "    times = times.tz_convert(\"UTC\").rename(None)\n",
    "\n",
    "    vt = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": times.to_numpy(copy=False),\n",
    "            \"VTEC_TECU\": np.asarray(V_t, dtype=float),\n",
    "        }\n",
    "    )\n",
    "    vt.sort_values(by=\"time\", inplace=True, kind=\"mergesort\")\n",
    "    vt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return float(D), vt, {\"N_obs\": int(len(df))}\n",
    "\n",
    "\n",
    "def wls_two_pass(\n",
    "    df: pd.DataFrame,\n",
    "    max_resid_m: float = MAX_RESID_M,\n",
    ") -> tuple[Optional[float], Optional[pd.DataFrame], dict]:\n",
    "    \"\"\"\n",
    "    Two-pass WLS:\n",
    "      1) initial fit via closed form\n",
    "      2) residual-based clipping and refit\n",
    "    \"\"\"\n",
    "    D1, vt1, _ = solve_closed_form(df)\n",
    "    if D1 is None:\n",
    "        return None, None, {}\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "    vt1[\"time\"] = pd.to_datetime(vt1[\"time\"], utc=True)\n",
    "\n",
    "    vt1_idx = vt1.set_index(\"time\")[\"VTEC_TECU\"]\n",
    "    d1 = df.join(vt1_idx, on=\"time\", how=\"inner\")\n",
    "\n",
    "    pred = D1 + d1[\"P\"].values * d1[\"VTEC_TECU\"].values\n",
    "    resid = d1[\"y_m\"].values - pred\n",
    "\n",
    "    keep = np.isfinite(resid) & (np.abs(resid) <= max_resid_m)\n",
    "    d2 = d1.loc[keep].copy()\n",
    "\n",
    "    D2, vt2, diag = solve_closed_form(d2)\n",
    "    if D2 is None:\n",
    "        return None, None, {}\n",
    "\n",
    "    vt2 = vt2[(vt2[\"VTEC_TECU\"] >= 0.0) & (vt2[\"VTEC_TECU\"] <= MAX_VTEC_TECU)].copy()\n",
    "\n",
    "    diag = {\n",
    "        \"N_obs_pass1\": int(len(d1)),\n",
    "        \"N_obs_pass2\": int(len(d2)),\n",
    "        \"N_epochs\": int(len(vt2)),\n",
    "        \"Nsat_mean\": float(d2.groupby(\"time\").size().mean()) if len(d2) else np.nan,\n",
    "    }\n",
    "    return float(D2), vt2, diag\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MINIMUM-SCALLOPING (MS)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def _scalloping_score(D: float, df: pd.DataFrame, night_only: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Measure nocturnal \"ripples\" of VTEC for a given D.\n",
    "    Lower score = smoother (preferred).\n",
    "    \"\"\"\n",
    "    vtec_i = (df[\"y_m\"].values - D) / df[\"P\"].values\n",
    "    tmp = pd.DataFrame({\"time\": pd.to_datetime(df[\"time\"], utc=True), \"vtec\": vtec_i})\n",
    "\n",
    "    if night_only:\n",
    "        lt = tmp[\"time\"].dt.tz_convert(\"Africa/Casablanca\").dt.hour\n",
    "        tmp = tmp[(lt <= 4) | (lt >= 22)]\n",
    "        if tmp.empty:\n",
    "            return float(\"inf\")\n",
    "\n",
    "    def mad(a: np.ndarray) -> float:\n",
    "        m = np.median(a)\n",
    "        return float(np.median(np.abs(a - m)))\n",
    "\n",
    "    disp = tmp.groupby(\"time\")[\"vtec\"].apply(lambda a: mad(a.to_numpy()))\n",
    "    disp = disp[np.isfinite(disp)]\n",
    "\n",
    "    if not len(disp):\n",
    "        return float(\"inf\")\n",
    "    return float(np.median(disp))\n",
    "\n",
    "\n",
    "def minimum_scalloping_D(\n",
    "    df: pd.DataFrame,\n",
    "    D_init: Optional[float] = None,\n",
    "    span_m: float = 10.0,\n",
    "    step_m: float = 0.1,\n",
    "    night_only: bool = True,\n",
    ") -> tuple[Optional[float], dict]:\n",
    "    \"\"\"\n",
    "    Grid-search around D_init to minimize nocturnal scalloping.\n",
    "    \"\"\"\n",
    "    if D_init is None:\n",
    "        D0, _, _ = solve_closed_form(df)\n",
    "        if D0 is None:\n",
    "            return None, {}\n",
    "        D_init = D0\n",
    "\n",
    "    grid = np.arange(D_init - span_m, D_init + span_m + step_m, step_m)\n",
    "    scores = [_scalloping_score(D, df, night_only=night_only) for D in grid]\n",
    "\n",
    "    k = int(np.argmin(scores))\n",
    "    return float(grid[k]), {\n",
    "        \"MS_score_min\": float(scores[k]),\n",
    "        \"MS_grid_step\": float(step_m),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ONE DAY WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def drx_for_one_day(\n",
    "    day_str: str,\n",
    "    elev_min_deg: float = ELEV_MIN_DEG,\n",
    "    night_only: bool = True,\n",
    "    nav: Optional[pd.DataFrame] = None,\n",
    "    dcb: Optional[pd.DataFrame] = None,\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Estimate daily receiver bias for a single UTC day.\n",
    "    \"\"\"\n",
    "    if nav is None:\n",
    "        nav = load_nav(NAV_FILE)\n",
    "    if dcb is None:\n",
    "        dcb = load_dcb(DCB_FILE)\n",
    "\n",
    "    t0 = pd.Timestamp(day_str, tz=\"UTC\").floor(\"D\")\n",
    "\n",
    "    obs_day = stack_obs_for_day(t0)\n",
    "    if obs_day is None or obs_day.empty:\n",
    "        return None\n",
    "\n",
    "    m = merge_nav(obs_day, nav, tol=MERGE_TOL)\n",
    "    if m is None or m.empty:\n",
    "        return None\n",
    "\n",
    "    dcb_day = dcb[dcb[\"dcb_date\"] == t0.floor(\"D\")][[\"prn\", \"sat_dcb_m\"]]\n",
    "    if dcb_day.empty:\n",
    "        return None\n",
    "\n",
    "    # Satellite positions and elevation\n",
    "    sv = np.vstack([sat_ecef_from_navrow(row[\"time\"], row) for _, row in m.iterrows()])\n",
    "    m[\"sv_x\"], m[\"sv_y\"], m[\"sv_z\"] = sv[:, 0], sv[:, 1], sv[:, 2]\n",
    "    m[\"elev_deg\"] = [\n",
    "        elevation_deg(RX_XYZ, (x, y, z)) for x, y, z in zip(m[\"sv_x\"], m[\"sv_y\"], m[\"sv_z\"])\n",
    "    ]\n",
    "\n",
    "    m = attach_sat_dcb(m, dcb_day)\n",
    "    if m.empty:\n",
    "        return None\n",
    "\n",
    "    m = m[m[\"elev_deg\"] >= float(elev_min_deg)].copy()\n",
    "    m = mapping_block(m)\n",
    "\n",
    "    D_wls, vt, diag = wls_two_pass(m, max_resid_m=MAX_RESID_M)\n",
    "    if D_wls is None:\n",
    "        return None\n",
    "\n",
    "    D_ms, info_ms = minimum_scalloping_D(\n",
    "        m,\n",
    "        D_init=D_wls,\n",
    "        span_m=10.0,\n",
    "        step_m=0.1,\n",
    "        night_only=night_only,\n",
    "    )\n",
    "\n",
    "    delta = (D_ms - D_wls) if D_ms is not None else np.nan\n",
    "\n",
    "    return {\n",
    "        \"date\": t0.date(),\n",
    "        \"D_rx_WLS_m\": float(D_wls),\n",
    "        \"D_rx_MS_m\": float(D_ms) if D_ms is not None else np.nan,\n",
    "        \"delta_MS_minus_WLS_m\": float(delta) if np.isfinite(delta) else np.nan,\n",
    "        \"N_obs_pass1\": diag.get(\"N_obs_pass1\", np.nan),\n",
    "        \"N_obs_pass2\": diag.get(\"N_obs_pass2\", np.nan),\n",
    "        \"N_epochs\": diag.get(\"N_epochs\", np.nan),\n",
    "        \"Nsat_mean\": diag.get(\"Nsat_mean\", np.nan),\n",
    "        \"MS_score_min\": info_ms.get(\"MS_score_min\", np.nan),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PERIOD RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def run_period(\n",
    "    date_start: str = DATE_START,\n",
    "    date_end: str = DATE_END,\n",
    "    out_csv: Path = OUT_DRX_CSV,\n",
    "    elev_min_deg: float = ELEV_MIN_DEG,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loop over a date range and compute daily receiver bias for each day.\n",
    "    \"\"\"\n",
    "    nav = load_nav(NAV_FILE)\n",
    "    dcb = load_dcb(DCB_FILE)\n",
    "\n",
    "    days = pd.date_range(\n",
    "        pd.Timestamp(date_start, tz=\"UTC\"),\n",
    "        pd.Timestamp(date_end, tz=\"UTC\"),\n",
    "        freq=\"D\",\n",
    "    )\n",
    "\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for t0 in days:\n",
    "        day_txt = str(t0.date())\n",
    "        try:\n",
    "            rec = drx_for_one_day(\n",
    "                day_txt,\n",
    "                elev_min_deg=elev_min_deg,\n",
    "                night_only=True,\n",
    "                nav=nav,\n",
    "                dcb=dcb,\n",
    "            )\n",
    "            if rec is None:\n",
    "                print(f\"{day_txt}: no result (missing obs/nav/dcb or no solution).\")\n",
    "                continue\n",
    "\n",
    "            rows.append(rec)\n",
    "            print(\n",
    "                f\"{day_txt}  WLS={rec['D_rx_WLS_m']:.3f}  \"\n",
    "                f\"MS={rec['D_rx_MS_m']:.3f}  \"\n",
    "                f\"Δ={rec['delta_MS_minus_WLS_m']:.3f}\"\n",
    "            )\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"{day_txt}: error → {exc}\")\n",
    "\n",
    "    if rows:\n",
    "        out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df = pd.DataFrame(rows).sort_values(\"date\")\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"\\nSaved receiver-bias table to: {out_csv}\")\n",
    "    else:\n",
    "        print(\"No rows produced for the requested period.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"display.width\", 160)\n",
    "    pd.set_option(\"display.max_columns\", 20)\n",
    "    run_period()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169760db-7fef-4c9d-9060-62722e8d9883",
   "metadata": {},
   "source": [
    "Generate a vtec_gim dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adac0a-2c38-486f-bc62-b1ff0c7ff710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample co-located Global Ionospheric Maps (GIM, IONEX format) at a given station latitude/longitude and build a continuous 30-minute VTEC \n",
    "time series over a specified date range.\n",
    "\n",
    "Outputs a CSV:\n",
    "    time (UTC), vtec_gim (TECU)\n",
    "\n",
    "Assumptions:\n",
    "- IONEX files are already decompressed into a single directory.\n",
    "- Both legacy CODG (short) and new COD0OPSFIN/COD0OPSRAP (long-name) formats\n",
    "  may be present; the script chooses the appropriate product per date.\n",
    "- TEC is stored as integers multiplied by 10^EXPONENT (standard IONEX).\n",
    "\n",
    "Dependencies:\n",
    "    numpy, pandas\n",
    "\n",
    "Author: Mohamed Kaab (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import subprocess\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "\n",
    "# Directory containing ALL decompressed IONEX files (.i / .INX, possibly .Z/.gz)\n",
    "DEC_DIR = PROJECT_ROOT / \"data\" / \"ionex_decompressed\"\n",
    "\n",
    "# Output CSV\n",
    "OUT_CSV = PROJECT_ROOT / \"outputs\" / \"VTEC_GIM_30min_20151001_20250926.csv\"\n",
    "\n",
    "# Station coordinates (geodetic degrees)\n",
    "STA_LAT = 31.206\n",
    "STA_LON = -7.866\n",
    "\n",
    "# Date range (inclusive)\n",
    "START_DATE = date(2015, 10, 1)\n",
    "END_DATE = date(2025, 9, 26)\n",
    "\n",
    "# =============================================================================\n",
    "# LOW-LEVEL I/O HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def _open_text(p: Path) -> io.StringIO:\n",
    "    \"\"\"\n",
    "    Open an IONEX/IONEX-compressed file as text and return a StringIO.\n",
    "\n",
    "    Supports:\n",
    "      - plain text (.i, .INX)\n",
    "      - .gz via `gzip -dc`\n",
    "      - .Z via `gzip -dc` first, then `7z e -so` as fallback\n",
    "\n",
    "    Requires `gzip` (and optionally `7z`) in PATH for compressed files.\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "    ext = p.suffix.lower()\n",
    "\n",
    "    if ext == \".gz\":\n",
    "        out = subprocess.run([\"gzip\", \"-dc\", str(p)], capture_output=True)\n",
    "        if out.returncode != 0:\n",
    "            raise RuntimeError(f\"gzip -dc failed on {p}\")\n",
    "        return io.StringIO(out.stdout.decode(\"ascii\", \"ignore\"))\n",
    "\n",
    "    if ext == \".z\":\n",
    "        # Try gzip first\n",
    "        gz = subprocess.run([\"gzip\", \"-dc\", str(p)], capture_output=True)\n",
    "        if gz.returncode == 0 and gz.stdout:\n",
    "            return io.StringIO(gz.stdout.decode(\"ascii\", \"ignore\"))\n",
    "\n",
    "        # Fallback to 7z\n",
    "        sz = subprocess.run([\"7z\", \"e\", \"-so\", str(p)], capture_output=True)\n",
    "        if sz.returncode != 0:\n",
    "            raise RuntimeError(f\"7z -so failed on {p}\")\n",
    "        return io.StringIO(sz.stdout.decode(\"ascii\", \"ignore\"))\n",
    "\n",
    "    # Plain text\n",
    "    return io.StringIO(p.read_text(encoding=\"ascii\", errors=\"ignore\"))\n",
    "\n",
    "\n",
    "def ionex_first_epoch_date(path: Path) -> Optional[date]:\n",
    "    \"\"\"\n",
    "    Read an IONEX file and return the date of the first map epoch,\n",
    "    based on the 'EPOCH OF FIRST MAP' header line.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = _open_text(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    for _ in range(400):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if \"EPOCH OF FIRST MAP\" in line:\n",
    "            yr, mo, dy, hh, mm, ss = map(int, line[:60].split()[:6])\n",
    "            return datetime(yr, mo, dy, hh, mm, ss, tzinfo=timezone.utc).date()\n",
    "        if \"END OF HEADER\" in line:\n",
    "            break\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IONEX PARSING\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def read_ionex(path: Path) -> Tuple[pd.DatetimeIndex, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parse a (possibly compressed) IONEX file and return:\n",
    "\n",
    "        times (UTC DatetimeIndex), lats (deg), lons (deg), TEC (array)\n",
    "    \n",
    "    where:\n",
    "        TEC has shape (Ntimes, Nlat, Nlon)\n",
    "        TEC is in TECU (floats, NaN for missing grid points).\n",
    "    \"\"\"\n",
    "    f = _open_text(path)\n",
    "\n",
    "    exp = -1\n",
    "    lat1 = lat2 = dlat = None\n",
    "    lon1 = lon2 = dlon = None\n",
    "\n",
    "    # --- header ---\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            raise ValueError(\"Incomplete IONEX header\")\n",
    "\n",
    "        if \"EXPONENT\" in line:\n",
    "            s = line[:8].strip()\n",
    "            exp = int(s) if s else -1\n",
    "\n",
    "        if \"LAT1 / LAT2 / DLAT\" in line:\n",
    "            lat1, lat2, dlat = map(float, line[:60].split()[:3])\n",
    "\n",
    "        if \"LON1 / LON2 / DLON\" in line:\n",
    "            lon1, lon2, dlon = map(float, line[:60].split()[:3])\n",
    "\n",
    "        if \"END OF HEADER\" in line:\n",
    "            break\n",
    "\n",
    "    if None in (lat1, lat2, dlat, lon1, lon2, dlon):\n",
    "        raise ValueError(\"Missing IONEX grid definition\")\n",
    "\n",
    "    nlat = int(round((lat2 - lat1) / dlat)) + 1\n",
    "    nlon = int(round((lon2 - lon1) / dlon)) + 1\n",
    "    lats = np.linspace(lat1, lat2, nlat)\n",
    "    lons = np.linspace(lon1, lon2, nlon)\n",
    "\n",
    "    times: List[pd.Timestamp] = []\n",
    "    maps: List[np.ndarray] = []\n",
    "\n",
    "    # --- TEC maps ---\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"START OF TEC MAP\" not in line:\n",
    "            continue\n",
    "\n",
    "        # Find EPOCH OF CURRENT MAP\n",
    "        line = f.readline()\n",
    "        while line and \"EPOCH OF CURRENT MAP\" not in line:\n",
    "            line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        yr, mo, dy, hh, mm, ss = map(int, line[:60].split()[:6])\n",
    "        t = pd.Timestamp(datetime(yr, mo, dy, hh, mm, ss, tzinfo=timezone.utc))\n",
    "\n",
    "        tec_map = np.full((nlat, nlon), np.nan, dtype=float)\n",
    "        bad = False\n",
    "\n",
    "        # One record per latitude\n",
    "        for ilat in range(nlat):\n",
    "            hdr = f.readline()\n",
    "            if not hdr or \"LAT/LON1/LON2/DLON/H\" not in hdr:\n",
    "                bad = True\n",
    "                break\n",
    "\n",
    "            vals: List[float] = []\n",
    "            while len(vals) < nlon:\n",
    "                data = f.readline()\n",
    "                if (\n",
    "                    not data\n",
    "                    or (\"START OF\" in data)\n",
    "                    or (\"END OF\" in data)\n",
    "                    or (\"LAT/LON1\" in data)\n",
    "                ):\n",
    "                    bad = True\n",
    "                    break\n",
    "\n",
    "                chunks = [data[i : i + 5] for i in range(0, len(data.rstrip()), 5)]\n",
    "                for c in chunks:\n",
    "                    c = c.strip().upper()\n",
    "                    if c == \"\" or c == \"9999\":\n",
    "                        vals.append(np.nan)\n",
    "                    else:\n",
    "                        try:\n",
    "                            vals.append(float(c) * (10.0**exp))\n",
    "                        except Exception:\n",
    "                            vals.append(np.nan)\n",
    "                    if len(vals) == nlon:\n",
    "                        break\n",
    "\n",
    "            if bad:\n",
    "                break\n",
    "            if len(vals) < nlon:\n",
    "                vals += [np.nan] * (nlon - len(vals))\n",
    "\n",
    "            tec_map[ilat, :] = np.asarray(vals, dtype=float)\n",
    "\n",
    "        if bad:\n",
    "            # Skip until END OF TEC MAP\n",
    "            x = hdr\n",
    "            while x:\n",
    "                if \"END OF TEC MAP\" in x:\n",
    "                    break\n",
    "                x = f.readline()\n",
    "            continue\n",
    "\n",
    "        times.append(t)\n",
    "        maps.append(tec_map)\n",
    "\n",
    "    if not maps:\n",
    "        raise ValueError(f\"No TEC maps found in IONEX: {path}\")\n",
    "\n",
    "    TEC = np.stack(maps, axis=0)\n",
    "\n",
    "    # Normalize longitude range to [-180, 180) if needed\n",
    "    if float(np.nanmin(lons)) >= 0.0 and float(np.nanmax(lons)) > 180.0:\n",
    "        order = np.argsort(((lons + 180.0) % 360.0) - 180.0)\n",
    "        lons = (((lons + 180.0) % 360.0) - 180.0)[order]\n",
    "        TEC = TEC[:, :, order]\n",
    "\n",
    "    # Ensure latitudes increasing\n",
    "    if lats[0] > lats[-1]:\n",
    "        lats = lats[::-1]\n",
    "        TEC = TEC[:, ::-1, :]\n",
    "\n",
    "    times_idx = pd.to_datetime(times, utc=True)\n",
    "    return times_idx, lats, lons, TEC\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INTERPOLATION HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def bilinear(\n",
    "    lats: np.ndarray,\n",
    "    lons: np.ndarray,\n",
    "    grid: np.ndarray,\n",
    "    lat: float,\n",
    "    lon: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Simple bilinear interpolation on a regular lat/lon grid.\n",
    "\n",
    "    lats, lons are 1D arrays (ascending).\n",
    "    grid is 2D (nlat, nlon) matching lats/lons.\n",
    "    \"\"\"\n",
    "    i = np.searchsorted(lats, lat) - 1\n",
    "    j = np.searchsorted(lons, lon) - 1\n",
    "    i = np.clip(i, 0, len(lats) - 2)\n",
    "    j = np.clip(j, 0, len(lons) - 2)\n",
    "\n",
    "    y1, y2 = lats[i], lats[i + 1]\n",
    "    x1, x2 = lons[j], lons[j + 1]\n",
    "\n",
    "    Q11 = grid[i, j]\n",
    "    Q12 = grid[i, j + 1]\n",
    "    Q21 = grid[i + 1, j]\n",
    "    Q22 = grid[i + 1, j + 1]\n",
    "\n",
    "    if (x2 - x1) == 0.0 or (y2 - y1) == 0.0:\n",
    "        return float(Q11)\n",
    "\n",
    "    wx = (lon - x1) / (x2 - x1)\n",
    "    wy = (lat - y1) / (y2 - y1)\n",
    "\n",
    "    return float(\n",
    "        (1.0 - wx) * (1.0 - wy) * Q11\n",
    "        + wx * (1.0 - wy) * Q12\n",
    "        + (1.0 - wx) * wy * Q21\n",
    "        + wx * wy * Q22\n",
    "    )\n",
    "\n",
    "\n",
    "def to_utc_index(x) -> pd.DatetimeIndex:\n",
    "    idx = pd.DatetimeIndex(x)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        idx = idx.tz_convert(\"UTC\")\n",
    "    return idx.sort_values().unique()\n",
    "\n",
    "\n",
    "def make_30min_grid(day: date) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Return a 30-min UTC grid for a given date (48 slots).\n",
    "    \"\"\"\n",
    "    t0 = pd.Timestamp(day, tz=\"UTC\")\n",
    "    return pd.date_range(t0, periods=48, freq=\"30min\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRODUCT SELECTION FOR A GIVEN DATE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def product_window(day: date) -> str:\n",
    "    \"\"\"\n",
    "    Decide which CODG/COD0OPSFIN/COD0OPSRAP product window applies\n",
    "    for a given date.\n",
    "\n",
    "    Returns:\n",
    "        'OLD'    for legacy codgDDD0.yyi (short name),\n",
    "        'OPSFIN' for COD0OPSFIN_*_GIM.INX,\n",
    "        'OPSRAP' for COD0OPSRAP_*_GIM.INX.\n",
    "    \"\"\"\n",
    "    if day <= date(2022, 11, 27):  # inclusive: 2022-DOY330\n",
    "        return \"OLD\"\n",
    "    if day <= date(2025, 9, 20):   # inclusive: 2025-DOY263\n",
    "        return \"OPSFIN\"\n",
    "    return \"OPSRAP\"                # after → RAPID\n",
    "\n",
    "\n",
    "def pick_ionex_for_day(day: date) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Select the appropriate IONEX file in DEC_DIR for a given date.\n",
    "\n",
    "    Logic:\n",
    "      - For 'OLD', look for codgDDD0.yyi / CODGDDD0.yyi and validate by header.\n",
    "      - For 'OPSFIN' or 'OPSRAP', prefer COD0OPSFIN or COD0OPSRAP long-name\n",
    "        products for that year+DOY, again validated by header.\n",
    "    \"\"\"\n",
    "    yy = f\"{day.year % 100:02d}\"\n",
    "    doy = f\"{int(pd.Timestamp(day).strftime('%j')):03d}\"\n",
    "    mode = product_window(day)\n",
    "\n",
    "    # Legacy CODG (short name)\n",
    "    if mode == \"OLD\":\n",
    "        candidates = [f\"codg{doy}0.{yy}i\", f\"CODG{doy}0.{yy}I\"]\n",
    "        for name in candidates:\n",
    "            p = DEC_DIR / name\n",
    "            if p.exists() and ionex_first_epoch_date(p) == day:\n",
    "                return p\n",
    "\n",
    "        # If several codg*.i exist, scan by header\n",
    "        for q in DEC_DIR.glob(f\"codg*{yy}i\"):\n",
    "            if ionex_first_epoch_date(q) == day:\n",
    "                return q\n",
    "        for q in DEC_DIR.glob(f\"CODG*{yy}I\"):\n",
    "            if ionex_first_epoch_date(q) == day:\n",
    "                return q\n",
    "        return None\n",
    "\n",
    "    # Long-name products\n",
    "    patterns: list[str]\n",
    "    if mode == \"OPSFIN\":\n",
    "        patterns = [\n",
    "            f\"COD0OPSFIN_*{day.year}{doy}*_GIM.INX\",\n",
    "            f\"COD0OPSRAP_*{day.year}{doy}*_GIM.INX\",\n",
    "        ]\n",
    "    else:  # OPSRAP window\n",
    "        patterns = [\n",
    "            f\"COD0OPSRAP_*{day.year}{doy}*_GIM.INX\",\n",
    "            f\"COD0OPSFIN_*{day.year}{doy}*_GIM.INX\",\n",
    "        ]\n",
    "\n",
    "    for pat in patterns:\n",
    "        for q in DEC_DIR.glob(pat):\n",
    "            if ionex_first_epoch_date(q) == day:\n",
    "                return q\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def vtec_30min_from_ionex(\n",
    "    ionex_path: Path,\n",
    "    day: date,\n",
    "    lat: float,\n",
    "    lon: float,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Sample a single IONEX file at (lat, lon) and interpolate in time\n",
    "    to a 30-minute local grid for the given day.\n",
    "\n",
    "    Returns a Series with:\n",
    "        index: 30-min UTC timestamps,\n",
    "        values: vtec_gim (TECU).\n",
    "    \"\"\"\n",
    "    times, lats, lons, TEC = read_ionex(ionex_path)\n",
    "\n",
    "    # If IONEX longitudes are [0, 360], convert station longitude accordingly\n",
    "    lon_match = lon\n",
    "    if float(np.nanmin(lons)) >= 0.0 and float(np.nanmax(lons)) > 180.0:\n",
    "        lon_match = (lon + 360.0) % 360.0\n",
    "\n",
    "    vals = [bilinear(lats, lons, TEC[k], lat, lon_match) for k in range(len(times))]\n",
    "\n",
    "    ser = pd.Series(vals, index=to_utc_index(times), name=\"vtec_gim\")\n",
    "\n",
    "    # Interpolate onto local 30-min grid for this day\n",
    "    t30 = make_30min_grid(day)\n",
    "    union = ser.index.union(t30)\n",
    "    ser30 = ser.reindex(union).interpolate(\"time\").reindex(t30)\n",
    "\n",
    "    return ser30\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOOP\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    all_rows: list[pd.DataFrame] = []\n",
    "\n",
    "    d = START_DATE\n",
    "    while d <= END_DATE:\n",
    "        ionex_path = pick_ionex_for_day(d)\n",
    "\n",
    "        if ionex_path is None:\n",
    "            # Missing day → NaN series for this day\n",
    "            t30 = make_30min_grid(d)\n",
    "            df_day = pd.DataFrame({\"time\": t30, \"vtec_gim\": np.nan})\n",
    "            all_rows.append(df_day)\n",
    "            print(f\"{d} : no IONEX file found, filled with NaN\")\n",
    "        else:\n",
    "            ser30 = vtec_30min_from_ionex(ionex_path, d, STA_LAT, STA_LON)\n",
    "            df_day = pd.DataFrame({\"time\": ser30.index, \"vtec_gim\": ser30.values})\n",
    "            all_rows.append(df_day)\n",
    "            print(f\"{d} : processed {ionex_path.name}\")\n",
    "\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    df.sort_values(\"time\", inplace=True, ignore_index=True)\n",
    "\n",
    "    OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "    print(f\"\\nWritten: {OUT_CSV}  |  rows: {len(df)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e8b09-8c54-4231-9ea3-62e013e9abe3",
   "metadata": {},
   "source": [
    "Cellule produisant deux fichiers csv par technique WLS ou MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2e8d7-09c4-46a9-a824-cac02a980baf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VTEC statistics with fixed receiver bias (MS & WLS) --> four CSV products.\n",
    "\n",
    "This script:\n",
    "- Reads daily P2-C1 observations (Parquet/CSV) for one station.\n",
    "- Merges them with broadcast ephemerides and satellite DCBs.\n",
    "- Uses pre-estimated daily receiver biases (MS and WLS).\n",
    "- Computes:\n",
    "  * 30-min VTEC stats (two outputs: MS, WLS),\n",
    "  * daily VTEC stats (two outputs: MS, WLS),\n",
    "  * basic obs–ephemeris match statistics (one output).\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG (ADAPT THESE PATHS) ----------\n",
    "# Directory with observation files containing: time, prn, P2_minus_C1_m\n",
    "OBS_DIR = Path(\"data/obs\")  # e.g., Path(\"data/All_Obs_master/Parquet\")\n",
    "\n",
    "# Broadcast ephemeris master file\n",
    "NAV_FILE = Path(\"data/master/NEW_all_nav_master.csv\")\n",
    "\n",
    "# Satellite DCB master file (daily + monthly merged)\n",
    "DCB_FILE = Path(\"data/master/NEW_all_dcb_master.csv\")\n",
    "\n",
    "# Daily receiver biases (output of the WLS vs MS script)\n",
    "# Must contain columns: date, D_rx_MS_m, D_rx_WLS_m\n",
    "DRX_FILE = Path(\"data/master/Drx_WLS_vs_MS_daily.csv\")\n",
    "\n",
    "# 30-min GIM VTEC at station (optional, for offset diagnostics)\n",
    "GIM_FILE = Path(\"data/master/VTEC_GIM_30min_20151001_20250926.csv\")\n",
    "\n",
    "# Output CSV files\n",
    "OUT_30MIN_MS = Path(\"outputs/2015_2025_MS_VTEC_30min_stats.csv\")\n",
    "OUT_30MIN_WL = Path(\"outputs/2015_2025_WLS_VTEC_30min_stats.csv\")\n",
    "OUT_DAILY_MS = Path(\"outputs/2015_2025_MS_VTEC_daily_stats.csv\")\n",
    "OUT_DAILY_WL = Path(\"outputs/2015_2025_WLS_VTEC_daily_stats.csv\")\n",
    "OUT_MATCH    = Path(\"outputs/2015_2025_MS_WLS_match_rates.csv\")\n",
    "\n",
    "# Date range to process\n",
    "DATE_START = \"2015-10-01\"\n",
    "DATE_END   = \"2025-09-26\"\n",
    "\n",
    "# Receiver ECEF coordinates (m)\n",
    "RX_XYZ = (5411106.3084, -747628.5974, 3286931.3223)\n",
    "\n",
    "# Elevation masks (deg)\n",
    "ELEV_MIN_DEG_BASE  = 10.0\n",
    "ELEV_MIN_DEG_BOOST = 15.0\n",
    "\n",
    "# Earth and ionospheric shell\n",
    "R_EARTH = 6378137.0\n",
    "H_IONO  = 350_000.0\n",
    "\n",
    "# GPS L1/L2 frequencies\n",
    "f1 = 1575.42e6\n",
    "f2 = 1227.60e6\n",
    "\n",
    "# Conversion factor: meters per TECU for P2-C1 combination\n",
    "# (TEC in TECU, 1 TECU = 1e16 el/m^2)\n",
    "K_m_per_TECU = 40.3e16 * (1.0 / f2**2 - 1.0 / f1**2)\n",
    "\n",
    "# Obs–ephemeris merge tolerance\n",
    "MERGE_TOL = \"2h\"\n",
    "\n",
    "# VTEC bounds and outlier control\n",
    "MAX_VTEC_TECU       = 200.0\n",
    "Hampel_k            = 3.0\n",
    "GIM_OFFSET_FLAG_TECU = 20.0\n",
    "\n",
    "# ---------- GEO / NAV GEOMETRY ----------\n",
    "\n",
    "def ecef_to_geodetic(x: float, y: float, z: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"ECEF → (lat, lon, h) using a standard WGS-84 approximation.\"\"\"\n",
    "    a = 6378137.0\n",
    "    f = 1 / 298.257223563\n",
    "    e2 = f * (2 - f)\n",
    "    b = a * (1 - f)\n",
    "    ep2 = (a * a - b * b) / (b * b)\n",
    "    r = math.hypot(x, y)\n",
    "    E2 = a * a - b * b\n",
    "    F = 54 * b * b * z * z\n",
    "    G = r * r + (1 - e2) * z * z - e2 * E2\n",
    "    c = (e2 * e2 * F * r * r) / (G * G * G)\n",
    "    s = (1 + c + math.sqrt(c * c + 2 * c)) ** (1 / 3)\n",
    "    P = F / (3 * (s + 1 / s + 1) ** 2 * G * G)\n",
    "    Q = math.sqrt(1 + 2 * e2 * e2 * P)\n",
    "    r0 = -(P * e2 * r) / (1 + Q) + math.sqrt(\n",
    "        0.5 * a * a * (1 + 1 / Q) - (P * (1 - e2) * z * z) / (Q * (1 + Q)) - 0.5 * P * r * r\n",
    "    )\n",
    "    U = math.sqrt((r - e2 * r0) ** 2 + z * z)\n",
    "    V = math.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n",
    "    Z0 = b * b * z / (a * V)\n",
    "    h = U * (1 - b * b / (a * V))\n",
    "    lat = math.atan2(z + ep2 * Z0, r)\n",
    "    lon = math.atan2(y, x)\n",
    "    return lat, lon, h\n",
    "\n",
    "\n",
    "def elevation_deg(rx_xyz: Tuple[float, float, float],\n",
    "                  sat_xyz: Tuple[float, float, float]) -> float:\n",
    "    \"\"\"Elevation angle (deg) from receiver to satellite in ECEF.\"\"\"\n",
    "    x, y, z = rx_xyz\n",
    "    lat, lon, _ = ecef_to_geodetic(x, y, z)\n",
    "    sl, cl = math.sin(lat), math.cos(lat)\n",
    "    slon, clon = math.sin(lon), math.cos(lon)\n",
    "    R = np.array(\n",
    "        [\n",
    "            [-slon,        clon,       0.0],\n",
    "            [-clon * sl, -slon * sl,   cl ],\n",
    "            [ clon * cl,  slon * cl,   sl ],\n",
    "        ]\n",
    "    )\n",
    "    d = np.array([sat_xyz[0] - x, sat_xyz[1] - y, sat_xyz[2] - z])\n",
    "    e, n, u = R @ d\n",
    "    return math.degrees(math.atan2(u, math.hypot(e, n)))\n",
    "\n",
    "\n",
    "def mapping_M(el_deg: np.ndarray,\n",
    "              Re: float = R_EARTH,\n",
    "              H: float = H_IONO) -> np.ndarray:\n",
    "    \"\"\"Thin-shell mapping function M(elev).\"\"\"\n",
    "    el = np.radians(el_deg)\n",
    "    sin_zp = (Re / (Re + H)) * np.cos(el)\n",
    "    sin_zp = np.clip(sin_zp, -1.0, 1.0)\n",
    "    zp = np.arcsin(sin_zp)\n",
    "    return 1.0 / np.cos(zp)\n",
    "\n",
    "\n",
    "def sat_ecef_from_navrow(t_utc: pd.Timestamp,\n",
    "                         rnav: pd.Series) -> Tuple[float, float, float]:\n",
    "    \"\"\"Compute satellite ECEF from broadcast ephemeris row at time t_utc.\"\"\"\n",
    "    MU = 3.986005e14\n",
    "    OMEGA_E = 7.2921151467e-5\n",
    "\n",
    "    t = pd.Timestamp(t_utc)\n",
    "    t = t.tz_localize(\"UTC\") if t.tzinfo is None else t.tz_convert(\"UTC\")\n",
    "    toe_time = pd.to_datetime(rnav[\"ephem_time\"], utc=True)\n",
    "    tk = (t - toe_time).total_seconds()\n",
    "    half = 302400.0\n",
    "    if tk > half:\n",
    "        tk -= 2 * half\n",
    "    if tk < -half:\n",
    "        tk += 2 * half\n",
    "\n",
    "    A = float(rnav[\"sqrtA\"]) ** 2\n",
    "    dn = float(rnav[\"d_n\"])\n",
    "    ecc = float(rnav[\"e\"])\n",
    "    M0 = float(rnav[\"M0\"])\n",
    "    Cuc = float(rnav[\"Cuc\"])\n",
    "    Cus = float(rnav[\"Cus\"])\n",
    "    Crc = float(rnav[\"Crc\"])\n",
    "    Crs = float(rnav[\"Crs\"])\n",
    "    Cic = float(rnav[\"Cic\"])\n",
    "    Cis = float(rnav[\"Cis\"])\n",
    "    i0 = float(rnav[\"i0\"])\n",
    "    IDOT = float(rnav[\"IDOT\"])\n",
    "    OMG0 = float(rnav[\"Omega0\"])\n",
    "    OMG_DOT = float(rnav[\"OMEGA_DOT\"])\n",
    "    omg = float(rnav[\"omega\"])\n",
    "    toe_s = float(rnav[\"Toe\"])\n",
    "\n",
    "    n0 = math.sqrt(MU / A**3)\n",
    "    n = n0 + dn\n",
    "    M = M0 + n * tk\n",
    "\n",
    "    E = M\n",
    "    for _ in range(12):\n",
    "        dE = (M + ecc * math.sin(E) - E) / (1 - ecc * math.cos(E))\n",
    "        E += dE\n",
    "        if abs(dE) < 1e-13:\n",
    "            break\n",
    "\n",
    "    v = math.atan2(math.sqrt(1 - ecc * ecc) * math.sin(E),\n",
    "                   math.cos(E) - ecc)\n",
    "    phi = v + omg\n",
    "\n",
    "    du = Cuc * math.cos(2 * phi) + Cus * math.sin(2 * phi)\n",
    "    dr = Crc * math.cos(2 * phi) + Crs * math.sin(2 * phi)\n",
    "    di = Cic * math.cos(2 * phi) + Cis * math.sin(2 * phi)\n",
    "\n",
    "    u = phi + du\n",
    "    r = A * (1 - ecc * math.cos(E)) + dr\n",
    "    i = i0 + IDOT * tk + di\n",
    "\n",
    "    x_orb = r * math.cos(u)\n",
    "    y_orb = r * math.sin(u)\n",
    "\n",
    "    OMG = OMG0 + (OMG_DOT - OMEGA_E) * tk - OMEGA_E * toe_s\n",
    "    cosO, sinO = math.cos(OMG), math.sin(OMG)\n",
    "    cosi, sini = math.cos(i), math.sin(i)\n",
    "\n",
    "    x = x_orb * cosO - y_orb * cosi * sinO\n",
    "    y = x_orb * sinO + y_orb * cosi * cosO\n",
    "    z = y_orb * sini\n",
    "    return x, y, z\n",
    "\n",
    "# ---------- I/O HELPERS ----------\n",
    "\n",
    "def load_nav(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load merged RINEX-2 NAV master CSV.\"\"\"\n",
    "    nav = pd.read_csv(path)\n",
    "    nav[\"prn\"] = nav[\"prn\"].str.upper().str.strip()\n",
    "    nav[\"toc\"] = pd.to_datetime(nav[\"toc\"], utc=True, errors=\"coerce\")\n",
    "    nav[\"ephem_time\"] = (\n",
    "        pd.to_datetime(\"1980-01-06\", utc=True)\n",
    "        + pd.to_timedelta(nav[\"week\"] * 7, unit=\"D\")\n",
    "        + pd.to_timedelta(nav[\"Toe\"], unit=\"s\")\n",
    "    )\n",
    "    keep = [\n",
    "        \"prn\", \"ephem_time\", \"week\", \"Toe\", \"sqrtA\", \"d_n\", \"e\", \"M0\",\n",
    "        \"Cuc\", \"Cus\", \"Crc\", \"Crs\", \"Cic\", \"Cis\", \"i0\", \"IDOT\",\n",
    "        \"Omega0\", \"OMEGA_DOT\", \"omega\"\n",
    "    ]\n",
    "    return nav[keep].sort_values([\"prn\", \"ephem_time\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def load_dcb(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load satellite DCB master (daily C2W-C1C).\"\"\"\n",
    "    dcb = pd.read_csv(path)\n",
    "    dcb[\"prn\"] = dcb[\"prn\"].str.upper().str.strip()\n",
    "    base = pd.to_datetime(dcb[\"year\"].astype(int).astype(str),\n",
    "                          format=\"%Y\", utc=True)\n",
    "    dcb[\"dcb_date\"] = (\n",
    "        base + pd.to_timedelta(dcb[\"doy\"].astype(int) - 1, unit=\"D\")\n",
    "    ).dt.floor(\"D\")\n",
    "    dcb = dcb.rename(columns={\"C2W-C1C_sat_m\": \"sat_dcb_m\"})\n",
    "    return dcb[[\"prn\", \"dcb_date\", \"sat_dcb_m\"]]\n",
    "\n",
    "\n",
    "def load_drx(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load daily receiver bias file (must have date, D_rx_MS_m, D_rx_WLS_m).\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True).dt.floor(\"D\")\n",
    "\n",
    "    # Try to standardize column names if slightly different\n",
    "    if \"D_rx_MS_m\" not in df.columns:\n",
    "        c = [c for c in df.columns if c.lower().startswith(\"d_rx_ms\")] \\\n",
    "            or [c for c in df.columns if \"MS\" in c]\n",
    "        if c:\n",
    "            df = df.rename(columns={c[0]: \"D_rx_MS_m\"})\n",
    "    if \"D_rx_WLS_m\" not in df.columns:\n",
    "        c = [c for c in df.columns if c.lower().startswith(\"d_rx_wls\")] \\\n",
    "            or [c for c in df.columns if \"WLS\" in c]\n",
    "        if c:\n",
    "            df = df.rename(columns={c[0]: \"D_rx_WLS_m\"})\n",
    "\n",
    "    required = {\"date\", \"D_rx_MS_m\", \"D_rx_WLS_m\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(\"DRX_FILE must have: date, D_rx_MS_m, D_rx_WLS_m\")\n",
    "\n",
    "    return df[list(required)]\n",
    "\n",
    "\n",
    "# Pre-resolve observation files\n",
    "OBS_FILES: List[Path] = sorted(\n",
    "    [*Path(OBS_DIR).glob(\"*.parquet\"), *Path(OBS_DIR).glob(\"*.csv\")]\n",
    ")\n",
    "print(f\"Found observation files: {len(OBS_FILES)}\")\n",
    "\n",
    "\n",
    "def _read_day_slice(fp: Path,\n",
    "                    t0: pd.Timestamp,\n",
    "                    t1: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Read a single-day slice from one obs file.\"\"\"\n",
    "    cols = [\"time\", \"prn\", \"P2_minus_C1_m\"]\n",
    "    if fp.suffix.lower() == \".parquet\":\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                fp,\n",
    "                columns=cols,\n",
    "                engine=\"pyarrow\",\n",
    "                filters=[(\"time\", \">=\", t0), (\"time\", \"<\", t1)],\n",
    "            )\n",
    "        except Exception:\n",
    "            df = pd.read_parquet(fp, columns=cols)\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "            df = df[(df[\"time\"] >= t0) & (df[\"time\"] < t1)]\n",
    "    else:\n",
    "        df = pd.read_csv(fp, usecols=cols, parse_dates=[\"time\"])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "        df = df[(df[\"time\"] >= t0) & (df[\"time\"] < t1)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def stack_obs_for_day(t0: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Stack all obs files for a given UTC day [t0, t0+1).\"\"\"\n",
    "    t1 = t0 + pd.Timedelta(days=1)\n",
    "    parts = []\n",
    "    for fp in OBS_FILES:\n",
    "        df = _read_day_slice(fp, t0, t1)\n",
    "        if df is not None and len(df):\n",
    "            parts.append(df)\n",
    "    if not parts:\n",
    "        return None\n",
    "    obs = pd.concat(parts, ignore_index=True)\n",
    "    obs[\"prn\"] = obs[\"prn\"].str.upper().str.strip()\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"], utc=True)\n",
    "    obs = obs.dropna(subset=[\"time\", \"prn\", \"P2_minus_C1_m\"])\n",
    "    return obs.sort_values([\"prn\", \"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def merge_nav(obs_day: pd.DataFrame,\n",
    "              nav: pd.DataFrame,\n",
    "              tol: Optional[str]) -> pd.DataFrame:\n",
    "    \"\"\"Merge observations with ephemerides via nearest-ephem time per PRN.\"\"\"\n",
    "    if obs_day is None or obs_day.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = []\n",
    "    for prn_val, left in obs_day.groupby(\"prn\", sort=False):\n",
    "        right = nav[nav[\"prn\"] == prn_val]\n",
    "        if right.empty:\n",
    "            continue\n",
    "        left = left.sort_values(\"time\", kind=\"mergesort\")\n",
    "        right = right.sort_values(\"ephem_time\", kind=\"mergesort\")\n",
    "        merge_kwargs: Dict[str, Any] = dict(direction=\"nearest\")\n",
    "        if tol:\n",
    "            merge_kwargs[\"tolerance\"] = pd.Timedelta(tol)\n",
    "        m = pd.merge_asof(\n",
    "            left=left,\n",
    "            right=right,\n",
    "            left_on=\"time\",\n",
    "            right_on=\"ephem_time\",\n",
    "            **merge_kwargs,\n",
    "        )\n",
    "        if m is None or m.empty:\n",
    "            continue\n",
    "        if \"prn_x\" in m.columns and \"prn\" not in m.columns:\n",
    "            m = m.rename(columns={\"prn_x\": \"prn\"})\n",
    "        if \"prn_y\" in m.columns:\n",
    "            m = m.drop(columns=[\"prn_y\"])\n",
    "        if \"prn\" not in m.columns:\n",
    "            m[\"prn\"] = prn_val\n",
    "        out.append(m)\n",
    "\n",
    "    if not out:\n",
    "        return pd.DataFrame()\n",
    "    m_all = pd.concat(out, ignore_index=True)\n",
    "    return m_all.dropna(subset=[\"ephem_time\"]).copy()\n",
    "\n",
    "# ---------- DCB & MAPPING ----------\n",
    "\n",
    "def attach_sat_dcb(m: pd.DataFrame,\n",
    "                   dcb_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Attach satellite DCB and form the corrected observable y_m.\"\"\"\n",
    "    m2 = m.merge(dcb_day, on=\"prn\", how=\"inner\").dropna(subset=[\"sat_dcb_m\"])\n",
    "    if m2.empty:\n",
    "        return m2\n",
    "    m2[\"y_m\"] = m2[\"P2_minus_C1_m\"] - m2[\"sat_dcb_m\"]\n",
    "    return m2\n",
    "\n",
    "\n",
    "def mapping_block(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add mapping M and factor P = K*M used for VTEC inversion.\"\"\"\n",
    "    df[\"M\"] = mapping_M(df[\"elev_deg\"].values)\n",
    "    df[\"P\"] = K_m_per_TECU * df[\"M\"].values\n",
    "    return df\n",
    "\n",
    "# ---------- GIM HANDLING ----------\n",
    "\n",
    "def load_gim(path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load 30-min GIM VTEC at the station (optional).\"\"\"\n",
    "    if path is None or not path.exists():\n",
    "        return None\n",
    "    g = pd.read_csv(path, parse_dates=[\"time\"])\n",
    "    g[\"time\"] = pd.to_datetime(g[\"time\"], utc=True)\n",
    "    return g[[\"time\", \"vtec_gim\"]]\n",
    "\n",
    "\n",
    "def daily_gim_offset(vt_df: pd.DataFrame,\n",
    "                     gim: Optional[pd.DataFrame]) -> Optional[float]:\n",
    "    \"\"\"Median daily VTEC offset between local VTEC and GIM.\"\"\"\n",
    "    if gim is None or vt_df is None or vt_df.empty:\n",
    "        return None\n",
    "    g = gim.set_index(\"time\")[\"vtec_gim\"].sort_index()\n",
    "    v = vt_df.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "    g_aligned = g.reindex(\n",
    "        v.index, method=\"nearest\", tolerance=pd.Timedelta(\"15min\")\n",
    "    )\n",
    "    d = (v - g_aligned).dropna()\n",
    "    return float(d.median()) if not d.empty else None\n",
    "\n",
    "# ---------- VTEC FROM FIXED D_rx ----------\n",
    "\n",
    "def vtec_from_fixed_drx(df_day: pd.DataFrame,\n",
    "                        D_rx_m: float,\n",
    "                        elev_min: float) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute per-epoch VTEC with a fixed D_rx (MS or WLS) and elevation mask.\n",
    "    Returns:\n",
    "      - vt: DataFrame(time, VTEC_TECU)\n",
    "      - diag: basic diagnostics\n",
    "    \"\"\"\n",
    "    df = df_day[df_day[\"elev_deg\"] >= elev_min].copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    df = mapping_block(df)\n",
    "    vtec_i = (df[\"y_m\"].values - D_rx_m) / df[\"P\"].values\n",
    "    df[\"VTEC_i\"] = vtec_i\n",
    "    df = df[(df[\"VTEC_i\"] >= 0) & (df[\"VTEC_i\"] <= MAX_VTEC_TECU)]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    vt = (\n",
    "        df.groupby(\"time\", sort=True)[\"VTEC_i\"]\n",
    "        .median()\n",
    "        .rename(\"VTEC_TECU\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    diag = {\n",
    "        \"N_obs_used\": int(len(df)),\n",
    "        \"N_epochs\":   int(len(vt)),\n",
    "        \"Nsat_mean\":  float(df.groupby(\"time\").size().mean()) if len(df) else np.nan,\n",
    "    }\n",
    "    return vt, diag\n",
    "\n",
    "# ---------- 30-MIN BINNING + GIM ----------\n",
    "\n",
    "def bin_30min_with_gim(vt_df: pd.DataFrame,\n",
    "                       gim: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Resample VTEC to 30-min bins and optionally attach GIM VTEC.\"\"\"\n",
    "    if vt_df is None or vt_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    s = vt_df.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "    r = s.resample(\"30min\")\n",
    "\n",
    "    v30 = pd.DataFrame(\n",
    "        {\n",
    "            \"time\":        r.median().index,\n",
    "            \"VTEC_median\": r.median().values,\n",
    "            \"VTEC_mean\":   r.mean().values,\n",
    "            \"VTEC_q25\":    r.quantile(0.25).values,\n",
    "            \"VTEC_q75\":    r.quantile(0.75).values,\n",
    "            \"N\":           r.count().values,\n",
    "        }\n",
    "    ).dropna(subset=[\"VTEC_median\", \"N\"])\n",
    "\n",
    "    # Simple Hampel-type outlier suppression on 30-min medians\n",
    "    med = v30[\"VTEC_median\"]\n",
    "    if med.notna().sum() >= 10:\n",
    "        m0 = med.median()\n",
    "        mad = (med - m0).abs().median()\n",
    "        thresh = Hampel_k * 1.4826 * mad if mad > 0 else np.inf\n",
    "        v30.loc[(med - m0).abs() > thresh, [\"VTEC_median\", \"VTEC_mean\"]] = np.nan\n",
    "\n",
    "    # Attach GIM VTEC, if provided\n",
    "    if gim is not None and not gim.empty:\n",
    "        g = gim.set_index(\"time\")[\"vtec_gim\"].sort_index()\n",
    "        g_al = g.reindex(\n",
    "            v30[\"time\"], method=\"nearest\", tolerance=pd.Timedelta(\"15min\")\n",
    "        )\n",
    "        v30[\"vtec_gim\"] = g_al.to_numpy()\n",
    "    else:\n",
    "        v30[\"vtec_gim\"] = np.nan\n",
    "\n",
    "    return v30.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "def append_csv(path: Path, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Append a DataFrame to CSV, creating it with header if needed.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    header = not path.exists()\n",
    "    df.to_csv(path, mode=\"a\", header=header, index=False)\n",
    "\n",
    "# ---------- MAIN DRIVER ----------\n",
    "\n",
    "def main() -> None:\n",
    "    # Load inputs\n",
    "    nav = load_nav(NAV_FILE)\n",
    "    dcb = load_dcb(DCB_FILE)\n",
    "    drx = load_drx(DRX_FILE)\n",
    "    gim = load_gim(GIM_FILE)\n",
    "\n",
    "    # Days: intersection of days in DCB and DRX, filtered by DATE_START/DATE_END\n",
    "    days_dcb = pd.to_datetime(sorted(dcb[\"dcb_date\"].unique()), utc=True)\n",
    "    days_drx = pd.to_datetime(sorted(drx[\"date\"].unique()), utc=True)\n",
    "    all_days = pd.Index(days_dcb).intersection(days_drx)\n",
    "\n",
    "    if DATE_START:\n",
    "        all_days = all_days[all_days >= pd.Timestamp(DATE_START, tz=\"UTC\")]\n",
    "    if DATE_END:\n",
    "        all_days = all_days[all_days <= pd.Timestamp(DATE_END, tz=\"UTC\")]\n",
    "    all_days = all_days.sort_values()\n",
    "\n",
    "    if all_days.empty:\n",
    "        raise SystemExit(\"No overlapping DCB/DRX days in the requested range.\")\n",
    "\n",
    "    print(f\"Days: {len(all_days)} from {all_days[0].date()} to {all_days[-1].date()}\")\n",
    "\n",
    "    # Clear previous outputs\n",
    "    for p in [OUT_30MIN_MS, OUT_30MIN_WL, OUT_DAILY_MS, OUT_DAILY_WL, OUT_MATCH]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "\n",
    "    match_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for t0 in all_days:\n",
    "        day_str = str(t0.date())\n",
    "\n",
    "        # Receiver biases (MS, WLS) for this day\n",
    "        row = drx[drx[\"date\"] == t0.floor(\"D\")]\n",
    "        if row.empty:\n",
    "            print(f\"Skip {day_str}: missing D_rx.\")\n",
    "            continue\n",
    "\n",
    "        D_ms = float(row[\"D_rx_MS_m\"].iloc[0])\n",
    "        D_wls = float(row[\"D_rx_WLS_m\"].iloc[0])\n",
    "\n",
    "        # Observations + NAV merge\n",
    "        obs_day = stack_obs_for_day(t0)\n",
    "        if obs_day is None or obs_day.empty:\n",
    "            print(f\"Skip {day_str}: no observations.\")\n",
    "            continue\n",
    "\n",
    "        m = merge_nav(obs_day, nav, tol=MERGE_TOL)\n",
    "        total_obs = int(len(obs_day))\n",
    "        matched = 0 if m is None or m.empty else int(m[\"ephem_time\"].notna().sum())\n",
    "        rate = 100.0 * matched / max(total_obs, 1)\n",
    "\n",
    "        if matched > 0:\n",
    "            sel = m[\"ephem_time\"].notna()\n",
    "            dt_min = (\n",
    "                m.loc[sel, \"time\"] - m.loc[sel, \"ephem_time\"]\n",
    "            ).dt.total_seconds() / 60.0\n",
    "            p50 = float(np.nanmedian(np.abs(dt_min)))\n",
    "            p95 = float(np.nanpercentile(np.abs(dt_min), 95))\n",
    "        else:\n",
    "            p50 = p95 = np.nan\n",
    "\n",
    "        match_rows.append(\n",
    "            {\n",
    "                \"date\": t0.floor(\"D\"),\n",
    "                \"total_obs\": total_obs,\n",
    "                \"matched_obs\": matched,\n",
    "                \"rate_pct\": rate,\n",
    "                \"abs_dt_p50_min\": p50,\n",
    "                \"abs_dt_p95_min\": p95,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if m is None or m.empty:\n",
    "            print(f\"Warning {day_str}: no NAV match.\")\n",
    "            continue\n",
    "\n",
    "        # DCB for this day\n",
    "        dcb_day = dcb[dcb[\"dcb_date\"] == t0.floor(\"D\")][[\"prn\", \"sat_dcb_m\"]]\n",
    "        if dcb_day.empty:\n",
    "            print(f\"Skip {day_str}: missing DCB.\")\n",
    "            continue\n",
    "\n",
    "        # Geometry\n",
    "        sv = np.vstack(\n",
    "            [sat_ecef_from_navrow(row[\"time\"], row) for _, row in m.iterrows()]\n",
    "        )\n",
    "        m[\"sv_x\"], m[\"sv_y\"], m[\"sv_z\"] = sv[:, 0], sv[:, 1], sv[:, 2]\n",
    "        m[\"elev_deg\"] = [\n",
    "            elevation_deg(RX_XYZ, (x, y, z))\n",
    "            for x, y, z in zip(m[\"sv_x\"], m[\"sv_y\"], m[\"sv_z\"])\n",
    "        ]\n",
    "\n",
    "        # Corrected observable y_m\n",
    "        m = attach_sat_dcb(m, dcb_day)\n",
    "        if m.empty:\n",
    "            print(f\"Skip {day_str}: no usable PRN.\")\n",
    "            continue\n",
    "\n",
    "        # ---- MS branch ----\n",
    "        vt_ms, diag_ms = vtec_from_fixed_drx(m, D_ms, elev_min=ELEV_MIN_DEG_BASE)\n",
    "        if vt_ms is None or vt_ms.empty:\n",
    "            print(f\"Skip {day_str}: MS solution not usable.\")\n",
    "        else:\n",
    "            gim_off_ms = daily_gim_offset(vt_ms, gim) if gim is not None else None\n",
    "            used_mitig_ms = False\n",
    "\n",
    "            # If |offset| >= 20 TECU, tighten elevation mask\n",
    "            if gim_off_ms is not None and abs(gim_off_ms) >= GIM_OFFSET_FLAG_TECU:\n",
    "                m2 = m[m[\"elev_deg\"] >= ELEV_MIN_DEG_BOOST].copy()\n",
    "                vt2, d2 = vtec_from_fixed_drx(m2, D_ms, elev_min=ELEV_MIN_DEG_BOOST)\n",
    "                if vt2 is not None and not vt2.empty:\n",
    "                    vt_ms, diag_ms, used_mitig_ms = vt2, d2, True\n",
    "\n",
    "            # 30-min stats + GIM\n",
    "            v30_ms = bin_30min_with_gim(vt_ms, gim)\n",
    "            if not v30_ms.empty:\n",
    "                append_csv(\n",
    "                    OUT_30MIN_MS, v30_ms.assign(date_utc=t0.floor(\"D\"))\n",
    "                )\n",
    "\n",
    "            # Daily stats\n",
    "            vt_idx = vt_ms.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "            gd = vt_idx.agg(\n",
    "                VTEC_median=\"median\",\n",
    "                VTEC_mean=\"mean\",\n",
    "                VTEC_min=\"min\",\n",
    "                VTEC_max=\"max\",\n",
    "                VTEC_q25=lambda s: s.quantile(0.25),\n",
    "                VTEC_q75=lambda s: s.quantile(0.75),\n",
    "                VTEC_std=\"std\",\n",
    "                N=\"count\",\n",
    "            ).to_frame().T\n",
    "            gd.insert(0, \"date\", t0.floor(\"D\"))\n",
    "            gd[\"D_rx_m_MS\"] = float(D_ms)\n",
    "            gd[\"gim_offset_tecu\"] = (\n",
    "                float(gim_off_ms) if gim_off_ms is not None else np.nan\n",
    "            )\n",
    "            gd[\"offset_flag_abs_ge_20\"] = bool(\n",
    "                gim_off_ms is not None and abs(gim_off_ms) >= GIM_OFFSET_FLAG_TECU\n",
    "            )\n",
    "            gd[\"mitigation_used\"] = bool(used_mitig_ms)\n",
    "            gd[\"N_epochs\"] = diag_ms.get(\"N_epochs\", np.nan)\n",
    "            gd[\"Nsat_mean\"] = diag_ms.get(\"Nsat_mean\", np.nan)\n",
    "            append_csv(OUT_DAILY_MS, gd)\n",
    "\n",
    "        # ---- WLS branch ----\n",
    "        vt_wl, diag_wl = vtec_from_fixed_drx(m, D_wls, elev_min=ELEV_MIN_DEG_BASE)\n",
    "        if vt_wl is None or vt_wl.empty:\n",
    "            print(f\"Skip {day_str}: WLS solution not usable.\")\n",
    "        else:\n",
    "            gim_off_wl = daily_gim_offset(vt_wl, gim) if gim is not None else None\n",
    "            used_mitig_wl = False\n",
    "\n",
    "            if gim_off_wl is not None and abs(gim_off_wl) >= GIM_OFFSET_FLAG_TECU:\n",
    "                m2 = m[m[\"elev_deg\"] >= ELEV_MIN_DEG_BOOST].copy()\n",
    "                vt2, d2 = vtec_from_fixed_drx(m2, D_wls, elev_min=ELEV_MIN_DEG_BOOST)\n",
    "                if vt2 is not None and not vt2.empty:\n",
    "                    vt_wl, diag_wl, used_mitig_wl = vt2, d2, True\n",
    "\n",
    "            v30_wl = bin_30min_with_gim(vt_wl, gim)\n",
    "            if not v30_wl.empty:\n",
    "                append_csv(\n",
    "                    OUT_30MIN_WL, v30_wl.assign(date_utc=t0.floor(\"D\"))\n",
    "                )\n",
    "\n",
    "            vt_idx = vt_wl.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "            gd = vt_idx.agg(\n",
    "                VTEC_median=\"median\",\n",
    "                VTEC_mean=\"mean\",\n",
    "                VTEC_min=\"min\",\n",
    "                VTEC_max=\"max\",\n",
    "                VTEC_q25=lambda s: s.quantile(0.25),\n",
    "                VTEC_q75=lambda s: s.quantile(0.75),\n",
    "                VTEC_std=\"std\",\n",
    "                N=\"count\",\n",
    "            ).to_frame().T\n",
    "            gd.insert(0, \"date\", t0.floor(\"D\"))\n",
    "            gd[\"D_rx_m_WLS\"] = float(D_wls)\n",
    "            gd[\"gim_offset_tecu\"] = (\n",
    "                float(gim_off_wl) if gim_off_wl is not None else np.nan\n",
    "            )\n",
    "            gd[\"offset_flag_abs_ge_20\"] = bool(\n",
    "                gim_off_wl is not None and abs(gim_off_wl) >= GIM_OFFSET_FLAG_TECU\n",
    "            )\n",
    "            gd[\"mitigation_used\"] = bool(used_mitig_wl)\n",
    "            gd[\"N_epochs\"] = diag_wl.get(\"N_epochs\", np.nan)\n",
    "            gd[\"Nsat_mean\"] = diag_wl.get(\"Nsat_mean\", np.nan)\n",
    "            append_csv(OUT_DAILY_WL, gd)\n",
    "\n",
    "        print(\n",
    "            f\"{day_str}  D_rx_MS={D_ms:.3f}  \"\n",
    "            f\"D_rx_WLS={D_wls:.3f}  match={rate:.1f}%\"\n",
    "        )\n",
    "\n",
    "    # Save match stats\n",
    "    pd.DataFrame(match_rows).sort_values(\"date\").to_csv(OUT_MATCH, index=False)\n",
    "    print(\n",
    "        f\"\\n→ 30-min MS : {OUT_30MIN_MS}\\n\"\n",
    "        f\"→ 30-min WLS : {OUT_30MIN_WL}\\n\"\n",
    "        f\"→ Daily MS   : {OUT_DAILY_MS}\\n\"\n",
    "        f\"→ Daily WLS  : {OUT_DAILY_WL}\\n\"\n",
    "        f\"→ Match      : {OUT_MATCH}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"display.width\", 160)\n",
    "    pd.set_option(\"display.max_columns\", 20)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefd20d-9ad8-46ca-9808-7a156da9f9d9",
   "metadata": {},
   "source": [
    "Generate dataframe with solar and geo indices with daily max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47707973-4510-430e-9c6d-4b408e62a3f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge daily VTEC statistics with solar/geomagnetic indices and the time of\n",
    "maximum VTEC (UTC).\n",
    "\n",
    "Inputs:\n",
    "- Daily VTEC stats CSV (one row per day).\n",
    "- 30-min VTEC stats CSV (to extract daily max VTEC and its time).\n",
    "- F10.7 index (CSV).\n",
    "- GFZ index file (Kp, Ap, sunspot number).\n",
    "- Kyoto Dst index file (text).\n",
    "\n",
    "Output:\n",
    "- A single daily CSV with:\n",
    "  * VTEC daily stats\n",
    "  * F10.7 (obs/adj)\n",
    "  * Kp (mean, max), Ap, SN\n",
    "  * Dst (min, mean, count)\n",
    "  * time / bin of max VTEC\n",
    "  * basic solar label (Low/High/NA).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ============================\n",
    "# CONFIG (ADAPT THESE PATHS)\n",
    "# ============================\n",
    "STATS_CSV = Path(\"data/2015_2025_MS_CS_VTEC_daily_stats.csv\")\n",
    "CSV_30MIN = Path(\"data/2015_2025_MS_CS_VTEC_30min_stats.csv\")  # time (UTC), VTEC_median at least\n",
    "CSV_F107  = Path(\"data/indices/F10_7_102015-092025.csv\")\n",
    "FILE_KP   = Path(\"data/indices/GFZ_all_indices.txt\")\n",
    "FILE_DST  = Path(\"data/indices/Kyoto_DST_index.txt\")\n",
    "\n",
    "LOCAL_TZ  = \"Africa/Casablanca\"  # currently unused, kept for future local-time work\n",
    "\n",
    "# Thresholds (edit as needed)\n",
    "THR_F107_LOW = 120.0   # sfu, \"low solar\" vs \"high solar\"\n",
    "\n",
    "# Additional thresholds, if you later add a geomagnetic label\n",
    "THR_KP_Q  = 2.0     # Kp_max <= 2\n",
    "THR_AP_Q  = 7.0     # Ap <= 7\n",
    "THR_DST_Q = -20.0   # Dst_min >= -20 nT\n",
    "\n",
    "THR_F107  = 120.0   # same as THR_F107_LOW, kept for compatibility\n",
    "THR_KP    = 3.0\n",
    "THR_DST   = -30.0\n",
    "\n",
    "# ---------- Generic helpers ----------\n",
    "\n",
    "def _to_utc_series(tcol: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert a time column to UTC timestamps.\"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(tcol):\n",
    "        # If numeric and expressed as (days since 1970-01-01) or similar,\n",
    "        # adjust accordingly; here we use Unix seconds via 2440587.5 as example.\n",
    "        return pd.to_datetime((tcol.astype(float) - 2440587.5) * 86400.0, unit=\"s\", utc=True)\n",
    "    return pd.to_datetime(tcol, utc=True, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _coerce_date_utc_ts(df: pd.DataFrame, col: str = \"date_utc\") -> pd.DataFrame:\n",
    "    \"\"\"Ensure df[col] is a normalized (midnight) UTC timestamp.\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    s = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "    s = s.dt.tz_convert(\"UTC\").dt.normalize()\n",
    "    df[col] = s\n",
    "    return df\n",
    "\n",
    "# ---------- F10.7 (CSV) → daily UTC ----------\n",
    "\n",
    "def load_f107_utc(csv_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load F10.7 index from CSV and build one row per UTC day with:\n",
    "      date_utc, f107_obs, f107_adj\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    tcol, obs_col, adj_col = \"time\", \"F10_7_obs\", \"F10_7_adj\"\n",
    "    if tcol not in df.columns:\n",
    "        raise ValueError(\"F10.7: missing 'time' column.\")\n",
    "\n",
    "    if obs_col not in df.columns:\n",
    "        # Try to infer observed column name\n",
    "        cand = [\n",
    "            c for c in df.columns\n",
    "            if \"obs\" in c.lower().replace(\" \", \"\")\n",
    "            and any(k in c.lower() for k in (\"10.7\", \"f107\", \"f10_7\"))\n",
    "        ]\n",
    "        if not cand:\n",
    "            raise ValueError(\"F10.7: cannot find an 'observed' column.\")\n",
    "        obs_col = cand[0]\n",
    "\n",
    "    ts_utc = _to_utc_series(df[tcol])\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"date_utc\": ts_utc.dt.floor(\"D\"),\n",
    "            \"f107_obs\": pd.to_numeric(df[obs_col], errors=\"coerce\"),\n",
    "            \"f107_adj\": pd.to_numeric(df[adj_col], errors=\"coerce\") if adj_col in df.columns else np.nan,\n",
    "        }\n",
    "    ).dropna(subset=[\"date_utc\"])\n",
    "\n",
    "    out = (\n",
    "        out.drop_duplicates(subset=[\"date_utc\"], keep=\"last\")\n",
    "           .sort_values(\"date_utc\")\n",
    "    )\n",
    "    return _coerce_date_utc_ts(out, \"date_utc\")\n",
    "\n",
    "# ---------- GFZ text → daily Kp (mean/max), Ap, SN ----------\n",
    "\n",
    "# GFZ format (classic all-indices file), per day:\n",
    "# tokens:\n",
    "# 0:Y 1:M 2:D 3:days 4:days_m 5:BSR 6:dB\n",
    "# 7..14: 8 × Kp\n",
    "# 15..22: 8 × ap\n",
    "# 23: Ap\n",
    "# 24: SN\n",
    "# 25: Fobs\n",
    "# 26: Fadj\n",
    "# 27: D\n",
    "\n",
    "def parse_gfz_kp_daily_utc(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Parse GFZ daily file to Kp, Ap, SN per UTC day.\"\"\"\n",
    "    if not path or not path.exists():\n",
    "        return pd.DataFrame(columns=[\"date_utc\"])\n",
    "\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip() or line.startswith(\"#\"):\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            if len(toks) < 27:\n",
    "                # Fallback for atypical line, try to read date from fixed positions\n",
    "                try:\n",
    "                    year = int(line[0:4])\n",
    "                    month = int(line[5:7])\n",
    "                    day = int(line[8:10])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                dt = pd.Timestamp(datetime(year, month, day, tzinfo=timezone.utc))\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"date_utc\": dt,\n",
    "                        \"kp_daily_mean\": np.nan,\n",
    "                        \"kp_daily_max\": np.nan,\n",
    "                        \"Ap\": np.nan,\n",
    "                        \"SN\": np.nan,\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                year, month, day = map(int, toks[:3])\n",
    "                kp_vals = np.array(list(map(float, toks[7:15])), float)   # 8 × Kp\n",
    "                ap_vals = np.array(list(map(float, toks[15:23])), float)  # 8 × ap (unused here)\n",
    "                Ap = float(toks[23]) if toks[23] not in (\"-1\", \"9999\") else np.nan\n",
    "                SN = float(toks[24]) if toks[24] not in (\"-1\", \"9999\") else np.nan\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            dt = pd.Timestamp(datetime(year, month, day, tzinfo=timezone.utc))\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"date_utc\": dt,\n",
    "                    \"kp_daily_mean\": float(np.nanmean(kp_vals)) if np.isfinite(np.nanmean(kp_vals)) else np.nan,\n",
    "                    \"kp_daily_max\": float(np.nanmax(kp_vals))  if np.isfinite(np.nanmax(kp_vals))  else np.nan,\n",
    "                    \"Ap\": Ap,\n",
    "                    \"SN\": SN,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "    return _coerce_date_utc_ts(df, \"date_utc\")\n",
    "\n",
    "# ---------- Kyoto Dst text → daily stats (UTC) ----------\n",
    "\n",
    "def parse_dst_kyoto_daily_utc(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse DST text file from WDC Kyoto and compute daily:\n",
    "      Dst_min, Dst_mean, Dst_n (number of hourly samples).\n",
    "    \"\"\"\n",
    "    if not path or not path.exists():\n",
    "        return pd.DataFrame(columns=[\"date_utc\"])\n",
    "\n",
    "    pat = re.compile(r\"^DST(?P<yy>\\d{2})(?P<mm>\\d{2})\\*(?P<dd>\\d{2})\")\n",
    "    rows = []\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            m = pat.match(line)\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            yy = int(m.group(\"yy\"))\n",
    "            mm = int(m.group(\"mm\"))\n",
    "            dd = int(m.group(\"dd\"))\n",
    "            # For 2015–2025 use 2000+yy convention\n",
    "            year = 2000 + yy\n",
    "\n",
    "            # Extract values after the 'X###' marker\n",
    "            after = re.split(r\"X\\d{3}\", line, maxsplit=1)\n",
    "            vals = []\n",
    "            if len(after) == 2:\n",
    "                nums = re.findall(r\"-?\\d+\", after[1])\n",
    "                vals = [float(n) for n in nums]\n",
    "            if len(vals) < 24:\n",
    "                continue\n",
    "\n",
    "            hourly = np.array(vals[-24:], dtype=float)\n",
    "            dt = pd.Timestamp(datetime(year, mm, dd, tzinfo=timezone.utc))\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"date_utc\": dt,\n",
    "                    \"Dst_min\": float(np.nanmin(hourly)) if np.isfinite(np.nanmin(hourly)) else np.nan,\n",
    "                    \"Dst_mean\": float(np.nanmean(hourly)) if np.isfinite(np.nanmean(hourly)) else np.nan,\n",
    "                    \"Dst_n\": int(np.sum(~np.isnan(hourly))),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "    return _coerce_date_utc_ts(df, \"date_utc\")\n",
    "\n",
    "# ---------- Daily max from 30-min series (UTC) ----------\n",
    "\n",
    "def daily_max_from_30min_utc(\n",
    "    csv_30min: Union[str, Path],\n",
    "    time_col: str = \"time\",\n",
    "    tec_col: str = \"VTEC_median\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From a 30-min VTEC series, compute the daily maximum VTEC and its UTC time.\n",
    "\n",
    "    Returns columns:\n",
    "      date_utc, max_bin_utc, max_ts_utc, VTEC_max_from_30min,\n",
    "      max_hour_utc, max_time_utc_str\n",
    "    \"\"\"\n",
    "    raw = pd.read_csv(csv_30min)\n",
    "    ts_utc = pd.to_datetime(raw[time_col], errors=\"coerce\", utc=True)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"date_utc\": ts_utc.dt.floor(\"D\"),\n",
    "            \"slot_utc\": (ts_utc.dt.hour * 2 + (ts_utc.dt.minute // 30)).astype(\"Int64\"),\n",
    "            \"ts_utc\": ts_utc,\n",
    "            \"tec\": pd.to_numeric(raw[tec_col], errors=\"coerce\"),\n",
    "        }\n",
    "    ).dropna(subset=[\"tec\"])\n",
    "\n",
    "    # Keep the last record per (day, slot)\n",
    "    df = (\n",
    "        df.sort_values([\"date_utc\", \"slot_utc\", \"ts_utc\"])\n",
    "          .drop_duplicates([\"date_utc\", \"slot_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"date_utc\",\n",
    "                \"max_bin_utc\",\n",
    "                \"max_ts_utc\",\n",
    "                \"VTEC_max_from_30min\",\n",
    "                \"max_hour_utc\",\n",
    "                \"max_time_utc_str\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    idx = df.groupby(\"date_utc\")[\"tec\"].idxmax()\n",
    "    out = (\n",
    "        df.loc[idx, [\"date_utc\", \"slot_utc\", \"ts_utc\", \"tec\"]]\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"slot_utc\": \"max_bin_utc\",\n",
    "                \"ts_utc\": \"max_ts_utc\",\n",
    "                \"tec\": \"VTEC_max_from_30min\",\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"date_utc\")\n",
    "    )\n",
    "\n",
    "    out[\"max_hour_utc\"] = out[\"max_ts_utc\"].dt.hour + out[\"max_ts_utc\"].dt.minute / 60.0\n",
    "    out[\"max_time_utc_str\"] = out[\"max_ts_utc\"].dt.strftime(\"%H:%M\")\n",
    "    return out\n",
    "\n",
    "# ============================\n",
    "#          MAIN\n",
    "# ============================\n",
    "\n",
    "def main() -> None:\n",
    "    # 1) Daily VTEC stats (UTC)\n",
    "    stats = pd.read_csv(STATS_CSV, parse_dates=[\"date\"])\n",
    "    if stats[\"date\"].dt.tz is None:\n",
    "        stats[\"date_utc\"] = stats[\"date\"].dt.tz_localize(\"UTC\").dt.normalize()\n",
    "    else:\n",
    "        stats[\"date_utc\"] = stats[\"date\"].dt.tz_convert(\"UTC\").dt.normalize()\n",
    "    stats = _coerce_date_utc_ts(stats, \"date_utc\")\n",
    "\n",
    "    # 2) Indices (UTC)\n",
    "    f107 = load_f107_utc(CSV_F107)             # date_utc, f107_obs, f107_adj\n",
    "    kp   = parse_gfz_kp_daily_utc(FILE_KP)     # date_utc, kp_daily_mean, kp_daily_max, Ap, SN\n",
    "    dst  = parse_dst_kyoto_daily_utc(FILE_DST) # date_utc, Dst_min, Dst_mean, Dst_n\n",
    "\n",
    "    # 3) Time of max VTEC (UTC)\n",
    "    dmax = daily_max_from_30min_utc(CSV_30MIN)\n",
    "\n",
    "    # Ensure all have consistent date_utc\n",
    "    for _df in (stats, f107, kp, dst, dmax):\n",
    "        _coerce_date_utc_ts(_df, \"date_utc\")\n",
    "\n",
    "    # 4) Merge\n",
    "    daily = (\n",
    "        stats\n",
    "        .merge(dmax, on=\"date_utc\", how=\"left\")\n",
    "        .merge(f107, on=\"date_utc\", how=\"left\")\n",
    "        .merge(kp,   on=\"date_utc\", how=\"left\")\n",
    "        .merge(dst,  on=\"date_utc\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # 5) Column ordering (only if columns exist)\n",
    "    first = [\n",
    "        \"date_utc\",\n",
    "        \"VTEC_median\", \"VTEC_mean\", \"VTEC_min\", \"VTEC_max\",\n",
    "        \"VTEC_q25\", \"VTEC_q75\", \"VTEC_std\", \"N\",\n",
    "        \"D_rx_m\",  # or D_rx_m_MS etc., depending on your input file\n",
    "        \"VTEC_max_from_30min\", \"max_bin_utc\",\n",
    "        \"max_time_utc_str\", \"max_hour_utc\", \"max_ts_utc\",\n",
    "        \"gim_offset_tecu\", \"offset_flag_abs_ge_20\", \"mitigation_used\",\n",
    "        \"f107_obs\", \"f107_adj\",\n",
    "        \"kp_daily_mean\", \"kp_daily_max\", \"Ap\", \"SN\",\n",
    "        \"Dst_min\", \"Dst_mean\", \"Dst_n\",\n",
    "    ]\n",
    "    cols = [c for c in first if c in daily.columns] + [\n",
    "        c for c in daily.columns if c not in first\n",
    "    ]\n",
    "    daily = daily[cols]\n",
    "\n",
    "    # 6) Solar label (Low / High / NA) based on F10.7 observed\n",
    "    daily[\"solar_label\"] = np.where(\n",
    "        daily[\"f107_obs\"].notna(),\n",
    "        np.where(daily[\"f107_obs\"] < THR_F107_LOW, \"Low\", \"High\"),\n",
    "        \"NA\",\n",
    "    )\n",
    "\n",
    "    # If you want a geomagnetic label (Q/NQ/NA), uncomment and adapt:\n",
    "    #\n",
    "    # have_all = daily[[\"kp_daily_max\", \"Ap\", \"Dst_min\"]].notna().all(axis=1)\n",
    "    # quiet_mask = (\n",
    "    #     (daily[\"kp_daily_max\"] <= THR_KP_Q) &\n",
    "    #     (daily[\"Ap\"]          <= THR_AP_Q) &\n",
    "    #     (daily[\"Dst_min\"]     >= THR_DST_Q)\n",
    "    # )\n",
    "    # daily[\"geomag_label\"] = np.where(\n",
    "    #     ~have_all, \"NA\",\n",
    "    #     np.where(quiet_mask, \"Q\", \"NQ\")\n",
    "    # )\n",
    "\n",
    "    # 7) Save\n",
    "    out_path = STATS_CSV.with_name(STATS_CSV.stem + \"_UTC_with_indices_and_max.csv\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    daily.to_csv(out_path, index=False)\n",
    "\n",
    "    # 8) Quick log\n",
    "    print(f\"UTC fusion saved: {out_path}\")\n",
    "    for col in [\n",
    "        \"f107_obs\", \"kp_daily_mean\", \"kp_daily_max\",\n",
    "        \"Ap\", \"SN\", \"Dst_min\", \"VTEC_max_from_30min\",\n",
    "    ]:\n",
    "        if col in daily.columns:\n",
    "            print(f\"Missing values — {col}: {daily[col].isna().sum()}\")\n",
    "\n",
    "    # Optional preview\n",
    "    print(daily.head(5))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13076719-0300-4f31-b2a6-c1b82f10c21a",
   "metadata": {},
   "source": [
    "Générer une colonne indiquant le label Q ou D à partir des données GFZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4526dd0-d8f1-4f73-9181-f7b700283d00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add GFZ Quiet/Disturbed labels (Q/D/NQ) to a daily VTEC stats CSV.\n",
    "\n",
    "Inputs:\n",
    "- Daily VTEC stats file (CSV) with a date column (\"date_utc\" or \"date\").\n",
    "- GFZ Quiet/Disturbed days file, e.g. GFZ_Q_NQ_days_2010-2025.txt\n",
    "  in the usual \"10 Q + 5 D per month\" format.\n",
    "\n",
    "Output:\n",
    "- A new CSV with additional columns:\n",
    "    gfz_label  (Q / D / NaN)\n",
    "    gfz_rank   (1..10 for Q, 1..5 for D)\n",
    "    gfz_flag   (e.g. '*', 'A', 'K', etc.)\n",
    "    gfz_code   (e.g. 'Q1', 'Q1*', 'D3', ...)\n",
    "    geomag_label_gfz_QDNQ  (Q / D / NQ; NQ = days not listed by GFZ)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# CONFIG (ADAPT THESE PATHS)\n",
    "# ============================\n",
    "\n",
    "GFZ_FILE = Path(\"data/indices/GFZ_Q_NQ_days_2010-2025.txt\")\n",
    "CSV_DAILY_WITH_OFF = Path(\n",
    "    \"data/2015_2025_WLS_VTEC_daily_stats_UTC_with_indices_and_max.csv\"\n",
    ")\n",
    "\n",
    "# ---------- Helpers for daily CSV ----------\n",
    "\n",
    "def _load_daily_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily VTEC stats CSV and ensure we have a 'date_utc' column\n",
    "    of type datetime.date.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if \"date_utc\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"date_utc\"], utc=True, errors=\"coerce\")\n",
    "    elif \"date\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\")\n",
    "    else:\n",
    "        raise ValueError(\"Daily CSV must contain 'date_utc' or 'date' column.\")\n",
    "\n",
    "    df[\"date_utc\"] = dt.dt.date\n",
    "    return df\n",
    "\n",
    "# ---------- GFZ Q/D file parser ----------\n",
    "\n",
    "# Month name → month number\n",
    "MONTHS = {\n",
    "    \"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"may\": 5, \"jun\": 6,\n",
    "    \"jul\": 7, \"aug\": 8, \"sep\": 9, \"sept\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12,\n",
    "}\n",
    "\n",
    "# Tokens look like \"20\", \"20*\", \"7A\", \"12K\", ...\n",
    "token_re = re.compile(r\"^(\\d{1,2})([A-Za-z\\*]?)$\")\n",
    "\n",
    "def parse_token(tok: str) -> tuple[int | None, str]:\n",
    "    \"\"\"\n",
    "    Parse a single GFZ token.\n",
    "    Returns (day:int, flag:str), or (None, \"\") if invalid.\n",
    "    \"\"\"\n",
    "    tok = tok.strip()\n",
    "    m = token_re.match(tok)\n",
    "    if not m:\n",
    "        return None, \"\"\n",
    "    d = int(m.group(1))\n",
    "    flag = m.group(2)  # '', '*', 'A', 'K', ...\n",
    "    return d, flag\n",
    "\n",
    "\n",
    "def parse_gfz_qd_file(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a GFZ Q/NQ text file of the form:\n",
    "\n",
    "        Month YYYY  d1 d2 ... (10 Quiet tokens) ... (5 Disturbed tokens)\n",
    "\n",
    "    and return a DataFrame with columns:\n",
    "        date_utc (datetime.date),\n",
    "        gfz_label ('Q' or 'D'),\n",
    "        gfz_rank  (1..10 for Q, 1..5 for D),\n",
    "        gfz_flag  ('', '*', 'A', 'K', ...),\n",
    "        gfz_code  (e.g. 'Q1', 'Q1*', 'D3', ...).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            # Month key is the first token, e.g. 'Jan', 'Feb', 'Sept', ...\n",
    "            mkey = parts[0].lower()[:4].strip()\n",
    "            if mkey not in MONTHS:\n",
    "                # Probably a header or malformed line\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                yr = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            mm = MONTHS[mkey]\n",
    "            tokens = parts[2:]  # remaining tokens\n",
    "\n",
    "            # Expected structure: 10 Quiet + 5 Disturbed = 15 tokens minimum\n",
    "            # We still try to parse even if less, to be robust.\n",
    "            q_tokens = tokens[:10]\n",
    "            d_tokens = tokens[10:15]\n",
    "\n",
    "            # Quiet days Q1..Q10\n",
    "            for rank, tok in enumerate(q_tokens, start=1):\n",
    "                day_num, flag = parse_token(tok)\n",
    "                if day_num is None:\n",
    "                    continue\n",
    "\n",
    "                # Validate date (avoid 31 in short months)\n",
    "                try:\n",
    "                    dt = date(yr, mm, day_num)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"date_utc\": dt,\n",
    "                        \"gfz_label\": \"Q\",\n",
    "                        \"gfz_rank\": rank,\n",
    "                        \"gfz_flag\": flag,\n",
    "                        \"gfz_code\": f\"Q{rank}{flag}\".strip(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Disturbed days D1..D5\n",
    "            for rank, tok in enumerate(d_tokens, start=1):\n",
    "                day_num, flag = parse_token(tok)\n",
    "                if day_num is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    dt = date(yr, mm, day_num)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"date_utc\": dt,\n",
    "                        \"gfz_label\": \"D\",\n",
    "                        \"gfz_rank\": rank,\n",
    "                        \"gfz_flag\": flag,\n",
    "                        \"gfz_code\": f\"D{rank}{flag}\".strip(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Keep at most one row per (date_utc, gfz_label) pair\n",
    "    df = df.drop_duplicates(subset=[\"date_utc\", \"gfz_label\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main() -> None:\n",
    "    # Load daily stats and GFZ Q/D info\n",
    "    if not CSV_DAILY_WITH_OFF.exists():\n",
    "        raise FileNotFoundError(f\"Daily stats file not found: {CSV_DAILY_WITH_OFF}\")\n",
    "    if not GFZ_FILE.exists():\n",
    "        raise FileNotFoundError(f\"GFZ Q/D file not found: {GFZ_FILE}\")\n",
    "\n",
    "    daily = _load_daily_csv(CSV_DAILY_WITH_OFF)\n",
    "    gfz = parse_gfz_qd_file(GFZ_FILE)\n",
    "\n",
    "    # Merge GFZ labels into daily stats (do not overwrite existing columns)\n",
    "    daily2 = daily.merge(\n",
    "        gfz[[\"date_utc\", \"gfz_label\", \"gfz_rank\", \"gfz_flag\", \"gfz_code\"]],\n",
    "        on=\"date_utc\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Build a simple \"Q / D / NQ\" label from GFZ only\n",
    "    daily2[\"geomag_label_gfz_QDNQ\"] = np.where(\n",
    "        daily2[\"gfz_label\"].eq(\"Q\"),\n",
    "        \"Q\",\n",
    "        np.where(daily2[\"gfz_label\"].eq(\"D\"), \"D\", \"NQ\"),\n",
    "    )\n",
    "\n",
    "    # OPTIONAL: if you want to also drive an existing 'geomag_label' column:\n",
    "    # daily2[\"geomag_label\"] = daily2[\"geomag_label_gfz_QDNQ\"]\n",
    "\n",
    "    # Output file\n",
    "    out_path = CSV_DAILY_WITH_OFF.with_name(\n",
    "        CSV_DAILY_WITH_OFF.stem + \"_with_GFZlabels.csv\"\n",
    "    )\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    daily2.to_csv(out_path, index=False)\n",
    "\n",
    "    # Summary\n",
    "    nQ = int((daily2[\"gfz_label\"] == \"Q\").sum())\n",
    "    nD = int((daily2[\"gfz_label\"] == \"D\").sum())\n",
    "    print(f\"GFZ labels added. Written: {out_path}\")\n",
    "    print(f\"Days recognized by GFZ: Q={nQ}, D={nD}, remaining = NQ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ddb9b-d5df-4010-8176-eb87ca16a05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
