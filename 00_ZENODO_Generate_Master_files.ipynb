{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9126cbe4-72ca-48bb-a657-4a146e460fcc",
   "metadata": {},
   "source": [
    "Generate a dcb master dataframe (with Daily dcb and/or Monthly code P1P2 and P1C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fef35-fc06-42fa-b732-d6882ce8116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import re, pandas as pd, importlib.util, math\n",
    "\n",
    "# --------- PARAMÈTRES À ADAPTER ---------\n",
    "DAILY_DIR    = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Raw_Data\\All_dcb_new\")\n",
    "MONTHLY_DIR  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Raw_Data\\All_dcb_new\\CODE_DCB\")  # fichiers .DCB (décompressés)\n",
    "PARSER_PATH  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\obs_nav_dcb_parsers\\latest_bsx_dsb_dcb_sat_c2w-c1c_parser_corr.py\")\n",
    "OUT_CSV      = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_dcb_master.csv\")\n",
    "START, END   = date(2015,10,1), date(2025,9,26)\n",
    "PREFER_ORDER = (\"DSB\",\"DCB\")   # ordre de préférence pour les lignes SINEX\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- charger le parseur fourni ---\n",
    "spec = importlib.util.spec_from_file_location(\"bsx_mod\", str(PARSER_PATH))\n",
    "if spec is None or spec.loader is None:\n",
    "    raise FileNotFoundError(f\"Parseur introuvable: {PARSER_PATH}\")\n",
    "bsx_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(bsx_mod)  # type: ignore\n",
    "\n",
    "# --- utilitaires ---\n",
    "def yymm_from_ym(y:int, m:int) -> str:\n",
    "    return f\"{y%100:02d}{m:02d}\"\n",
    "\n",
    "def monthlies_paths(y:int, m:int):\n",
    "    yymm = yymm_from_ym(y, m)\n",
    "    p1c1 = MONTHLY_DIR / f\"P1C1{yymm}.DCB\"\n",
    "    p1p2 = MONTHLY_DIR / f\"P1P2{yymm}_ALL.DCB\"\n",
    "    return p1c1 if p1c1.exists() else None, p1p2 if p1p2.exists() else None\n",
    "\n",
    "def cache_monthly(cache: dict, y:int, m:int):\n",
    "    key = (y, m)\n",
    "    if key in cache: \n",
    "        return cache[key]\n",
    "    p1c1_path, p1p2_path = monthlies_paths(y, m)\n",
    "    m1 = bsx_mod.parse_code_monthly_table(str(p1c1_path)) if p1c1_path else None\n",
    "    m2 = bsx_mod.parse_code_monthly_table(str(p1p2_path)) if p1p2_path else None\n",
    "    cache[key] = (m1, m2, p1c1_path, p1p2_path)\n",
    "    return cache[key]\n",
    "\n",
    "FN_CANDIDATES = lambda y,doy: [\n",
    "    f\"CAS0OPSRAP_{y}{doy:03d}0000_01D_01D_DCB.BIA\",\n",
    "    f\"CAS0MGXRAP_{y}{doy:03d}0000_01D_01D_DCB.BSX\",\n",
    "    f\"GFZ0OPSRAP_{y}{doy:03d}0000_01D_01D_DCB.BIA\",\n",
    "]\n",
    "\n",
    "def pick_daily_file(y:int, doy:int) -> Path|None:\n",
    "    # cherche .BIA/.BSX ou .gz correspondants\n",
    "    for stem in FN_CANDIDATES(y, doy):\n",
    "        p = DAILY_DIR / stem\n",
    "        if p.exists(): return p\n",
    "        gz = p.with_suffix(p.suffix + \".gz\")\n",
    "        if gz.exists(): return gz\n",
    "    return None\n",
    "\n",
    "def parse_year_doy_from_name(name: str):\n",
    "    m = re.search(r\"_(\\d{4})(\\d{3})\\d{5}_\", name)\n",
    "    if m: return int(m.group(1)), int(m.group(2))\n",
    "    m2 = re.search(r\"_(\\d{4})(\\d{3})_\", name)\n",
    "    if m2: return int(m2.group(1)), int(m2.group(2))\n",
    "    return None, None\n",
    "\n",
    "def build_from_monthlies(month_p1c1: dict|None, month_p1p2: dict|None):\n",
    "    \"\"\"Retourne [(prn, val_m, source)] en utilisant (P1-C1) - (P1-P2) si dispo.\"\"\"\n",
    "    rows = []\n",
    "    if not (month_p1c1 and month_p1p2):\n",
    "        return rows\n",
    "    common = set(month_p1c1.keys()) & set(month_p1p2.keys())\n",
    "    for prn in sorted(common):\n",
    "        val = month_p1c1[prn] - month_p1p2[prn]  # = C2W-C1C\n",
    "        rows.append((prn, float(val), \"derived:monthly(P1-C1)-(P1-P2)\"))\n",
    "    return rows\n",
    "\n",
    "# --- boucle jours ---\n",
    "records = []\n",
    "monthly_cache = {}\n",
    "\n",
    "cur = START\n",
    "while cur <= END:\n",
    "    y, doy = cur.year, int(cur.strftime(\"%j\"))\n",
    "    m = cur.month\n",
    "    daily_path = pick_daily_file(y, doy)\n",
    "\n",
    "    # charge mensuels du mois (serviront au besoin)\n",
    "    month_p1c1, month_p1p2, p1c1_path, p1p2_path = cache_monthly(monthly_cache, y, m)\n",
    "\n",
    "    if daily_path:\n",
    "        # 1) essaye direct/inversé/derivé SINEX\n",
    "        try:\n",
    "            rows = bsx_mod.parse_sinex_bias(str(daily_path))\n",
    "        except Exception as e:\n",
    "            rows = []\n",
    "        tbl = bsx_mod.build_table(rows, prefer=PREFER_ORDER, monthly_p1c1=month_p1c1, monthly_p1p2=None)\n",
    "\n",
    "        if not tbl:\n",
    "            # 3) pas de lignes SINEX: bascule mensuels seuls (si dispos)\n",
    "            tbl = build_from_monthlies(month_p1c1, month_p1p2)\n",
    "\n",
    "        # remplir enregistrements\n",
    "        for prn, val_m, src in tbl:\n",
    "            records.append({\n",
    "                \"date\": cur.isoformat(),\n",
    "                \"year\": y,\n",
    "                \"doy\": doy,\n",
    "                \"prn\": prn,\n",
    "                \"C2W-C1C_sat_m\": val_m,\n",
    "                \"source\": src,\n",
    "                \"daily_file\": daily_path.name if daily_path else \"\",\n",
    "                \"monthly_p1c1\": p1c1_path.name if p1c1_path else \"\",\n",
    "                \"monthly_p1p2\": p1p2_path.name if p1p2_path else \"\",\n",
    "            })\n",
    "    else:\n",
    "        # 2/3) aucun journalier: construire uniquement depuis mensuels\n",
    "        tbl = build_from_monthlies(month_p1c1, month_p1p2)\n",
    "        for prn, val_m, src in tbl:\n",
    "            records.append({\n",
    "                \"date\": cur.isoformat(),\n",
    "                \"year\": y,\n",
    "                \"doy\": doy,\n",
    "                \"prn\": prn,\n",
    "                \"C2W-C1C_sat_m\": val_m,\n",
    "                \"source\": src,\n",
    "                \"daily_file\": \"\",\n",
    "                \"monthly_p1c1\": p1c1_path.name if p1c1_path else \"\",\n",
    "                \"monthly_p1p2\": p1p2_path.name if p1p2_path else \"\",\n",
    "            })\n",
    "\n",
    "    cur += timedelta(days=1)\n",
    "\n",
    "# --- DataFrame + export ---\n",
    "df = pd.DataFrame.from_records(records,\n",
    "       columns=[\"date\",\"year\",\"doy\",\"prn\",\"C2W-C1C_sat_m\",\"source\",\"daily_file\",\"monthly_p1c1\",\"monthly_p1p2\"])\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"rows: {len(df)}  | écrit: {OUT_CSV}\")\n",
    "\n",
    "# aperçu\n",
    "display(df.head(20))\n",
    "print(\"\\nComptage par source:\")\n",
    "print(df[\"source\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9cf84-2917-465e-8da0-bac6b11a3be9",
   "metadata": {},
   "source": [
    "Generate a navigation master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71db1e7-3d3f-4282-992b-6e558f103560",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MINI-BATCH NAV RINEX2 → DataFrame unique (option CSV)\n",
    "#   - Parcourt *.??n / *.n (majuscules/minuscules)\n",
    "#   - Utilise le parseur nav_rinex2_to_df.py\n",
    "#   - Sortie: DataFrame \"df_nav\" (et optionnellement un CSV)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import pandas as pd\n",
    "\n",
    "# >>>>>>>>> ADAPTE ICI <<<<<<<<<\n",
    "IN_DIR = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Raw_Data\\All_RINEX\")           # dossier contenant les .YYn / .n\n",
    "PARSER = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\obs_nav_dcb_parsers\\nav_rinex2_to_df.py\")     # chemin vers ton parseur\n",
    "RECURSIVE = False                                         # True = inclut les sous-dossiers\n",
    "WRITE_CSV = True                                        # True pour écrire un CSV\n",
    "OUT_CSV = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_nav_master.csv\")  # chemin CSV si WRITE_CSV=True        \n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# -- Charger le parseur comme module --\n",
    "spec = importlib.util.spec_from_file_location(\"nav_mod\", str(PARSER))\n",
    "if spec is None or spec.loader is None:\n",
    "    raise FileNotFoundError(f\"Impossible de charger le parseur: {PARSER}\")\n",
    "nav_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(nav_mod)  # type: ignore\n",
    "\n",
    "# -- Collecte des fichiers NAV --\n",
    "if not IN_DIR.is_dir():\n",
    "    raise SystemExit(f\"Dossier introuvable: {IN_DIR}\")\n",
    "patterns = [\"*.??n\", \"*.n\", \"*.??N\", \"*.N\"]\n",
    "files = []\n",
    "for pat in patterns:\n",
    "    files += list(IN_DIR.rglob(pat) if RECURSIVE else IN_DIR.glob(pat))\n",
    "files = sorted({p.resolve() for p in files if p.is_file()})\n",
    "print(f\"Fichiers NAV trouvés: {len(files)}\")\n",
    "\n",
    "# -- Parse & agrégation --\n",
    "rows_all = []\n",
    "for p in files:\n",
    "    try:\n",
    "        ephs = nav_mod.parse_rinex2_nav(str(p))  # liste de dicts (voir parser)\n",
    "        if not ephs:\n",
    "            print(f\"⚠️ Aucun éphéméride lu dans: {p.name}\")\n",
    "            continue\n",
    "        for e in ephs:\n",
    "            d = dict(e)\n",
    "            # convertir datetime -> ISO 8601 comme dans l’exemple\n",
    "            if \"toc\" in d and hasattr(d[\"toc\"], \"isoformat\"):\n",
    "                d[\"toc\"] = d[\"toc\"].isoformat()\n",
    "            d[\"nav_file\"] = p.name  # traçabilité\n",
    "            rows_all.append(d)\n",
    "    except Exception as ex:\n",
    "        print(f\"⚠️ Erreur sur {p.name}: {ex}\")\n",
    "\n",
    "# -- DataFrame final (ordre de colonnes calqué sur le parseur) --\n",
    "cols_order = [\n",
    "    \"prn\",\"toc\",\"af0\",\"af1\",\"af2\",\n",
    "    \"IODE\",\"Crs\",\"d_n\",\"M0\",\"Cuc\",\"e\",\"Cus\",\"sqrtA\",\n",
    "    \"Toe\",\"Cic\",\"Omega0\",\"Cis\",\"i0\",\"Crc\",\"omega\",\"OMEGA_DOT\",\n",
    "    \"IDOT\",\"week\",\"sv_accuracy\",\"sv_health\",\"Tgd\",\"IODC\",\"TxTime\",\"fit_int\",\n",
    "    \"nav_file\"\n",
    "]\n",
    "df_nav = pd.DataFrame(rows_all)\n",
    "# garder uniquement les colonnes attendues (si certaines manquent/NaN, ce n’est pas bloquant)\n",
    "df_nav = df_nav[[c for c in cols_order if c in df_nav.columns]].copy()\n",
    "\n",
    "# -- Tri & déduplication douce (une ligne par PRN & toc) --\n",
    "if {\"prn\",\"toc\"}.issubset(df_nav.columns):\n",
    "    before = len(df_nav)\n",
    "    df_nav.drop_duplicates(subset=[\"prn\",\"toc\"], inplace=True)\n",
    "    after = len(df_nav)\n",
    "    if after != before:\n",
    "        print(f\"Déduplication: {before} -> {after} lignes (clé: prn+toc)\")\n",
    "df_nav.sort_values([\"prn\",\"toc\"], inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"Lignes agrégées: {len(df_nav)}\")\n",
    "\n",
    "# -- Affichage interactif (si dispo), sinon head() --\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"NAV_master\", df_nav)\n",
    "except Exception:\n",
    "    display(df_nav.head(20))\n",
    "\n",
    "# -- Option: écrire un CSV unique sur disque --\n",
    "if WRITE_CSV:\n",
    "    OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_nav.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"✅ CSV écrit: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a0afa-49cc-49cf-aef7-c0af086ab476",
   "metadata": {},
   "source": [
    "Generate Parquet files from RINEX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82431ebe-d2a7-4889-b841-0f0b39e9fbe3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Generate Parquet (and .csv) files from obs rinex files\n",
    "# Cette cellule est destinée à générer des fichiers csv et parquet à partir des fichiers rinex obs (.yyo)\n",
    "# contenant les informations necessaires pour le calcul du vtec (en utilisant le fichier dcb matser et nav master). Le calcul final\n",
    "# fait appel au notebook 00_Generate_vtec_stats_csv_from_master_files_vf.ipynb\n",
    "# --------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rinex2_obs_lean import parse_rinex2_obs_lean\n",
    "\n",
    "# >>> adapte le chemin dossier des .??o\n",
    "IN_DIR = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Raw_Data\\All_RINEX\\rnx_from_raw\")   # contient *.??o / *.o\n",
    "RECURSIVE = True                          # True si sous-dossiers à parcourir\n",
    "WRITE_PARQUET = True                    # True si tu veux aussi enregistrer un fichier léger\n",
    "OUT_MASTER_CSV = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\All_Obs_master\\NEW_obs_master_1200_1477.csv\")\n",
    "OUT_PARQUET = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\All_Obs_master\\Parquet\\NEW_OBS_lean_1200_1477.parquet\")\n",
    "\n",
    "patterns = [\"*.??o\",\"*.o\",\"*.??O\",\"*.O\"]\n",
    "files = []\n",
    "for pat in patterns:\n",
    "    files += list(IN_DIR.rglob(pat) if RECURSIVE else IN_DIR.glob(pat))\n",
    "files = sorted({p.resolve() for p in files if p.is_file()})\n",
    "print(f\"Fichiers trouvés: {len(files)}\")\n",
    "\n",
    "dfs = []\n",
    "for i, p in enumerate(files[1200:1478], 1): #c'set dans cette ligne où on doit pointer vers les fichiers .yyo en question!\n",
    "    try:\n",
    "        df = parse_rinex2_obs_lean(str(p))\n",
    "        if not df.empty:\n",
    "            df[\"source_file\"] = p.name  # traçabilité (catégorie)\n",
    "            df[\"source_file\"] = df[\"source_file\"].astype(\"category\")\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"  - {p.name}: aucun C1/P2 lisible\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {p.name}: {e}\")\n",
    "\n",
    "if dfs:\n",
    "    obs_lean = pd.concat(dfs, ignore_index=True)\n",
    "    obs_lean.sort_values([\"time\",\"prn\"], inplace=True, ignore_index=True)\n",
    "    print(f\"Lignes totalisées: {len(obs_lean)}\")\n",
    "else:\n",
    "    obs_lean = pd.DataFrame(columns=[\"time\",\"prn\",\"P2_minus_C1_m\",\"source_file\"])\n",
    "    print(\"Aucune donnée.\")\n",
    "\n",
    "# Affichage rapide\n",
    "display(obs_lean.head(20))\n",
    "\n",
    "# Option: sauvegarde ultra-rapide & compacte\n",
    "if WRITE_PARQUET and not obs_lean.empty:\n",
    "    OUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "    obs_lean.to_csv(OUT_MASTER_CSV, index=False)\n",
    "    obs_lean.to_parquet(OUT_PARQUET, index=False)\n",
    "    print(f\"✅ Parquet écrit: {OUT_PARQUET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df674f81-c5d9-42dc-971b-7b5b7f2b0ced",
   "metadata": {},
   "source": [
    "Generate Receiver bias per day over a period: WLS vs Minimum-Scalloping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337409e-2bde-4192-ac87-9dd7050fa067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Receiver bias per day over a period: WLS vs Minimum-Scalloping (self-contained)\n",
    "\n",
    "import math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIG =====\n",
    "#OBS_DIR   = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\All_Obs_master\\Parquet\")   # parquet/csv: time, prn, P2_minus_C1_m\n",
    "#NAV_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_nav_master.csv\")   # broadcast ephemeris CSV\n",
    "#DCB_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_dcb_master.csv\")   # date|year+doy, prn, C2W-C1C_sat_m\n",
    "#OUT_DRX_CSV = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Drx_WLS_vs_MS_daily.csv\")\n",
    "\n",
    "OBS_DIR   = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/All_Obs_master/Parquet/\")   # parquet/csv: time, prn, P2_minus_C1_m\n",
    "NAV_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/NEW_all_nav_master.csv\")   # broadcast ephemeris CSV\n",
    "DCB_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/NEW_all_dcb_master.csv\")   # date|year+doy, prn, C2W-C1C_sat_m\n",
    "OUT_DRX_CSV = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Drx_WLS_vs_MS_daily.csv\")\n",
    "\n",
    "\n",
    "DATE_START = \"2015-10-01\"\n",
    "DATE_END   = \"2025-09-26\"\n",
    "\n",
    "RX_XYZ = (5411106.3084, -747628.5974, 3286931.3223)  # station ECEF (m)\n",
    "\n",
    "ELEV_MIN_DEG   = 10.0\n",
    "H_IONO         = 350_000.0\n",
    "R_EARTH        = 6378137.0\n",
    "MAX_RESID_M    = 5.0\n",
    "MAX_VTEC_TECU  = 200.0\n",
    "MERGE_TOL      = \"2h\"   # obs-ephemeris nearest tolerance\n",
    "\n",
    "# GPS L1/L2\n",
    "f1 = 1575.42e6; f2 = 1227.60e6\n",
    "K_m_per_TECU = 40.3*(1.0/f2**2 - 1.0/f1**2)  # ≈0.105 m/TECU\n",
    "\n",
    "# ===== NAV / DCB =====\n",
    "def load_nav(path: Path) -> pd.DataFrame:\n",
    "    nav = pd.read_csv(path)\n",
    "    nav[\"prn\"] = nav[\"prn\"].str.upper().str.strip()\n",
    "    nav[\"toc\"] = pd.to_datetime(nav[\"toc\"], utc=True, errors=\"coerce\")\n",
    "    nav[\"ephem_time\"] = (pd.to_datetime(\"1980-01-06\", utc=True)\n",
    "                         + pd.to_timedelta(nav[\"week\"]*7, unit=\"D\")\n",
    "                         + pd.to_timedelta(nav[\"Toe\"], unit=\"s\"))\n",
    "    keep = [\"prn\",\"ephem_time\",\"week\",\"Toe\",\"sqrtA\",\"d_n\",\"e\",\"M0\",\"Cuc\",\"Cus\",\n",
    "            \"Crc\",\"Crs\",\"Cic\",\"Cis\",\"i0\",\"IDOT\",\"Omega0\",\"OMEGA_DOT\",\"omega\"]\n",
    "    return nav[keep].sort_values([\"prn\",\"ephem_time\"]).reset_index(drop=True)\n",
    "\n",
    "def load_dcb(path: Path) -> pd.DataFrame:\n",
    "    dcb = pd.read_csv(path)\n",
    "    dcb[\"prn\"] = dcb[\"prn\"].str.upper().str.strip()\n",
    "    if \"date\" in dcb.columns:\n",
    "        dcb[\"dcb_date\"] = pd.to_datetime(dcb[\"date\"], utc=True).dt.floor(\"D\")\n",
    "    else:\n",
    "        base = pd.to_datetime(dcb[\"year\"].astype(int).astype(str), format=\"%Y\", utc=True)\n",
    "        dcb[\"dcb_date\"] = (base + pd.to_timedelta(dcb[\"doy\"].astype(int)-1, unit=\"D\")).dt.floor(\"D\")\n",
    "    dcb = dcb.rename(columns={\"C2W-C1C_sat_m\":\"sat_dcb_m\"})\n",
    "    return dcb[[\"prn\",\"dcb_date\",\"sat_dcb_m\"]]\n",
    "\n",
    "# ===== OBS =====\n",
    "def _read_day_slice(fp: Path, t0: pd.Timestamp, t1: pd.Timestamp) -> pd.DataFrame:\n",
    "    cols = [\"time\",\"prn\",\"P2_minus_C1_m\"]\n",
    "    if fp.suffix.lower()==\".parquet\":\n",
    "        try:\n",
    "            df = pd.read_parquet(fp, columns=cols,\n",
    "                                 filters=[(\"time\", \">=\", t0), (\"time\", \"<\", t1)])\n",
    "        except Exception:\n",
    "            df = pd.read_parquet(fp, columns=cols)\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "            df = df[(df[\"time\"]>=t0)&(df[\"time\"]<t1)]\n",
    "    else:\n",
    "        df = pd.read_csv(fp, usecols=cols, parse_dates=[\"time\"])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "        df = df[(df[\"time\"]>=t0)&(df[\"time\"]<t1)]\n",
    "    return df\n",
    "\n",
    "def stack_obs_for_day(t0: pd.Timestamp) -> pd.DataFrame | None:\n",
    "    t1 = t0 + pd.Timedelta(days=1)\n",
    "    files = sorted([*OBS_DIR.glob(\"*.parquet\"), *OBS_DIR.glob(\"*.csv\")])\n",
    "    parts=[]\n",
    "    for fp in files:\n",
    "        df = _read_day_slice(fp, t0, t1)\n",
    "        if df is not None and len(df): parts.append(df)\n",
    "    if not parts: return None\n",
    "    obs = pd.concat(parts, ignore_index=True)\n",
    "    obs[\"prn\"]  = obs[\"prn\"].str.upper().str.strip()\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"], utc=True)\n",
    "    obs = obs.dropna(subset=[\"time\",\"prn\",\"P2_minus_C1_m\"])\n",
    "    return obs.sort_values([\"prn\",\"time\"]).reset_index(drop=True)\n",
    "\n",
    "# ===== Geometry =====\n",
    "def ecef_to_geodetic(x,y,z):\n",
    "    a=6378137.0; f=1/298.257223563; e2=f*(2-f); b=a*(1-f); ep2=(a*a-b*b)/(b*b)\n",
    "    r=math.hypot(x,y); E2=a*a-b*b; F=54*b*b*z*z; G=r*r+(1-e2)*z*z-e2*E2\n",
    "    c=(e2*e2*F*r*r)/(G*G*G); s=(1+c+math.sqrt(c*c+2*c))**(1/3)\n",
    "    P=F/(3*(s+1/s+1)**2*G*G); Q=math.sqrt(1+2*e2*e2*P)\n",
    "    r0=-(P*e2*r)/(1+Q)+math.sqrt(0.5*a*a*(1+1/Q)-(P*(1-e2)*z*z)/(Q*(1+Q))-0.5*P*r*r)\n",
    "    U=math.sqrt((r-e2*r0)**2+z*z); V=math.sqrt((r-e2*r0)**2+(1-e2)*z*z)\n",
    "    Z0=b*b*z/(a*V); h=U*(1-b*b/(a*V)); lat=math.atan2(z+ep2*Z0, r); lon=math.atan2(y,x)\n",
    "    return lat,lon,h\n",
    "\n",
    "def elevation_deg(rx_xyz, sat_xyz):\n",
    "    x,y,z = rx_xyz\n",
    "    lat,lon,_ = ecef_to_geodetic(x,y,z)\n",
    "    sl,cl = math.sin(lat), math.cos(lat); slon,clon = math.sin(lon), math.cos(lon)\n",
    "    R = np.array([[-slon,        clon,       0.0],\n",
    "                  [-clon*sl, -slon*sl,      cl ],\n",
    "                  [ clon*cl,  slon*cl,      sl ]])\n",
    "    d = np.array([sat_xyz[0]-x, sat_xyz[1]-y, sat_xyz[2]-z])\n",
    "    e,n,u = R @ d\n",
    "    return math.degrees(math.atan2(u, math.hypot(e,n)))\n",
    "\n",
    "def sat_ecef_from_navrow(t_utc, rnav):\n",
    "    MU=3.986005e14; OMEGA_E=7.2921151467e-5\n",
    "    t = pd.Timestamp(t_utc).tz_convert(\"UTC\")\n",
    "    toe_time = pd.to_datetime(rnav[\"ephem_time\"], utc=True)\n",
    "    tk = (t - toe_time).total_seconds()\n",
    "    half=302400.0\n",
    "    if tk>half: tk-=2*half\n",
    "    if tk<-half: tk+=2*half\n",
    "    A=float(rnav[\"sqrtA\"])**2; dn=float(rnav[\"d_n\"]); ecc=float(rnav[\"e\"]); M0=float(rnav[\"M0\"])\n",
    "    Cuc=float(rnav[\"Cuc\"]); Cus=float(rnav[\"Cus\"]); Crc=float(rnav[\"Crc\"]); Crs=float(rnav[\"Crs\"])\n",
    "    Cic=float(rnav[\"Cic\"]); Cis=float(rnav[\"Cis\"]); i0=float(rnav[\"i0\"]); IDOT=float(rnav[\"IDOT\"])\n",
    "    OMG0=float(rnav[\"Omega0\"]); OMG_DOT=float(rnav[\"OMEGA_DOT\"]); omg=float(rnav[\"omega\"]); toe_s=float(rnav[\"Toe\"])\n",
    "    n0=math.sqrt(MU/A**3); n=n0+dn; M=M0+n*tk\n",
    "    E=M\n",
    "    for _ in range(12):\n",
    "        dE=(M+ecc*math.sin(E)-E)/(1-ecc*math.cos(E)); E+=dE\n",
    "        if abs(dE)<1e-13: break\n",
    "    v=math.atan2(math.sqrt(1-ecc*ecc)*math.sin(E), math.cos(E)-ecc)\n",
    "    phi=v+omg\n",
    "    du=Cuc*math.cos(2*phi)+Cus*math.sin(2*phi)\n",
    "    dr=Crc*math.cos(2*phi)+Crs*math.sin(2*phi)\n",
    "    di=Cic*math.cos(2*phi)+Cis*math.sin(2*phi)\n",
    "    u=phi+du; r=A*(1-ecc*math.cos(E))+dr; i=i0+IDOT*tk+di\n",
    "    x_orb=r*math.cos(u); y_orb=r*math.sin(u)\n",
    "    OMG=OMG0+(OMG_DOT-OMEGA_E)*tk - OMEGA_E*toe_s\n",
    "    cosO, sinO = math.cos(OMG), math.sin(OMG); cosi, sini = math.cos(i), math.sin(i)\n",
    "    x= x_orb*cosO - y_orb*cosi*sinO\n",
    "    y= x_orb*sinO + y_orb*cosi*cosO\n",
    "    z= y_orb*sini\n",
    "    return x,y,z\n",
    "\n",
    "def merge_nav(obs_day: pd.DataFrame, nav: pd.DataFrame, tol: str | None) -> pd.DataFrame:\n",
    "    out=[]\n",
    "    for prn, left in obs_day.groupby(\"prn\", sort=False):\n",
    "        right = nav[nav[\"prn\"]==prn]\n",
    "        if right.empty: continue\n",
    "        left  = left.sort_values(\"time\")\n",
    "        right = right.sort_values(\"ephem_time\")\n",
    "        kw=dict(direction=\"nearest\")\n",
    "        if tol: kw[\"tolerance\"]=pd.Timedelta(tol)\n",
    "        m = pd.merge_asof(left=left, right=right, left_on=\"time\", right_on=\"ephem_time\", **kw)\n",
    "        if m is None or m.empty: continue\n",
    "        if \"prn_x\" in m.columns and \"prn\" not in m.columns:\n",
    "            m = m.rename(columns={\"prn_x\":\"prn\"})\n",
    "        if \"prn_y\" in m.columns:\n",
    "            m = m.drop(columns=[\"prn_y\"])\n",
    "        out.append(m)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# ===== Mapping + DCB =====\n",
    "def mapping_M(el_deg, Re=R_EARTH, H=H_IONO):\n",
    "    el = np.radians(el_deg)\n",
    "    arg = (Re*np.cos(el))/(Re+H)\n",
    "    M = 1.0/np.sqrt(1.0 - np.clip(arg*arg, 0.0, 0.9999))\n",
    "    return np.clip(M, 1.0, 20.0)\n",
    "\n",
    "def attach_sat_dcb(m: pd.DataFrame, dcb_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    mm = m.merge(dcb_day, on=\"prn\", how=\"inner\").dropna(subset=[\"sat_dcb_m\"])\n",
    "    if mm.empty: return mm\n",
    "    mm[\"y_m\"] = mm[\"P2_minus_C1_m\"] - mm[\"sat_dcb_m\"]\n",
    "    return mm\n",
    "\n",
    "def mapping_block(df: pd.DataFrame):\n",
    "    df[\"M\"] = mapping_M(df[\"elev_deg\"].values)\n",
    "    df[\"P\"] = K_m_per_TECU * df[\"M\"].values\n",
    "    df[\"w\"] = 1.0/(df[\"M\"].values**2)\n",
    "    return df\n",
    "\n",
    "# ===== WLS (closed form, 2 passes) =====\n",
    "def solve_closed_form(df: pd.DataFrame):\n",
    "    df = df.dropna(subset=[\"y_m\",\"M\",\"P\",\"w\",\"time\"])\n",
    "    if df.empty: return None, None, None\n",
    "    d = df.assign(wP=df[\"w\"]*df[\"P\"], wP2=df[\"w\"]*(df[\"P\"]**2),\n",
    "                  wy=df[\"w\"]*df[\"y_m\"], wPy=df[\"w\"]*df[\"P\"]*df[\"y_m\"])\n",
    "    G = d.groupby(\"time\", sort=True)[[\"w\",\"wP\",\"wP2\",\"wPy\"]].sum()\n",
    "    S0,S1,S2,b_t = G[\"w\"],G[\"wP\"],G[\"wP2\"],G[\"wPy\"]\n",
    "    b0 = d[\"wy\"].sum()\n",
    "    valid = (S2>0) & (S0>=3)\n",
    "    if not valid.any(): return None, None, None\n",
    "    S0,S1,S2,b_t = S0[valid],S1[valid],S2[valid],b_t[valid]\n",
    "    num = b0 - np.sum(S1*b_t/S2); den = np.sum(S0) - np.sum((S1**2)/S2)\n",
    "    if not np.isfinite(den) or den==0: return None, None, None\n",
    "    D = num/den; V_t = (b_t - S1*D)/S2\n",
    "    times = pd.DatetimeIndex(G.index[valid])\n",
    "    if times.tz is None: times = times.tz_localize(\"UTC\")\n",
    "    times = times.tz_convert(\"UTC\").rename(None)\n",
    "    vt = pd.DataFrame({\"time\": times.to_numpy(copy=False),\n",
    "                       \"VTEC_TECU\": np.asarray(V_t, dtype=float)})\n",
    "    vt.sort_values(by=\"time\", inplace=True, kind=\"mergesort\")\n",
    "    vt.reset_index(drop=True, inplace=True)\n",
    "    return float(D), vt, {\"N_obs\": int(len(df))}\n",
    "\n",
    "def wls_two_pass(df: pd.DataFrame, max_resid_m=MAX_RESID_M):\n",
    "    D1, vt1, _ = solve_closed_form(df)\n",
    "    if D1 is None: return None, None, {}\n",
    "    df[\"time\"]  = pd.to_datetime(df[\"time\"],  utc=True)\n",
    "    vt1[\"time\"] = pd.to_datetime(vt1[\"time\"], utc=True)\n",
    "    vt1_idx = vt1.set_index(\"time\")[\"VTEC_TECU\"]\n",
    "    d1 = df.join(vt1_idx, on=\"time\", how=\"inner\")\n",
    "    pred = D1 + d1[\"P\"].values * d1[\"VTEC_TECU\"].values\n",
    "    resid = d1[\"y_m\"].values - pred\n",
    "    keep = np.isfinite(resid) & (np.abs(resid) <= max_resid_m)\n",
    "    d2 = d1.loc[keep].copy()\n",
    "    D2, vt2, diag = solve_closed_form(d2)\n",
    "    if D2 is None: return None, None, {}\n",
    "    vt2 = vt2[(vt2[\"VTEC_TECU\"]>=0) & (vt2[\"VTEC_TECU\"]<=MAX_VTEC_TECU)].copy()\n",
    "    diag = {\n",
    "        \"N_obs_pass1\": int(len(d1)),\n",
    "        \"N_obs_pass2\": int(len(d2)),\n",
    "        \"N_epochs\":    int(len(vt2)),\n",
    "        \"Nsat_mean\":   float(d2.groupby(\"time\").size().mean()) if len(d2) else np.nan,\n",
    "    }\n",
    "    return float(D2), vt2, diag\n",
    "\n",
    "# ===== Minimum-scalloping =====\n",
    "def _scalloping_score(D, df, night_only=True):\n",
    "    vtec_i = (df[\"y_m\"].values - D) / df[\"P\"].values\n",
    "    tmp = pd.DataFrame({\"time\": pd.to_datetime(df[\"time\"], utc=True), \"vtec\": vtec_i})\n",
    "    if night_only:\n",
    "        lt = tmp[\"time\"].dt.tz_convert(\"Africa/Casablanca\").dt.hour\n",
    "        tmp = tmp[(lt<=4) | (lt>=22)]\n",
    "        if tmp.empty: return np.inf\n",
    "    def mad(a):\n",
    "        m = np.median(a); return np.median(np.abs(a - m))\n",
    "    disp = tmp.groupby(\"time\")[\"vtec\"].apply(mad)\n",
    "    disp = disp[np.isfinite(disp)]\n",
    "    return float(np.median(disp)) if len(disp) else np.inf\n",
    "\n",
    "def minimum_scalloping_D(df, D_init=None, span_m=10.0, step_m=0.1, night_only=True):\n",
    "    if D_init is None:\n",
    "        D0,_,_ = solve_closed_form(df)\n",
    "        if D0 is None: return None, {}\n",
    "        D_init = D0\n",
    "    grid = np.arange(D_init - span_m, D_init + span_m + step_m, step_m)\n",
    "    scores = [ _scalloping_score(D, df, night_only=night_only) for D in grid ]\n",
    "    k = int(np.argmin(scores))\n",
    "    return float(grid[k]), {\"MS_score_min\": float(scores[k]), \"MS_grid_step\": float(step_m)}\n",
    "\n",
    "# ===== One day =====\n",
    "def drx_for_one_day(day_str, elev_min_deg=ELEV_MIN_DEG, night_only=True,\n",
    "                    nav=None, dcb=None) -> dict | None:\n",
    "    if nav is None: nav = load_nav(NAV_FILE)\n",
    "    if dcb is None: dcb = load_dcb(DCB_FILE)\n",
    "    t0 = pd.Timestamp(day_str, tz=\"UTC\").floor(\"D\")\n",
    "\n",
    "    obs_day = stack_obs_for_day(t0)\n",
    "    if obs_day is None or obs_day.empty: return None\n",
    "\n",
    "    m = merge_nav(obs_day, nav, tol=MERGE_TOL)\n",
    "    if m is None or m.empty: return None\n",
    "\n",
    "    dcb_day = dcb[dcb[\"dcb_date\"]==t0.floor(\"D\")][[\"prn\",\"sat_dcb_m\"]]\n",
    "    if dcb_day.empty: return None\n",
    "\n",
    "    sv = np.vstack([sat_ecef_from_navrow(row[\"time\"], row) for _,row in m.iterrows()])\n",
    "    m[\"sv_x\"],m[\"sv_y\"],m[\"sv_z\"] = sv[:,0], sv[:,1], sv[:,2]\n",
    "    m[\"elev_deg\"] = [elevation_deg(RX_XYZ, (x,y,z)) for x,y,z in zip(m[\"sv_x\"],m[\"sv_y\"],m[\"sv_z\"])]\n",
    "\n",
    "    m = attach_sat_dcb(m, dcb_day)\n",
    "    if m.empty: return None\n",
    "\n",
    "    m = m[m[\"elev_deg\"] >= float(elev_min_deg)].copy()\n",
    "    m = mapping_block(m)\n",
    "\n",
    "    D_wls, vt, diag = wls_two_pass(m, max_resid_m=MAX_RESID_M)\n",
    "    if D_wls is None: return None\n",
    "\n",
    "    D_ms, info_ms = minimum_scalloping_D(m, D_init=D_wls, span_m=10.0, step_m=0.1, night_only=True)\n",
    "    delta = (D_ms - D_wls) if D_ms is not None else np.nan\n",
    "\n",
    "    return {\n",
    "        \"date\": t0.date(),\n",
    "        \"D_rx_WLS_m\": float(D_wls),\n",
    "        \"D_rx_MS_m\": float(D_ms) if D_ms is not None else np.nan,\n",
    "        \"delta_MS_minus_WLS_m\": float(delta) if np.isfinite(delta) else np.nan,\n",
    "        \"N_obs_pass1\": diag.get(\"N_obs_pass1\", np.nan),\n",
    "        \"N_obs_pass2\": diag.get(\"N_obs_pass2\", np.nan),\n",
    "        \"N_epochs\":    diag.get(\"N_epochs\",    np.nan),\n",
    "        \"Nsat_mean\":   diag.get(\"Nsat_mean\",   np.nan),\n",
    "        \"MS_score_min\": info_ms.get(\"MS_score_min\", np.nan)\n",
    "    }\n",
    "\n",
    "# ===== Period runner =====\n",
    "def run_period(date_start=DATE_START, date_end=DATE_END, out_csv=OUT_DRX_CSV,\n",
    "               elev_min_deg=ELEV_MIN_DEG):\n",
    "    nav = load_nav(NAV_FILE)\n",
    "    dcb = load_dcb(DCB_FILE)\n",
    "\n",
    "    days = pd.date_range(pd.Timestamp(date_start, tz=\"UTC\"),\n",
    "                         pd.Timestamp(date_end,   tz=\"UTC\"),\n",
    "                         freq=\"D\")\n",
    "    if out_csv.exists(): out_csv.unlink()\n",
    "    rows = []\n",
    "\n",
    "    for t0 in days:\n",
    "        day_txt = str(t0.date())\n",
    "        try:\n",
    "            rec = drx_for_one_day(day_txt, elev_min_deg=elev_min_deg, night_only=True,\n",
    "                                  nav=nav, dcb=dcb)\n",
    "            if rec is None:\n",
    "                print(f\"⏭️ {day_txt}: no result (missing obs/nav/dcb/solvability).\")\n",
    "                continue\n",
    "            rows.append(rec)\n",
    "            pd.DataFrame([rec]).to_csv(out_csv, mode=\"a\", header=not out_csv.exists(), index=False)\n",
    "            print(f\"✅ {day_txt}  WLS={rec['D_rx_WLS_m']:.3f}  MS={rec['D_rx_MS_m']:.3f}  Δ={rec['delta_MS_minus_WLS_m']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {day_txt}: error → {e}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"date\")\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"\\n→ Saved: {out_csv}\")\n",
    "    else:\n",
    "        print(\"No rows produced.\")\n",
    "\n",
    "# ===== Run =====\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"display.width\", 160)\n",
    "    pd.set_option(\"display.max_columns\", 20)\n",
    "    run_period()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169760db-7fef-4c9d-9060-62722e8d9883",
   "metadata": {},
   "source": [
    "Generate a vtec_gim dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adac0a-2c38-486f-bc62-b1ff0c7ff710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import io, subprocess\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "\n",
    "# ====== PARAMÈTRES ======\n",
    "DEC_DIR   = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Raw_Data\\IONEX\\IONEX_decompressed\")  # dossier UNIQUE des IONEX décompressés (.i / .INX)\n",
    "OUT_CSV   = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\VTEC_GIM_30min_20151001_20250926.csv\")\n",
    "STA_LAT   = 31.206\n",
    "STA_LON   = -7.866\n",
    "START     = date(2015, 10, 1)\n",
    "END       = date(2025, 9, 26)\n",
    "# ========================\n",
    "\n",
    "# --- décompression lecture (supporte aussi .Z/.gz si restés par erreur) ---\n",
    "def _open_text(p: Path) -> io.StringIO:\n",
    "    p = Path(p)\n",
    "    if not p.exists(): raise FileNotFoundError(p)\n",
    "    ext = p.suffix.lower()\n",
    "    if ext == \".gz\":\n",
    "        out = subprocess.run([\"gzip\", \"-dc\", str(p)], capture_output=True)\n",
    "        if out.returncode != 0: raise RuntimeError(\"gzip -dc failed\")\n",
    "        return io.StringIO(out.stdout.decode(\"ascii\",\"ignore\"))\n",
    "    if ext == \".z\":\n",
    "        # essaye gzip d’abord\n",
    "        gz = subprocess.run([\"gzip\", \"-dc\", str(p)], capture_output=True)\n",
    "        if gz.returncode == 0 and gz.stdout:\n",
    "            return io.StringIO(gz.stdout.decode(\"ascii\",\"ignore\"))\n",
    "        # sinon 7z\n",
    "        sz = subprocess.run([\"7z\", \"e\", \"-so\", str(p)], capture_output=True)\n",
    "        if sz.returncode != 0: raise RuntimeError(\"7z -so failed\")\n",
    "        return io.StringIO(sz.stdout.decode(\"ascii\",\"ignore\"))\n",
    "    return io.StringIO(p.read_text(encoding=\"ascii\", errors=\"ignore\"))\n",
    "\n",
    "def ionex_first_epoch_date(path: Path):\n",
    "    try:\n",
    "        f = _open_text(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "    for _ in range(400):\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        if \"EPOCH OF FIRST MAP\" in line:\n",
    "            yr, mo, dy, hh, mm, ss = map(int, line[:60].split()[:6])\n",
    "            return datetime(yr, mo, dy, hh, mm, ss, tzinfo=timezone.utc).date()\n",
    "        if \"END OF HEADER\" in line:\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def read_ionex(path: Path):\n",
    "    f = _open_text(path)\n",
    "    exp = -1\n",
    "    lat1=lat2=dlat=lon1=lon2=dlon=None\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: raise ValueError(\"Header IONEX incomplet\")\n",
    "        if \"EXPONENT\" in line:\n",
    "            s = line[:8].strip()\n",
    "            exp = int(s) if s else -1\n",
    "        if \"LAT1 / LAT2 / DLAT\" in line:\n",
    "            lat1,lat2,dlat = map(float, line[:60].split()[:3])\n",
    "        if \"LON1 / LON2 / DLON\" in line:\n",
    "            lon1,lon2,dlon = map(float, line[:60].split()[:3])\n",
    "        if \"END OF HEADER\" in line:\n",
    "            break\n",
    "    if None in (lat1,lat2,dlat,lon1,lon2,dlon): raise ValueError(\"Grille absente\")\n",
    "    nlat = int(round((lat2-lat1)/dlat))+1\n",
    "    nlon = int(round((lon2-lon1)/dlon))+1\n",
    "    lats = np.linspace(lat1, lat2, nlat)\n",
    "    lons = np.linspace(lon1, lon2, nlon)\n",
    "\n",
    "    times, maps = [], []\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        if \"START OF TEC MAP\" in line:\n",
    "            # epoch\n",
    "            line = f.readline()\n",
    "            while line and \"EPOCH OF CURRENT MAP\" not in line:\n",
    "                line = f.readline()\n",
    "            if not line: break\n",
    "            yr,mo,dy,hh,mm,ss = map(int, line[:60].split()[:6])\n",
    "            t = pd.Timestamp(datetime(yr,mo,dy,hh,mm,ss, tzinfo=timezone.utc))\n",
    "            tec = np.full((nlat,nlon), np.nan)\n",
    "            bad = False\n",
    "            for _ in range(nlat):\n",
    "                hdr = f.readline()\n",
    "                if not hdr or \"LAT/LON1/LON2/DLON/H\" not in hdr:\n",
    "                    bad=True; break\n",
    "                vals=[]\n",
    "                while len(vals) < nlon:\n",
    "                    data = f.readline()\n",
    "                    if not data or (\"START OF\" in data) or (\"END OF\" in data) or (\"LAT/LON1\" in data):\n",
    "                        bad=True; break\n",
    "                    chunks = [data[i:i+5] for i in range(0, len(data.rstrip()), 5)]\n",
    "                    for c in chunks:\n",
    "                        c = c.strip().upper()\n",
    "                        if c==\"\" or c==\"9999\": vals.append(np.nan)\n",
    "                        else:\n",
    "                            try: vals.append(float(c)*(10.0**exp))\n",
    "                            except: vals.append(np.nan)\n",
    "                        if len(vals)==nlon: break\n",
    "                if bad: break\n",
    "                if len(vals)<nlon: vals += [np.nan]*(nlon-len(vals))\n",
    "                tec[len(times) if False else _ , :]  # dummy no-op to keep linters calm\n",
    "                tec[_ , :] = vals  # uses loop index positionally\n",
    "            if bad:\n",
    "                # consume until end of map\n",
    "                x = hdr\n",
    "                while x:\n",
    "                    if \"END OF TEC MAP\" in x: break\n",
    "                    x = f.readline()\n",
    "                continue\n",
    "            times.append(t)\n",
    "            maps.append(tec)\n",
    "    if not maps: raise ValueError(\"Aucune carte TEC\")\n",
    "    TEC = np.stack(maps, axis=0)\n",
    "    if lons.min()>=0 and lons.max()>180:\n",
    "        order = np.argsort(((lons+180)%360)-180)\n",
    "        lons = (((lons+180)%360)-180)[order]\n",
    "        TEC  = TEC[:,:,order]\n",
    "    if lats[0]>lats[-1]:\n",
    "        lats = lats[::-1]; TEC = TEC[:, ::-1, :]\n",
    "    times = pd.to_datetime(times, utc=True)\n",
    "    return times, lats, lons, TEC\n",
    "\n",
    "def bilinear(lats, lons, grid, y, x):\n",
    "    i = np.searchsorted(lats, y) - 1\n",
    "    j = np.searchsorted(lons, x) - 1\n",
    "    i = np.clip(i, 0, len(lats)-2); j = np.clip(j, 0, len(lons)-2)\n",
    "    y1,y2 = lats[i], lats[i+1]; x1,x2 = lons[j], lons[j+1]\n",
    "    Q11,Q12,Q21,Q22 = grid[i,j], grid[i,j+1], grid[i+1,j], grid[i+1,j+1]\n",
    "    if (x2-x1)==0 or (y2-y1)==0: return float(Q11)\n",
    "    wx = (x-x1)/(x2-x1); wy = (y-y1)/(y2-y1)\n",
    "    return float((1-wx)*(1-wy)*Q11 + wx*(1-wy)*Q12 + (1-wx)*wy*Q21 + wx*wy*Q22)\n",
    "\n",
    "def to_utc_index(x) -> pd.DatetimeIndex:\n",
    "    idx = pd.DatetimeIndex(x)\n",
    "    if idx.tz is None: idx = idx.tz_localize(\"UTC\")\n",
    "    else: idx = idx.tz_convert(\"UTC\")\n",
    "    return idx.sort_values().unique()\n",
    "\n",
    "def make_30min_grid(day: date) -> pd.DatetimeIndex:\n",
    "    t0 = pd.Timestamp(day, tz=\"UTC\")\n",
    "    return pd.date_range(t0, periods=48, freq=\"30min\")\n",
    "\n",
    "# --- fenêtre produit par date ---\n",
    "def product_window(day: date) -> str:\n",
    "    # renvoie 'OLD', 'OPSFIN', ou 'OPSRAP'\n",
    "    if day <= date(2022, 11, 27):  # 2022-DOY330 inclus -> OLD\n",
    "        return \"OLD\"\n",
    "    if day <= date(2025, 9, 20):   # 2025-DOY263 inclus -> OPSFIN\n",
    "        return \"OPSFIN\"\n",
    "    return \"OPSRAP\"                # après -> RAPID\n",
    "\n",
    "# --- sélection du fichier IONEX décompressé dans DEC_DIR ---\n",
    "def pick_ionex_for_day(day: date) -> Path | None:\n",
    "    yy = f\"{day.year%100:02d}\"; doy = f\"{int(pd.Timestamp(day).strftime('%j')):03d}\"\n",
    "    mode = product_window(day)\n",
    "\n",
    "    # OLD: codgDDD0.yyi (min/maj)\n",
    "    if mode == \"OLD\":\n",
    "        cand = [f\"codg{doy}0.{yy}i\", f\"CODG{doy}0.{yy}I\"]\n",
    "        for name in cand:\n",
    "            p = DEC_DIR / name\n",
    "            if p.exists() and ionex_first_epoch_date(p) == day:\n",
    "                return p\n",
    "        # si plusieurs codg*.i présents, valider par entête\n",
    "        for q in DEC_DIR.glob(f\"codg*{yy}i\"):\n",
    "            if ionex_first_epoch_date(q) == day: return q\n",
    "        for q in DEC_DIR.glob(f\"CODG*{yy}I\"):\n",
    "            if ionex_first_epoch_date(q) == day: return q\n",
    "        return None\n",
    "\n",
    "    # Long names: préférer COD0OPSFIN, sinon COD0OPSRAP. On valide l'entête.\n",
    "    patterns = []\n",
    "    if mode == \"OPSFIN\":\n",
    "        patterns = [f\"COD0OPSFIN_*{day.year}{doy}*_GIM.INX\",\n",
    "                    f\"COD0OPSRAP_*{day.year}{doy}*_GIM.INX\"]\n",
    "    else:  # OPSRAP window\n",
    "        patterns = [f\"COD0OPSRAP_*{day.year}{doy}*_GIM.INX\",\n",
    "                    f\"COD0OPSFIN_*{day.year}{doy}*_GIM.INX\"]\n",
    "\n",
    "    for pat in patterns:\n",
    "        for q in DEC_DIR.glob(pat):\n",
    "            if ionex_first_epoch_date(q) == day:\n",
    "                return q\n",
    "    return None\n",
    "\n",
    "def vtec_30min_from_ionex(ionex_path: Path, day: date, lat: float, lon: float) -> pd.Series:\n",
    "    times, lats, lons, TEC = read_ionex(ionex_path)\n",
    "    lon_match = lon\n",
    "    if float(np.nanmin(lons)) >= 0 and float(np.nanmax(lons)) > 180:\n",
    "        lon_match = (lon + 360) % 360\n",
    "    vals = [bilinear(lats, lons, TEC[k], lat, lon_match) for k in range(len(times))]\n",
    "    ser = pd.Series(vals, index=to_utc_index(times), name=\"vtec_gim\")\n",
    "    # interpolation temporelle vers la grille 30 min du jour\n",
    "    t30 = make_30min_grid(day)\n",
    "    union = ser.index.union(t30)\n",
    "    ser30 = ser.reindex(union).interpolate(\"time\").reindex(t30)\n",
    "    return ser30\n",
    "\n",
    "# ====== BOUCLE PRINCIPALE ======\n",
    "all_rows = []\n",
    "d = START\n",
    "while d <= END:\n",
    "    p = pick_ionex_for_day(d)\n",
    "    if p is None:\n",
    "        # jour manquant -> NaN\n",
    "        t30 = make_30min_grid(d)\n",
    "        all_rows.append(pd.DataFrame({\"time\": t30, \"vtec_gim\": np.nan}))\n",
    "    else:\n",
    "        ser30 = vtec_30min_from_ionex(p, d, STA_LAT, STA_LON)\n",
    "        all_rows.append(pd.DataFrame({\"time\": ser30.index, \"vtec_gim\": ser30.values}))\n",
    "    d += timedelta(days=1)\n",
    "\n",
    "df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "df.sort_values(\"time\", inplace=True, ignore_index=True)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"écrit: {OUT_CSV}  |  lignes: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e8b09-8c54-4231-9ea3-62e013e9abe3",
   "metadata": {},
   "source": [
    "Cellule produisant deux fichiers csv par technique WLS ou MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2e8d7-09c4-46a9-a824-cac02a980baf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === VTEC stats with fixed D_rx (MS & WLS) → four CSVs ===\n",
    "\n",
    "import math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "#OBS_DIR   = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\All_Obs_master\\Parquet\")\n",
    "#NAV_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_nav_master.csv\")\n",
    "#DCB_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\NEW_all_dcb_master.csv\")\n",
    "#GIM_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Master_files\\VTEC_GIM_30min_20151001_20250926.csv\")\n",
    "#DRX_FILE  = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\Drx_WLS_vs_MS_daily.csv\")  # doit contenir: date, D_rx_MS_m, D_rx_WLS_m\n",
    "\n",
    "OBS_DIR   = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/All_Obs_master/Parquet/\")   # parquet/csv: time, prn, P2_minus_C1_m\n",
    "NAV_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/NEW_all_nav_master.csv\")   # broadcast ephemeris CSV\n",
    "DCB_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/NEW_all_dcb_master.csv\")   # date|year+doy, prn, C2W-C1C_sat_m\n",
    "DRX_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Drx_WLS_vs_MS_daily_cycle_slips.csv\")\n",
    "GIM_FILE  = Path(\"/home/mk-laptop/Downloads/TEC_DATA/Master/VTEC_GIM_30min_20151001_20250926.csv\")\n",
    "\n",
    "\n",
    "#OUT_30MIN_MS = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\2015_2025_MS_VTEC_30min_stats.csv\")\n",
    "#OUT_30MIN_WL = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\2015_2025_WLS_VTEC_30min_stats.csv\")\n",
    "#OUT_DAILY_MS = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\2015_2025_MS_VTEC_daily_stats.csv\")\n",
    "#OUT_DAILY_WL = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\2015_2025_WLS_VTEC_daily_stats.csv\")\n",
    "#OUT_MATCH    = Path(r\"G:\\My Drive\\14-DATA\\TEC_DATA\\2015_2025_MS_WLS_match_rates.csv\")\n",
    "\n",
    "OUT_30MIN_MS = Path(\"/home/mk-laptop/Downloads/TEC_DATA/2015_2025_MS_CS_VTEC_30min_stats.csv\")\n",
    "OUT_30MIN_WL = Path(\"/home/mk-laptop/Downloads/TEC_DATA/2015_2025_WLS_CS_VTEC_30min_stats.csv\")\n",
    "OUT_DAILY_MS = Path(\"/home/mk-laptop/Downloads/TEC_DATA/2015_2025_MS_CS_VTEC_daily_stats.csv\")\n",
    "OUT_DAILY_WL = Path(\"/home/mk-laptop/Downloads/TEC_DATA/2015_2025_WLS_CS_VTEC_daily_stats.csv\")\n",
    "OUT_MATCH    = Path(\"/home/mk-laptop/Downloads/TEC_DATA/2015_2025_MS_WLS_CS_match_rates.csv\")\n",
    "\n",
    "DATE_START = \"2015-10-01\"\n",
    "DATE_END   = \"2025-09-26\"\n",
    "\n",
    "RX_XYZ = (5411106.3084, -747628.5974, 3286931.3223)\n",
    "\n",
    "ELEV_MIN_DEG_BASE  = 10.0\n",
    "ELEV_MIN_DEG_BOOST = 15.0\n",
    "R_EARTH = 6378137.0\n",
    "H_IONO  = 350_000.0\n",
    "\n",
    "f1 = 1575.42e6\n",
    "f2 = 1227.60e6\n",
    "K_m_per_TECU = 40.3e16 * (1.0/f2**2 - 1.0/f1**2)  # m/TECU\n",
    "\n",
    "MERGE_TOL = \"2h\"\n",
    "MAX_VTEC_TECU = 200.0\n",
    "Hampel_k = 3.0\n",
    "GIM_OFFSET_FLAG_TECU = 20.0\n",
    "\n",
    "# ---------- GEO / NAV ----------\n",
    "def ecef_to_geodetic(x, y, z):\n",
    "    a=6378137.0; f=1/298.257223563; e2=f*(2-f); b=a*(1-f); ep2=(a*a-b*b)/(b*b)\n",
    "    r=math.hypot(x,y); E2=a*a-b*b; F=54*b*b*z*z; G=r*r+(1-e2)*z*z-e2*E2\n",
    "    c=(e2*e2*F*r*r)/(G*G*G); s=(1+c+math.sqrt(c*c+2*c))**(1/3)\n",
    "    P=F/(3*(s+1/s+1)**2*G*G); Q=math.sqrt(1+2*e2*e2*P)\n",
    "    r0=-(P*e2*r)/(1+Q)+math.sqrt(0.5*a*a*(1+1/Q)-(P*(1-e2)*z*z)/(Q*(1+Q))-0.5*P*r*r)\n",
    "    U=math.sqrt((r-e2*r0)**2+z*z); V=math.sqrt((r-e2*r0)**2+(1-e2)*z*z)\n",
    "    Z0=b*b*z/(a*V); h=U*(1-b*b/(a*V)); lat=math.atan2(z+ep2*Z0, r); lon=math.atan2(y,x)\n",
    "    return lat,lon,h\n",
    "\n",
    "def elevation_deg(rx_xyz, sat_xyz):\n",
    "    x,y,z = rx_xyz\n",
    "    lat,lon,_ = ecef_to_geodetic(x,y,z)\n",
    "    sl,cl = math.sin(lat), math.cos(lat); slon,clon = math.sin(lon), math.cos(lon)\n",
    "    R = np.array([[-slon,        clon,       0.0],\n",
    "                  [-clon*sl, -slon*sl,      cl ],\n",
    "                  [ clon*cl,  slon*cl,      sl ]])\n",
    "    d = np.array([sat_xyz[0]-x, sat_xyz[1]-y, sat_xyz[2]-z])\n",
    "    e,n,u = R @ d\n",
    "    return math.degrees(math.atan2(u, math.hypot(e,n)))\n",
    "\n",
    "def mapping_M(el_deg, Re=R_EARTH, H=H_IONO):\n",
    "    el = np.radians(el_deg)\n",
    "    sin_zp = (Re/(Re + H)) * np.cos(el)\n",
    "    sin_zp = np.clip(sin_zp, -1.0, 1.0)\n",
    "    zp = np.arcsin(sin_zp)\n",
    "    return 1.0/np.cos(zp)\n",
    "\n",
    "def sat_ecef_from_navrow(t_utc, rnav):\n",
    "    MU=3.986005e14; OMEGA_E=7.2921151467e-5\n",
    "    t = pd.Timestamp(t_utc); t = t.tz_localize(\"UTC\") if t.tzinfo is None else t.tz_convert(\"UTC\")\n",
    "    toe_time = pd.to_datetime(rnav[\"ephem_time\"], utc=True)\n",
    "    tk = (t - toe_time).total_seconds(); half=302400.0\n",
    "    if tk>half: tk-=2*half\n",
    "    if tk<-half: tk+=2*half\n",
    "    A=float(rnav[\"sqrtA\"])**2; dn=float(rnav[\"d_n\"]); ecc=float(rnav[\"e\"]); M0=float(rnav[\"M0\"])\n",
    "    Cuc=float(rnav[\"Cuc\"]); Cus=float(rnav[\"Cus\"]); Crc=float(rnav[\"Crc\"]); Crs=float(rnav[\"Crs\"])\n",
    "    Cic=float(rnav[\"Cic\"]); Cis=float(rnav[\"Cis\"]); i0=float(rnav[\"i0\"]); IDOT=float(rnav[\"IDOT\"])\n",
    "    OMG0=float(rnav[\"Omega0\"]); OMG_DOT=float(rnav[\"OMEGA_DOT\"]); omg=float(rnav[\"omega\"]); toe_s=float(rnav[\"Toe\"])\n",
    "    n0=math.sqrt(MU/A**3); n=n0+dn; M=M0+n*tk\n",
    "    E=M\n",
    "    for _ in range(12):\n",
    "        dE=(M+ecc*math.sin(E)-E)/(1-ecc*math.cos(E)); E+=dE\n",
    "        if abs(dE)<1e-13: break\n",
    "    v=math.atan2(math.sqrt(1-ecc*ecc)*math.sin(E), math.cos(E)-ecc)\n",
    "    phi=v+omg\n",
    "    du=Cuc*math.cos(2*phi)+Cus*math.sin(2*phi); dr=Crc*math.cos(2*phi)+Crs*math.sin(2*phi); di=Cic*math.cos(2*phi)+Cis*math.sin(2*phi)\n",
    "    u=phi+du; r=A*(1-ecc*math.cos(E))+dr; i=i0+IDOT*tk+di\n",
    "    x_orb=r*math.cos(u); y_orb=r*math.sin(u)\n",
    "    OMG=OMG0+(OMG_DOT-OMEGA_E)*tk - OMEGA_E*toe_s\n",
    "    cosO,sinO = math.cos(OMG), math.sin(OMG); cosi,sini = math.cos(i), math.sin(i)\n",
    "    x= x_orb*cosO - y_orb*cosi*sinO\n",
    "    y= x_orb*sinO + y_orb*cosi*cosO\n",
    "    z= y_orb*sini\n",
    "    return x,y,z\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def load_nav(path: Path) -> pd.DataFrame:\n",
    "    nav = pd.read_csv(path)\n",
    "    nav[\"prn\"] = nav[\"prn\"].str.upper().str.strip()\n",
    "    nav[\"toc\"] = pd.to_datetime(nav[\"toc\"], utc=True, errors=\"coerce\")\n",
    "    nav[\"ephem_time\"] = (pd.to_datetime(\"1980-01-06\", utc=True)\n",
    "                         + pd.to_timedelta(nav[\"week\"]*7, unit=\"D\")\n",
    "                         + pd.to_timedelta(nav[\"Toe\"], unit=\"s\"))\n",
    "    keep = [\"prn\",\"ephem_time\",\"week\",\"Toe\",\"sqrtA\",\"d_n\",\"e\",\"M0\",\"Cuc\",\"Cus\",\"Crc\",\"Crs\",\"Cic\",\"Cis\",\"i0\",\"IDOT\",\"Omega0\",\"OMEGA_DOT\",\"omega\"]\n",
    "    return nav[keep].sort_values([\"prn\",\"ephem_time\"]).reset_index(drop=True)\n",
    "\n",
    "def load_dcb(path: Path) -> pd.DataFrame:\n",
    "    dcb = pd.read_csv(path)\n",
    "    dcb[\"prn\"] = dcb[\"prn\"].str.upper().str.strip()\n",
    "    base = pd.to_datetime(dcb[\"year\"].astype(int).astype(str), format=\"%Y\", utc=True)\n",
    "    dcb[\"dcb_date\"] = (base + pd.to_timedelta(dcb[\"doy\"].astype(int)-1, unit=\"D\")).dt.floor(\"D\")\n",
    "    dcb = dcb.rename(columns={\"C2W-C1C_sat_m\":\"sat_dcb_m\"})\n",
    "    return dcb[[\"prn\",\"dcb_date\",\"sat_dcb_m\"]]\n",
    "\n",
    "def load_drx(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True).dt.floor(\"D\")\n",
    "    # robust naming\n",
    "    if \"D_rx_MS_m\" not in df.columns:\n",
    "        c = [c for c in df.columns if c.lower().startswith(\"d_rx_ms\")] or [c for c in df.columns if \"MS\" in c]\n",
    "        if c: df = df.rename(columns={c[0]:\"D_rx_MS_m\"})\n",
    "    if \"D_rx_WLS_m\" not in df.columns:\n",
    "        c = [c for c in df.columns if c.lower().startswith(\"d_rx_wls\")] or [c for c in df.columns if \"WLS\" in c]\n",
    "        if c: df = df.rename(columns={c[0]:\"D_rx_WLS_m\"})\n",
    "    need = {\"date\",\"D_rx_MS_m\",\"D_rx_WLS_m\"}\n",
    "    if not need.issubset(df.columns): raise ValueError(\"DRX_FILE must have: date, D_rx_MS_m, D_rx_WLS_m\")\n",
    "    return df[list(need)]\n",
    "\n",
    "# --- obs files ---\n",
    "OBS_FILES = sorted([*Path(OBS_DIR).glob(\"*.parquet\"), *Path(OBS_DIR).glob(\"*.csv\")])\n",
    "print(f\"📄 Fichiers obs trouvés: {len(OBS_FILES)}\")\n",
    "\n",
    "def _read_day_slice(fp: Path, t0: pd.Timestamp, t1: pd.Timestamp) -> pd.DataFrame:\n",
    "    cols = [\"time\",\"prn\",\"P2_minus_C1_m\"]\n",
    "    if fp.suffix.lower()==\".parquet\":\n",
    "        try:\n",
    "            df = pd.read_parquet(fp, columns=cols, engine=\"pyarrow\",\n",
    "                                 filters=[(\"time\", \">=\", t0), (\"time\", \"<\", t1)])\n",
    "        except Exception:\n",
    "            df = pd.read_parquet(fp, columns=cols)\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True); df = df[(df[\"time\"]>=t0)&(df[\"time\"]<t1)]\n",
    "    else:\n",
    "        df = pd.read_csv(fp, usecols=cols, parse_dates=[\"time\"])\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True); df = df[(df[\"time\"]>=t0)&(df[\"time\"]<t1)]\n",
    "    return df\n",
    "\n",
    "def stack_obs_for_day(t0: pd.Timestamp) -> pd.DataFrame | None:\n",
    "    t1 = t0 + pd.Timedelta(days=1)\n",
    "    parts=[]\n",
    "    for fp in OBS_FILES:\n",
    "        df = _read_day_slice(fp, t0, t1)\n",
    "        if df is not None and len(df): parts.append(df)\n",
    "    if not parts: return None\n",
    "    obs = pd.concat(parts, ignore_index=True)\n",
    "    obs[\"prn\"]  = obs[\"prn\"].str.upper().str.strip()\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"], utc=True)\n",
    "    obs = obs.dropna(subset=[\"time\",\"prn\",\"P2_minus_C1_m\"])\n",
    "    return obs.sort_values([\"prn\",\"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "def merge_nav(obs_day: pd.DataFrame, nav: pd.DataFrame, tol: str | None) -> pd.DataFrame:\n",
    "    if obs_day is None or obs_day.empty: return pd.DataFrame()\n",
    "    out=[]\n",
    "    for prn_val, left in obs_day.groupby(\"prn\", sort=False):\n",
    "        right = nav[nav[\"prn\"]==prn_val]\n",
    "        if right.empty: continue\n",
    "        left  = left.sort_values(\"time\", kind=\"mergesort\")\n",
    "        right = right.sort_values(\"ephem_time\", kind=\"mergesort\")\n",
    "        kw=dict(direction=\"nearest\"); \n",
    "        if tol: kw[\"tolerance\"]=pd.Timedelta(tol)\n",
    "        m = pd.merge_asof(left=left, right=right, left_on=\"time\", right_on=\"ephem_time\", **kw)\n",
    "        if m is None or m.empty: continue\n",
    "        if \"prn_x\" in m.columns and \"prn\" not in m.columns: m = m.rename(columns={\"prn_x\":\"prn\"})\n",
    "        if \"prn_y\" in m.columns: m = m.drop(columns=[\"prn_y\"])\n",
    "        if \"prn\" not in m.columns: m[\"prn\"] = prn_val\n",
    "        out.append(m)\n",
    "    if not out: return pd.DataFrame()\n",
    "    m = pd.concat(out, ignore_index=True)\n",
    "    return m.dropna(subset=[\"ephem_time\"]).copy()\n",
    "\n",
    "# ---------- DCB & mapping ----------\n",
    "def attach_sat_dcb(m: pd.DataFrame, dcb_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    m2 = m.merge(dcb_day, on=\"prn\", how=\"inner\").dropna(subset=[\"sat_dcb_m\"])\n",
    "    if m2.empty: return m2\n",
    "    m2[\"y_m\"] = m2[\"P2_minus_C1_m\"] - m2[\"sat_dcb_m\"]\n",
    "    return m2\n",
    "\n",
    "def mapping_block(df: pd.DataFrame):\n",
    "    df[\"M\"] = mapping_M(df[\"elev_deg\"].values)\n",
    "    df[\"P\"] = K_m_per_TECU * df[\"M\"].values\n",
    "    return df\n",
    "\n",
    "# ---------- GIM ----------\n",
    "def load_gim(path: Path) -> pd.DataFrame | None:\n",
    "    if path is None or not Path(path).exists(): return None\n",
    "    g = pd.read_csv(path, parse_dates=[\"time\"])\n",
    "    g[\"time\"] = pd.to_datetime(g[\"time\"], utc=True)\n",
    "    return g[[\"time\",\"vtec_gim\"]]\n",
    "\n",
    "def daily_gim_offset(vt_df: pd.DataFrame, gim: pd.DataFrame) -> float | None:\n",
    "    if gim is None or vt_df is None or vt_df.empty: return None\n",
    "    g = gim.set_index(\"time\")[\"vtec_gim\"].sort_index()\n",
    "    v = vt_df.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "    g_al = g.reindex(v.index, method=\"nearest\", tolerance=pd.Timedelta(\"15min\"))\n",
    "    d = (v - g_al).dropna()\n",
    "    return float(d.median()) if not d.empty else None\n",
    "\n",
    "# ---------- VTEC from fixed D_rx ----------\n",
    "def vtec_from_fixed_drx(df_day: pd.DataFrame, D_rx_m: float, elev_min: float) -> tuple[pd.DataFrame, dict]:\n",
    "    df = df_day[df_day[\"elev_deg\"] >= elev_min].copy()\n",
    "    if df.empty: return pd.DataFrame(), {}\n",
    "    df = mapping_block(df)\n",
    "    vtec_i = (df[\"y_m\"].values - D_rx_m) / df[\"P\"].values\n",
    "    df[\"VTEC_i\"] = vtec_i\n",
    "    df = df[(df[\"VTEC_i\"] >= 0) & (df[\"VTEC_i\"] <= MAX_VTEC_TECU)]\n",
    "    if df.empty: return pd.DataFrame(), {}\n",
    "    vt = df.groupby(\"time\", sort=True)[\"VTEC_i\"].median().rename(\"VTEC_TECU\").reset_index()\n",
    "    diag = {\"N_obs_used\": int(len(df)),\n",
    "            \"N_epochs\":   int(len(vt)),\n",
    "            \"Nsat_mean\":  float(df.groupby(\"time\").size().mean()) if len(df) else np.nan}\n",
    "    return vt, diag\n",
    "\n",
    "# ---------- 30-min bin (+ GIM) ----------\n",
    "def bin_30min_with_gim(vt_df: pd.DataFrame, gim: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    if vt_df is None or vt_df.empty: return pd.DataFrame()\n",
    "    s = vt_df.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "    r = s.resample(\"30min\")\n",
    "    v30 = pd.DataFrame({\n",
    "        \"time\":        r.median().index,\n",
    "        \"VTEC_median\": r.median().values,\n",
    "        \"VTEC_mean\":   r.mean().values,\n",
    "        \"VTEC_q25\":    r.quantile(0.25).values,\n",
    "        \"VTEC_q75\":    r.quantile(0.75).values,\n",
    "        \"N\":           r.count().values\n",
    "    }).dropna(subset=[\"VTEC_median\",\"N\"])\n",
    "    med = v30[\"VTEC_median\"]\n",
    "    if med.notna().sum() >= 10:\n",
    "        m0 = med.median(); mad = (med-m0).abs().median()\n",
    "        thr = Hampel_k*1.4826*mad if mad>0 else np.inf\n",
    "        v30.loc[(med-m0).abs()>thr, [\"VTEC_median\",\"VTEC_mean\"]] = np.nan\n",
    "    if gim is not None and not gim.empty:\n",
    "        g = gim.set_index(\"time\")[\"vtec_gim\"].sort_index()\n",
    "        g_al = g.reindex(v30[\"time\"], method=\"nearest\", tolerance=pd.Timedelta(\"15min\"))\n",
    "        v30[\"vtec_gim\"] = g_al.to_numpy()\n",
    "    else:\n",
    "        v30[\"vtec_gim\"] = np.nan\n",
    "    return v30.dropna(how=\"all\")\n",
    "\n",
    "def append_csv(path: Path, df: pd.DataFrame):\n",
    "    if df is None or df.empty: return\n",
    "    header = not path.exists()\n",
    "    df.to_csv(path, mode=\"a\", header=header, index=False)\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    # load inputs\n",
    "    nav = load_nav(NAV_FILE)\n",
    "    dcb = load_dcb(DCB_FILE)\n",
    "    drx = load_drx(DRX_FILE)\n",
    "    gim = load_gim(GIM_FILE)\n",
    "\n",
    "    # days = intersection des jours disponibles en DCB et DRX + fenêtre\n",
    "    days_dcb = pd.to_datetime(sorted(dcb[\"dcb_date\"].unique()), utc=True)\n",
    "    days_drx = pd.to_datetime(sorted(drx[\"date\"].unique()), utc=True)\n",
    "    all_days = pd.Index(days_dcb).intersection(days_drx)\n",
    "    if DATE_START: all_days = all_days[all_days >= pd.Timestamp(DATE_START, tz=\"UTC\")]\n",
    "    if DATE_END:   all_days = all_days[all_days <= pd.Timestamp(DATE_END,   tz=\"UTC\")]\n",
    "    all_days = all_days.sort_values()\n",
    "    print(f\"🗓️  Jours: {len(all_days)} de {all_days[0].date()} à {all_days[-1].date()}\")\n",
    "\n",
    "    # clear outputs\n",
    "    for p in [OUT_30MIN_MS, OUT_30MIN_WL, OUT_DAILY_MS, OUT_DAILY_WL, OUT_MATCH]:\n",
    "        if p.exists(): p.unlink()\n",
    "\n",
    "    match_rows = []\n",
    "\n",
    "    for t0 in all_days:\n",
    "        day = str(t0.date())\n",
    "        # D_rx par technique\n",
    "        row = drx[drx[\"date\"]==t0.floor(\"D\")]\n",
    "        if row.empty:\n",
    "            print(f\"⏭️ {day}: D_rx manquant.\"); continue\n",
    "        D_ms  = float(row[\"D_rx_MS_m\"].iloc[0])\n",
    "        D_wls = float(row[\"D_rx_WLS_m\"].iloc[0])\n",
    "\n",
    "        # obs + nav\n",
    "        obs_day = stack_obs_for_day(t0)\n",
    "        if obs_day is None or obs_day.empty:\n",
    "            print(f\"⏭️ {day}: pas d’observations.\"); continue\n",
    "        m = merge_nav(obs_day, nav, tol=MERGE_TOL)\n",
    "        total_obs = int(len(obs_day))\n",
    "        matched   = 0 if m is None or m.empty else int(m[\"ephem_time\"].notna().sum())\n",
    "        rate = 100.0*matched/max(total_obs,1)\n",
    "        if matched>0:\n",
    "            sel = m[\"ephem_time\"].notna()\n",
    "            dt_min = (m.loc[sel,\"time\"] - m.loc[sel,\"ephem_time\"]).dt.total_seconds()/60.0\n",
    "            p50 = float(np.nanmedian(np.abs(dt_min))); p95 = float(np.nanpercentile(np.abs(dt_min),95))\n",
    "        else:\n",
    "            p50=p95=np.nan\n",
    "        match_rows.append({\"date\": t0.floor(\"D\"), \"total_obs\": total_obs, \"matched_obs\": matched,\n",
    "                           \"rate_pct\": rate, \"abs_dt_p50_min\": p50, \"abs_dt_p95_min\": p95})\n",
    "        if m is None or m.empty:\n",
    "            print(f\"⚠️ {day}: appariement NAV nul.\"); continue\n",
    "\n",
    "        # DCB du jour\n",
    "        dcb_day = dcb[dcb[\"dcb_date\"]==t0.floor(\"D\")][[\"prn\",\"sat_dcb_m\"]]\n",
    "        if dcb_day.empty:\n",
    "            print(f\"⏭️ {day}: DCB manquant.\"); continue\n",
    "\n",
    "        # géométrie\n",
    "        sv = np.vstack([sat_ecef_from_navrow(row[\"time\"], row) for _,row in m.iterrows()])\n",
    "        m[\"sv_x\"],m[\"sv_y\"],m[\"sv_z\"] = sv[:,0], sv[:,1], sv[:,2]\n",
    "        m[\"elev_deg\"] = [elevation_deg(RX_XYZ, (x,y,z)) for x,y,z in zip(m[\"sv_x\"],m[\"sv_y\"],m[\"sv_z\"])]\n",
    "\n",
    "        # observable corrigée\n",
    "        m = attach_sat_dcb(m, dcb_day)\n",
    "        if m.empty:\n",
    "            print(f\"⏭️ {day}: aucun PRN utilisable.\"); continue\n",
    "\n",
    "        # ---- MS: VTEC avec D_rx_MS ----\n",
    "        vt_ms, diag_ms = vtec_from_fixed_drx(m, D_ms, elev_min=ELEV_MIN_DEG_BASE)\n",
    "        if vt_ms is None or vt_ms.empty:\n",
    "            print(f\"⏭️ {day}: MS solvabilité insuffisante.\"); \n",
    "        else:\n",
    "            gim_off_ms = daily_gim_offset(vt_ms, gim) if gim is not None else None\n",
    "            used_mitig_ms = False\n",
    "            if gim_off_ms is not None and abs(gim_off_ms) >= GIM_OFFSET_FLAG_TECU:\n",
    "                m2 = m[m[\"elev_deg\"] >= ELEV_MIN_DEG_BOOST].copy()\n",
    "                vt2, d2 = vtec_from_fixed_drx(m2, D_ms, elev_min=ELEV_MIN_DEG_BOOST)\n",
    "                if vt2 is not None and not vt2.empty: vt_ms, diag_ms, used_mitig_ms = vt2, d2, True\n",
    "            # 30 min + GIM\n",
    "            v30_ms = bin_30min_with_gim(vt_ms, gim)\n",
    "            if not v30_ms.empty:\n",
    "                append_csv(OUT_30MIN_MS, v30_ms.assign(date_utc=t0.floor(\"D\")))\n",
    "            # daily\n",
    "            vt_idx = vt_ms.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "            gd = vt_idx.agg(VTEC_median=\"median\", VTEC_mean=\"mean\",\n",
    "                            VTEC_min=\"min\", VTEC_max=\"max\",\n",
    "                            VTEC_q25=lambda s: s.quantile(0.25),\n",
    "                            VTEC_q75=lambda s: s.quantile(0.75),\n",
    "                            VTEC_std=\"std\", N=\"count\").to_frame().T\n",
    "            gd.insert(0, \"date\", t0.floor(\"D\"))\n",
    "            gd[\"D_rx_m_MS\"] = float(D_ms)\n",
    "            gd[\"gim_offset_tecu\"] = float(gim_off_ms) if gim_off_ms is not None else np.nan\n",
    "            gd[\"offset_flag_abs_ge_20\"] = bool(gim_off_ms is not None and abs(gim_off_ms) >= GIM_OFFSET_FLAG_TECU)\n",
    "            gd[\"mitigation_used\"] = bool(used_mitig_ms)\n",
    "            gd[\"N_epochs\"] = diag_ms.get(\"N_epochs\", np.nan)\n",
    "            gd[\"Nsat_mean\"] = diag_ms.get(\"Nsat_mean\", np.nan)\n",
    "            append_csv(OUT_DAILY_MS, gd)\n",
    "\n",
    "        # ---- WLS: VTEC avec D_rx_WLS (fixe) ----\n",
    "        vt_wl, diag_wl = vtec_from_fixed_drx(m, D_wls, elev_min=ELEV_MIN_DEG_BASE)\n",
    "        if vt_wl is None or vt_wl.empty:\n",
    "            print(f\"⏭️ {day}: WLS solvabilité insuffisante.\"); \n",
    "        else:\n",
    "            gim_off_wl = daily_gim_offset(vt_wl, gim) if gim is not None else None\n",
    "            used_mitig_wl = False\n",
    "            if gim_off_wl is not None and abs(gim_off_wl) >= GIM_OFFSET_FLAG_TECU:\n",
    "                m2 = m[m[\"elev_deg\"] >= ELEV_MIN_DEG_BOOST].copy()\n",
    "                vt2, d2 = vtec_from_fixed_drx(m2, D_wls, elev_min=ELEV_MIN_DEG_BOOST)\n",
    "                if vt2 is not None and not vt2.empty: vt_wl, diag_wl, used_mitig_wl = vt2, d2, True\n",
    "            # 30 min + GIM\n",
    "            v30_wl = bin_30min_with_gim(vt_wl, gim)\n",
    "            if not v30_wl.empty:\n",
    "                append_csv(OUT_30MIN_WL, v30_wl.assign(date_utc=t0.floor(\"D\")))\n",
    "            # daily\n",
    "            vt_idx = vt_wl.set_index(\"time\")[\"VTEC_TECU\"].sort_index()\n",
    "            gd = vt_idx.agg(VTEC_median=\"median\", VTEC_mean=\"mean\",\n",
    "                            VTEC_min=\"min\", VTEC_max=\"max\",\n",
    "                            VTEC_q25=lambda s: s.quantile(0.25),\n",
    "                            VTEC_q75=lambda s: s.quantile(0.75),\n",
    "                            VTEC_std=\"std\", N=\"count\").to_frame().T\n",
    "            gd.insert(0, \"date\", t0.floor(\"D\"))\n",
    "            gd[\"D_rx_m_WLS\"] = float(D_wls)\n",
    "            gd[\"gim_offset_tecu\"] = float(gim_off_wl) if gim_off_wl is not None else np.nan\n",
    "            gd[\"offset_flag_abs_ge_20\"] = bool(gim_off_wl is not None and abs(gim_off_wl) >= GIM_OFFSET_FLAG_TECU)\n",
    "            gd[\"mitigation_used\"] = bool(used_mitig_wl)\n",
    "            gd[\"N_epochs\"] = diag_wl.get(\"N_epochs\", np.nan)\n",
    "            gd[\"Nsat_mean\"] = diag_wl.get(\"Nsat_mean\", np.nan)\n",
    "            append_csv(OUT_DAILY_WL, gd)\n",
    "\n",
    "        print(f\"✅ {day}  D_rx_MS={D_ms:.3f}  D_rx_WLS={D_wls:.3f}  match={rate:.1f}%\")\n",
    "\n",
    "    pd.DataFrame(match_rows).sort_values(\"date\").to_csv(OUT_MATCH, index=False)\n",
    "    print(f\"\\n→ 30min MS : {OUT_30MIN_MS}\\n→ 30min WLS : {OUT_30MIN_WL}\\n→ Daily MS : {OUT_DAILY_MS}\\n→ Daily WLS : {OUT_DAILY_WL}\\n→ Match : {OUT_MATCH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"display.width\", 160)\n",
    "    pd.set_option(\"display.max_columns\", 20)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefd20d-9ad8-46ca-9808-7a156da9f9d9",
   "metadata": {},
   "source": [
    "Generate dataframe with solar and geo indices with daily max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8351e86-b102-4d28-be4b-f730fc79c3f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, date, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47707973-4510-430e-9c6d-4b408e62a3f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# PIPELINE FUSION (UTC) VTEC + F107 + Kp (GFZ) + Ap + SN + Dst (Kyoto) + heure du max VTEC\n",
    "# ============================\n",
    "# --- CONFIG À ADAPTER ---\n",
    "STATS_CSV = Path(r\"C:/Users/mkmoh/Dropbox/1-DATA/TEC_DATA/New_Data/2015_2025_MS_CS_VTEC_daily_stats.csv\")\n",
    "CSV_30MIN = Path(r\"C:/Users/mkmoh/Dropbox/1-DATA/TEC_DATA/New_Data/2015_2025_MS_CS_VTEC_30min_stats.csv\")             # time (UTC), VTEC_median (au moins)\n",
    "CSV_F107  = Path(r\"C:/Users/mkmoh/Dropbox/1-DATA/TEC_DATA/New_Data/Indices/F10_7_102015-092025.csv\")\n",
    "FILE_KP   = Path(r\"C:\\Users\\mkmoh\\Dropbox\\1-DATA\\TEC_DATA\\New_Data\\Indices\\GFZ_all_indices.txt\")\n",
    "FILE_DST  = Path(r\"C:/Users/mkmoh/Dropbox/1-DATA/TEC_DATA/New_Data/Indices/Kyoto_DST_index.txt\")\n",
    "\n",
    "LOCAL_TZ  = \"Africa/Casablanca\"\n",
    "# --- Seuils (modifiable si besoin) ---\n",
    "THR_F107_LOW = 120.0   # sfu\n",
    "THR_KP_Q     = 2.0     # Kp_max <= 2\n",
    "THR_AP_Q     = 7.0     # Ap <= 7\n",
    "THR_DST_Q    = -20.0   # Dst_min >= -20 nT\n",
    "THR_F107  = 120.0\n",
    "THR_KP    = 3.0     # max < 4 (par défaut) ; mets agg='mean' côté parseur si tu veux la moyenne\n",
    "THR_DST   = -30.0   # Dst_min > -50 nT\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _to_utc_series(tcol: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_numeric_dtype(tcol):\n",
    "        return pd.to_datetime((tcol.astype(float) - 2440587.5) * 86400.0, unit=\"s\", utc=True)\n",
    "    return pd.to_datetime(tcol, utc=True, errors=\"coerce\")\n",
    "\n",
    "def _coerce_date_utc_ts(df: pd.DataFrame, col: str = \"date_utc\") -> pd.DataFrame:\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    s = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "    s = s.dt.tz_convert(\"UTC\").dt.normalize()\n",
    "    df[col] = s\n",
    "    return df\n",
    "\n",
    "# ---------- F10.7 (CSV) -> jour UTC ----------\n",
    "def load_f107_utc(csv_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    tcol, obs_col, adj_col = \"time\", \"F10_7_obs\", \"F10_7_adj\"\n",
    "    if tcol not in df.columns:\n",
    "        raise ValueError(\"F10.7: colonne 'time' manquante.\")\n",
    "    if obs_col not in df.columns:\n",
    "        cand = [c for c in df.columns if \"obs\" in c.lower().replace(\" \", \"\") and any(k in c.lower() for k in (\"10.7\",\"f107\",\"f10_7\"))]\n",
    "        if not cand:\n",
    "            raise ValueError(\"F10.7: colonne 'observed' introuvable.\")\n",
    "        obs_col = cand[0]\n",
    "    ts_utc = _to_utc_series(df[tcol])\n",
    "    out = pd.DataFrame({\n",
    "        \"date_utc\": ts_utc.dt.floor(\"D\"),\n",
    "        \"f107_obs\": pd.to_numeric(df[obs_col], errors=\"coerce\"),\n",
    "        \"f107_adj\": pd.to_numeric(df[adj_col], errors=\"coerce\") if adj_col in df.columns else np.nan\n",
    "    }).dropna(subset=[\"date_utc\"])\n",
    "    out = out.drop_duplicates(subset=[\"date_utc\"], keep=\"last\").sort_values(\"date_utc\")\n",
    "    return _coerce_date_utc_ts(out, \"date_utc\")\n",
    "\n",
    "# ---------- GFZ (texte) -> Kp journaliers + Ap + SN en UTC ----------\n",
    "# Format GFZ: 40 lignes header '#', puis 1 ligne/jour, tokens:\n",
    "# 0:Y 1:M 2:D 3:days 4:days_m 5:BSR 6:dB 7..14: 8 Kp  15..22: 8 ap  23:Ap  24:SN  25:Fobs  26:Fadj  27:D\n",
    "def parse_gfz_kp_daily_utc(path: Path) -> pd.DataFrame:\n",
    "    if not path or not path.exists():\n",
    "        return pd.DataFrame(columns=[\"date_utc\"])\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip() or line.startswith(\"#\"):\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            if len(toks) < 27:\n",
    "                # fallback minimal si ligne atypique\n",
    "                try:\n",
    "                    year = int(line[0:4]); month = int(line[5:7]); day = int(line[8:10])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                dt = pd.Timestamp(datetime(year, month, day, tzinfo=timezone.utc))\n",
    "                rows.append({\"date_utc\": dt, \"kp_daily_mean\": np.nan, \"kp_daily_max\": np.nan, \"Ap\": np.nan, \"SN\": np.nan})\n",
    "                continue\n",
    "            try:\n",
    "                year, month, day = map(int, toks[:3])\n",
    "                kp_vals = np.array(list(map(float, toks[7:15])), float)     # 8×Kp\n",
    "                ap_vals = np.array(list(map(float, toks[15:23])), float)    # 8×ap (pas utilisé ici)\n",
    "                Ap = float(toks[23]) if toks[23] not in (\"-1\",\"9999\") else np.nan\n",
    "                SN = float(toks[24]) if toks[24] not in (\"-1\",\"9999\") else np.nan\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            dt = pd.Timestamp(datetime(year, month, day, tzinfo=timezone.utc))\n",
    "            rows.append({\n",
    "                \"date_utc\":      dt,\n",
    "                \"kp_daily_mean\": float(np.nanmean(kp_vals)) if np.isfinite(np.nanmean(kp_vals)) else np.nan,\n",
    "                \"kp_daily_max\":  float(np.nanmax(kp_vals))  if np.isfinite(np.nanmax(kp_vals))  else np.nan,\n",
    "                \"Ap\":            Ap,\n",
    "                \"SN\":            SN,\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "    return _coerce_date_utc_ts(df, \"date_utc\")\n",
    "\n",
    "# ---------- Dst Kyoto (texte) -> stats journalières UTC ----------\n",
    "def parse_dst_kyoto_daily_utc(path: Path) -> pd.DataFrame:\n",
    "    if not path or not path.exists():\n",
    "        return pd.DataFrame(columns=[\"date_utc\"])\n",
    "    pat = re.compile(r\"^DST(?P<yy>\\d{2})(?P<mm>\\d{2})\\*(?P<dd>\\d{2})\")\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            m = pat.match(line)\n",
    "            if not m:\n",
    "                continue\n",
    "            yy = int(m.group(\"yy\")); mm = int(m.group(\"mm\")); dd = int(m.group(\"dd\"))\n",
    "            year = 2000 + yy  # période 2015–2021 → on force 20xx\n",
    "            # extraire nombres après le tag X###\n",
    "            after = re.split(r\"X\\d{3}\", line, maxsplit=1)\n",
    "            vals = []\n",
    "            if len(after) == 2:\n",
    "                nums = re.findall(r\"[-]?\\d+\", after[1])\n",
    "                vals = [float(n) for n in nums]\n",
    "            if len(vals) < 24:\n",
    "                continue\n",
    "            hourly = np.array(vals[-24:], dtype=float)  # 24 dernières valeurs = heures 0..23\n",
    "            dt = pd.Timestamp(datetime(year, mm, dd, tzinfo=timezone.utc))\n",
    "            rows.append({\n",
    "                \"date_utc\":  dt,\n",
    "                \"Dst_min\":   float(np.nanmin(hourly)) if np.isfinite(np.nanmin(hourly)) else np.nan,\n",
    "                \"Dst_mean\":  float(np.nanmean(hourly)) if np.isfinite(np.nanmean(hourly)) else np.nan,\n",
    "                \"Dst_n\":     int(np.sum(~np.isnan(hourly))),\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "    return _coerce_date_utc_ts(df, \"date_utc\")\n",
    "\n",
    "# ---------- Max quotidien depuis la série 30 min (UTC) ----------\n",
    "def daily_max_from_30min_utc(csv_30min: Union[str, Path],\n",
    "                             time_col=\"time\",\n",
    "                             tec_col=\"VTEC_median\") -> pd.DataFrame:\n",
    "    raw = pd.read_csv(csv_30min)\n",
    "    ts_utc = pd.to_datetime(raw[time_col], errors=\"coerce\", utc=True)\n",
    "    df = pd.DataFrame({\n",
    "        \"date_utc\": ts_utc.dt.floor(\"D\"),\n",
    "        \"slot_utc\": (ts_utc.dt.hour*2 + (ts_utc.dt.minute//30)).astype(\"Int64\"),\n",
    "        \"ts_utc\":   ts_utc,\n",
    "        \"tec\":      pd.to_numeric(raw[tec_col], errors=\"coerce\")\n",
    "    }).dropna(subset=[\"tec\"])\n",
    "    df = df.sort_values([\"date_utc\",\"slot_utc\",\"ts_utc\"]).drop_duplicates([\"date_utc\",\"slot_utc\"], keep=\"last\")\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"date_utc\",\"max_bin_utc\",\"max_ts_utc\",\"VTEC_max_from_30min\",\"max_hour_utc\",\"max_time_utc_str\"])\n",
    "    idx = df.groupby(\"date_utc\")[\"tec\"].idxmax()\n",
    "    out = df.loc[idx, [\"date_utc\",\"slot_utc\",\"ts_utc\",\"tec\"]].rename(\n",
    "        columns={\"slot_utc\":\"max_bin_utc\",\"ts_utc\":\"max_ts_utc\",\"tec\":\"VTEC_max_from_30min\"}\n",
    "    ).sort_values(\"date_utc\")\n",
    "    out[\"max_hour_utc\"] = out[\"max_ts_utc\"].dt.hour + out[\"max_ts_utc\"].dt.minute/60.0\n",
    "    out[\"max_time_utc_str\"] = out[\"max_ts_utc\"].dt.strftime(\"%H:%M\")\n",
    "    return out\n",
    "\n",
    "# ============================\n",
    "#        EXÉCUTION\n",
    "# ============================\n",
    "\n",
    "# 1) Stats VTEC quotidiennes (UTC)\n",
    "stats = pd.read_csv(STATS_CSV, parse_dates=[\"date\"])\n",
    "if stats[\"date\"].dt.tz is None:\n",
    "    stats[\"date_utc\"] = stats[\"date\"].dt.tz_localize(\"UTC\").dt.normalize()\n",
    "else:\n",
    "    stats[\"date_utc\"] = stats[\"date\"].dt.tz_convert(\"UTC\").dt.normalize()\n",
    "stats = _coerce_date_utc_ts(stats, \"date_utc\")\n",
    "\n",
    "# 2) Indices (UTC)\n",
    "f107 = load_f107_utc(CSV_F107)             # date_utc, f107_obs, f107_adj\n",
    "kp   = parse_gfz_kp_daily_utc(FILE_KP)     # date_utc, kp_daily_mean, kp_daily_max, Ap, SN\n",
    "dst  = parse_dst_kyoto_daily_utc(FILE_DST) # date_utc, Dst_min, Dst_mean, Dst_n\n",
    "\n",
    "# 3) Heure du max VTEC (UTC)\n",
    "dmax = daily_max_from_30min_utc(CSV_30MIN)\n",
    "\n",
    "# Harmoniser la clé\n",
    "for _df in (stats, f107, kp, dst, dmax):\n",
    "    _coerce_date_utc_ts(_df, \"date_utc\")\n",
    "\n",
    "# 4) Fusion\n",
    "daily = (\n",
    "    stats\n",
    "    .merge(dmax, on=\"date_utc\", how=\"left\")\n",
    "    .merge(f107, on=\"date_utc\", how=\"left\")\n",
    "    .merge(kp,   on=\"date_utc\", how=\"left\")\n",
    "    .merge(dst,  on=\"date_utc\", how=\"left\")\n",
    "    \n",
    ")\n",
    "\n",
    "# 5) Ordonner les colonnes (si présentes)\n",
    "first = [\n",
    "    \"date_utc\", \"VTEC_median\",\"VTEC_mean\",\"VTEC_min\",\"VTEC_max\",\"VTEC_q25\",\"VTEC_q75\",\"VTEC_std\",\"N\",\"D_rx_m\",\"VTEC_max_from_30min\", \n",
    "    \"max_bin_utc\",\"max_time_utc_str\",\"max_hour_utc\",\"max_ts_utc\",\"gim_offset_tecu\",\"offset_flag_abs_ge_20\",\"mitigation_used\", \"f107_obs\",\"f107_adj\", \"kp_daily_mean\",\"kp_daily_max\",\"Ap\",\"SN\", \"Dst_min\",\n",
    "    \"Dst_mean\",\"Dst_n\"]\n",
    "cols = [c for c in first if c in daily.columns] + [c for c in daily.columns if c not in first]\n",
    "daily = daily[cols]\n",
    "\n",
    "\n",
    "\n",
    "# 1) Label solaire (Low / High / NA)\n",
    "daily[\"solar_label\"] = np.where(daily[\"f107_obs\"].notna(), np.where(daily[\"f107_obs\"] < THR_F107_LOW, \"Low\", \"High\"),\"NA\")\n",
    "\n",
    "# 2) Label géomagnétique (Q / NQ / NA)\n",
    "#have_all = daily[[\"kp_daily_max\", \"Ap\", \"Dst_min\"]].notna().all(axis=1)\n",
    "#quiet_mask = (\n",
    "#    (daily[\"kp_daily_max\"] <= THR_KP_Q) &\n",
    "#    (daily[\"Ap\"]          <= THR_AP_Q) &\n",
    "#    (daily[\"Dst_min\"]     >= THR_DST_Q)\n",
    "#)\n",
    "\n",
    "#daily[\"geomag_label\"] = np.where(\n",
    "#    ~have_all, \"NA\",\n",
    "#    np.where(quiet_mask, \"Q\", \"NQ\")\n",
    "#)\n",
    "\n",
    "\n",
    "# 6) Sauvegarde\n",
    "OUT = STATS_CSV.with_name(STATS_CSV.stem + \"_UTC_with_indices_and_max.csv\")\n",
    "daily.to_csv(OUT, index=False)\n",
    "\n",
    "# 7) Quick log\n",
    "print(f\"✅ Fusion UTC sauvegardée: {OUT}\")\n",
    "for col in [\"f107_obs\",\"kp_daily_mean\",\"kp_daily_max\",\"Ap\",\"SN\",\"Dst_min\",\"VTEC_max_from_30min\"]:\n",
    "    if col in daily.columns:\n",
    "        print(f\"Manquants — {col}: {daily[col].isna().sum()}\")\n",
    "\n",
    "#print(daily.head(3))\n",
    "\n",
    "daily.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13076719-0300-4f31-b2a6-c1b82f10c21a",
   "metadata": {},
   "source": [
    "Générer une colonne indiquant le label Q ou D à partir des données GFZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4526dd0-d8f1-4f73-9181-f7b700283d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ddb9b-d5df-4010-8176-eb87ca16a05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
